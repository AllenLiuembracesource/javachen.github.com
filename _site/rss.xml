<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:itunes="http://www.itunes.com/dtds/podcast-1.0.dtd" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#">
	<channel>
		<title>JavaChen Blog</title>
		<description>关注开源、Java、Pentaho、Hadoop、Cassandra以及数据可视化</description>
		<link>http://blog.javachen.com</link>
		<lastBuildDate>Fri, 15 Nov 2013 16:23:43 +0800</lastBuildDate>
		<pubDate>Fri, 15 Nov 2013 16:23:43 +0800</pubDate>
		<ttl>1800</ttl>


		<item>
			<title><![CDATA[安装saltstack和halite]]></title>
			<description>&lt;p&gt;本文记录安装saltstack和halite过程。&lt;/p&gt;

&lt;p&gt;首先准备两台rhel或者centos虚拟机sk1和sk2，sk1用于安装master，sk2安装minion。&lt;/p&gt;

&lt;h1&gt;下载yum源&lt;/h1&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;ruby language-ruby&quot; data-lang=&quot;ruby&quot;&gt;&lt;span class=&quot;n&quot;&gt;rpm&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ivh&lt;/span&gt; &lt;span class=&quot;ss&quot;&gt;http&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;sr&quot;&gt;//mi&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rrors&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sohu&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;com&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fedora&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;epel&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;6&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x86_64&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;epel&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;release&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;6&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;8&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;noarch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rpm&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h1&gt;安装依赖&lt;/h1&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;bash language-bash&quot; data-lang=&quot;bash&quot;&gt;yum install python-jinja2 -y
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h1&gt;安装saltstack&lt;/h1&gt;

&lt;p&gt;在sk1上安装master：&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;text language-text&quot; data-lang=&quot;text&quot;&gt;yum install salt-master
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;在sk1上安装minion：&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;text language-text&quot; data-lang=&quot;text&quot;&gt;yum install salt-minion
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;!-- more --&gt;

&lt;h1&gt;关闭防火墙&lt;/h1&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;text language-text&quot; data-lang=&quot;text&quot;&gt;iptables -F
setenforce 0
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h1&gt;修改配置文件&lt;/h1&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;text language-text&quot; data-lang=&quot;text&quot;&gt;/etc/salt/master
interface: 0.0.0.0
auto_accept: True
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;text language-text&quot; data-lang=&quot;text&quot;&gt;/etc/salt/minion
master: sk1
id: sk2
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h1&gt;启动&lt;/h1&gt;

&lt;p&gt;分别在sk1和sk2上配置开机启动：&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;text language-text&quot; data-lang=&quot;text&quot;&gt;chkconfig salt-master on
chkconfig salt-minion on
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;分别在sk1和sk2上以service方式启动：&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;text language-text&quot; data-lang=&quot;text&quot;&gt;/etc/init.d/salt-master start
/etc/init.d/salt-minion start
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;你可以在sk2上以后台运行salt-minion&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;text language-text&quot; data-lang=&quot;text&quot;&gt;salt-minion -d
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;或者在sk2上debug方式运行：&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;text language-text&quot; data-lang=&quot;text&quot;&gt;salt-minion -l debug
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h1&gt;排错&lt;/h1&gt;

&lt;p&gt;如果启动提示如下错误：&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;text language-text&quot; data-lang=&quot;text&quot;&gt;[root@sk1 vagrant]# /etc/init.d/salt-master start
Starting salt-master daemon: Traceback (most recent call last):
 File &amp;quot;/usr/bin/salt-master&amp;quot;, line 10, in &amp;lt;module&amp;gt;
   salt_master()
 File &amp;quot;/usr/lib/python2.6/site-packages/salt/scripts.py&amp;quot;, line 20, in salt_master
   master.start()
 File &amp;quot;/usr/lib/python2.6/site-packages/salt/__init__.py&amp;quot;, line 114, in start
   if check_user(self.config[&amp;#39;user&amp;#39;]):
 File &amp;quot;/usr/lib/python2.6/site-packages/salt/utils/verify.py&amp;quot;, line 296, in check_user
   if user in e.gr_mem] + [pwuser.gid])
AttributeError: &amp;#39;pwd.struct_passwd&amp;#39; object has no attribute &amp;#39;gid&amp;#39;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;请下载saltstack源码重新编译：&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;text language-text&quot; data-lang=&quot;text&quot;&gt;wget https://github.com/saltstack/salt/archive/develop.zip
unzip develop
cd salt-develop/
python2.6 setup.py install
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h1&gt;salt minion和master的认证过程&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;minion在第一次启动时，会在/etc/salt/pki/minion/下自动生成minion.pem(private key), minion.pub(public key)，然后将minion.pub发送给master&lt;/li&gt;
&lt;li&gt;master在接收到minion的public key后，通过salt-key命令accept minion public key，这样在master的/etc/salt/pki/master/minions下的将会存放以minion id命名的public key, 然后master就能对minion发送指令了&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;master上执行：&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;text language-text&quot; data-lang=&quot;text&quot;&gt;[root@sk1 pillar]# salt-key -L
Accepted Keys:
Unaccepted Keys:
Rejected Keys:
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;接受所有的认证请求：&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;text language-text&quot; data-lang=&quot;text&quot;&gt;[root@sk1 pillar]# salt-key -A
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;再次查看：&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;text language-text&quot; data-lang=&quot;text&quot;&gt;[root@sk1 pillar]# salt-key -L
Accepted Keys:
sk2
Unaccepted Keys:
Rejected Keys:
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;code&gt;salt-key&lt;/code&gt;更多说明：&lt;a href=&quot;http://docs.saltstack.com/ref/cli/salt-key.html&quot;&gt;http://docs.saltstack.com/ref/cli/salt-key.html&lt;/a&gt;&lt;/p&gt;

&lt;h1&gt;测试运行&lt;/h1&gt;

&lt;p&gt;在master上运行ping：&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;text language-text&quot; data-lang=&quot;text&quot;&gt;[root@sk1 pillar]# salt &amp;#39;*&amp;#39; test.ping
sk2:salt &amp;#39;*&amp;#39; test.ping
    True
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;True表明测试成功。&lt;/p&gt;

&lt;h1&gt;安装halite&lt;/h1&gt;

&lt;h2&gt;下载代码&lt;/h2&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;text language-text&quot; data-lang=&quot;text&quot;&gt;git clone https://github.com/saltstack/halite
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h2&gt;生成index.html&lt;/h2&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;text language-text&quot; data-lang=&quot;text&quot;&gt;cd halite/halite
./genindex.py -C
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h2&gt;安装salt-api&lt;/h2&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;text language-text&quot; data-lang=&quot;text&quot;&gt;yum install salt-api
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h2&gt;配置salt master文件&lt;/h2&gt;

&lt;p&gt;配置salt的master文件，添加：&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;python language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;n&quot;&gt;rest_cherrypy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
 &lt;span class=&quot;n&quot;&gt;host&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.0&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.0&lt;/span&gt;
 &lt;span class=&quot;n&quot;&gt;port&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;8080&lt;/span&gt;
 &lt;span class=&quot;n&quot;&gt;debug&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;true&lt;/span&gt;
 &lt;span class=&quot;n&quot;&gt;static&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;root&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;halite&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;halite&lt;/span&gt;
 &lt;span class=&quot;n&quot;&gt;app&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;root&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;halite&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;halite&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;index&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;html&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;external_auth&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
   &lt;span class=&quot;n&quot;&gt;pam&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
     &lt;span class=&quot;n&quot;&gt;admin&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
     &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;.*&lt;/span&gt;
     &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&amp;#39;@runner&amp;#39;&lt;/span&gt;
     &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&amp;#39;@wheel&amp;#39;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;重启master;&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;text language-text&quot; data-lang=&quot;text&quot;&gt;/etc/init.d/salt-master restart
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h2&gt;添加登陆用户&lt;/h2&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;text language-text&quot; data-lang=&quot;text&quot;&gt;useradd admin
echo admin|passwd –stdin admin
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h2&gt;启动 salt-api&lt;/h2&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;text language-text&quot; data-lang=&quot;text&quot;&gt;cd halite/halite
python2.6 server_bottle.py -d -C -l debug -s cherrypy
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;然后打开&lt;code&gt;http://ip:8080/app&lt;/code&gt;，通过admin/admin登陆即可。&lt;/p&gt;
</description>
			<link>http://blog.javachen.com/saltstack/2013/11/11/install-saltstack-and-halite</link>
			<guid>http://blog.javachen.com/saltstack/2013/11/11/install-saltstack-and-halite</guid>
			<pubDate>Mon, 11 Nov 2013 00:00:00 +0800</pubDate>
		</item>

		<item>
			<title><![CDATA[在eclipse中调试运行hbase]]></title>
			<description>&lt;p&gt;这篇文章记录一下如何在eclipse中调试运行hbase。&lt;/p&gt;

&lt;h1&gt;下载并编译源代码&lt;/h1&gt;

&lt;p&gt;请参考&lt;a href=&quot;http://blog.javachen.com/hbase/2013/10/28/compile-hbase-source-code-and-apply-patches/&quot;&gt;编译hbase源代码并打补丁&lt;/a&gt;&lt;/p&gt;

&lt;h1&gt;修改配置文件&lt;/h1&gt;

&lt;p&gt;修改 &lt;code&gt;conf/hbase-site.xml&lt;/code&gt;文件：&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;text language-text&quot; data-lang=&quot;text&quot;&gt;&amp;lt;property&amp;gt;
&amp;lt;name&amp;gt;hbase.defaults.for.version&amp;lt;/name&amp;gt;
&amp;lt;value&amp;gt;0.94.6-cdh4.4.0&amp;lt;/value&amp;gt;
&amp;lt;/property&amp;gt;

&amp;lt;property&amp;gt;
&amp;lt;name&amp;gt;hbase.rootdir&amp;lt;/name&amp;gt;
&amp;lt;value&amp;gt;file:///home/june/tmp/data&amp;lt;/value&amp;gt;
&amp;lt;/property&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;把conf文件夹加到Classpath中&lt;/p&gt;

&lt;h1&gt;运行HMaster&lt;/h1&gt;

&lt;p&gt;新建一个&lt;code&gt;Debug Configuration&lt;/code&gt;,  main class 是&lt;code&gt;org.apache.hadoop.hbase.master.HMaster&lt;/code&gt;,  参数填start&lt;/p&gt;

&lt;!-- more --&gt;

&lt;p&gt;调试运行该类，运行成功之后日志如下：&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;text language-text&quot; data-lang=&quot;text&quot;&gt;13/10/29 14:38:29 WARN zookeeper.RecoverableZooKeeper: Node /hbase/table/.META. already deleted, and this is not a retry
13/10/29 14:38:29 INFO regionserver.HRegionServer: Received request to open region: .META.,,1.1028785192
13/10/29 14:38:29 INFO regionserver.HRegion: Setting up tabledescriptor config now ...
13/10/29 14:38:29 INFO regionserver.Store: time to purge deletes set to 0ms in store info
13/10/29 14:38:29 INFO regionserver.HRegion: Onlined .META.,,1.1028785192; next sequenceid=1
13/10/29 14:38:29 INFO regionserver.HRegionServer: Post open deploy tasks for region=.META.,,1.1028785192, daughter=false
13/10/29 14:38:29 INFO catalog.MetaEditor: Updated row .META.,,1.1028785192 with server=june-mint,47477,1383028701871
13/10/29 14:38:29 INFO regionserver.HRegionServer: Done with post open deploy task for region=.META.,,1.1028785192, daughter=false
13/10/29 14:38:29 INFO handler.OpenedRegionHandler: Handling OPENED event for .META.,,1.1028785192 from june-mint,47477,1383028701871; deleting unassigned node
13/10/29 14:38:29 INFO master.AssignmentManager: The master has opened the region .META.,,1.1028785192 that was online on june-mint,47477,1383028701871
13/10/29 14:38:29 INFO master.HMaster: .META. assigned=2, rit=false, location=june-mint,47477,1383028701871
13/10/29 14:38:29 INFO catalog.MetaMigrationRemovingHTD: Meta version=0; migrated=true
13/10/29 14:38:29 INFO catalog.MetaMigrationRemovingHTD: ROOT/Meta already up-to date with new HRI.
13/10/29 14:38:29 INFO master.AssignmentManager: Clean cluster startup. Assigning userregions
13/10/29 14:38:29 INFO master.HMaster: Registered HMaster MXBean
13/10/29 14:38:29 INFO master.HMaster: Master has completed initialization
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;如果想修改日志级别，请修改&lt;code&gt;conf/log4j.properties&lt;/code&gt;中级别为INFO:&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;text language-text&quot; data-lang=&quot;text&quot;&gt;#Logging Threshold
log4j.threshold=INFO
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h1&gt;运行HRegionServer&lt;/h1&gt;

&lt;p&gt;参考上面的方法，运行HRegionServer，这时候会出现如下日志：&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;text language-text&quot; data-lang=&quot;text&quot;&gt;13/11/04 11:50:47 INFO util.VersionInfo: HBase 0.94.6-cdh4.4.0
13/11/04 11:50:47 INFO util.VersionInfo: Subversion git://june-mint/chan/workspace/hadoop/hbase -r 979969e1d0d95ce3b8c1d14593f55148da8bc98f
13/11/04 11:50:47 INFO util.VersionInfo: Compiled by june on Tue Oct 29 15:11:51 CST 2013
13/11/04 11:50:47 WARN regionserver.HRegionServerCommandLine: Not starting a distinct region server because hbase.cluster.distributed is false
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;这是因为当&lt;code&gt;hbase.cluster.distributed=false&lt;/code&gt;时，hbase为本地模式，master和regionserver在同一个jvm启动，并且会启动一个最小化的zookeeper集群。请参看：&lt;code&gt;HMasterCommandLine.java&lt;/code&gt;的startMaster()方法。&lt;/p&gt;

&lt;p&gt;如果你把该值设为true，则hbase集群为分布式模式，这时候默认会连接&lt;code&gt;127.0.0.1：2181&lt;/code&gt;对应的zookeeper集群（该集群需要在master启动之前启动）。当然，你可以修改参数让hbase自己维护一个zookeeper集群。&lt;/p&gt;

&lt;h1&gt;调试hbase shell&lt;/h1&gt;

&lt;p&gt;新建一个&lt;code&gt;Debug Configuration&lt;/code&gt;,  main class 是&lt;code&gt;org.jruby.Main&lt;/code&gt;，在程序参数中添加&lt;code&gt;bin/hirb.rb&lt;/code&gt;,然后运行即可。&lt;/p&gt;

&lt;h1&gt;一些技巧&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;调试java代码的时候, byte[]的变量总是显示成数字,如果要显示对应的字符&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;text language-text&quot; data-lang=&quot;text&quot;&gt;Window-&amp;gt;Preference-&amp;gt;Java-&amp;gt;Debug-&amp;gt;Primitive Display Options-&amp;gt;Check some of them
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;hbase源码中默认依赖的是hadoop 1.0.x版本，所以mavne依赖中会引入hadoop-core-1.0.4.jar，你可以修改pom.xml文件，将默认的profile修改为你需要的hadoop版本，如2.0版本的hadoop。这样做之后，当你看HMaster的源代码时，你会很方便的关联并浏览ToolRunner类中的源代码。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;默认的profile是hadoop-1.0，配置文件如下：&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;text language-text&quot; data-lang=&quot;text&quot;&gt;  &amp;lt;!-- profile for building against Hadoop 1.0.x: This is the default. --&amp;gt;
    &amp;lt;profile&amp;gt;
      &amp;lt;id&amp;gt;hadoop-1.0&amp;lt;/id&amp;gt;
      &amp;lt;activation&amp;gt;
        &amp;lt;property&amp;gt;
          &amp;lt;name&amp;gt;!hadoop.profile&amp;lt;/name&amp;gt;
        &amp;lt;/property&amp;gt;

   &amp;lt;profile&amp;gt;
      &amp;lt;id&amp;gt;hadoop-2.0&amp;lt;/id&amp;gt;
      &amp;lt;activation&amp;gt;
        &amp;lt;property&amp;gt;
          &amp;lt;name&amp;gt;hadoop.profile&amp;lt;/name&amp;gt;
          &amp;lt;value&amp;gt;2.0&amp;lt;/value&amp;gt;
        &amp;lt;/property&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;你可以将默认的profile改为hadoop-2.0,修改之后的配置文件如下：&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;text language-text&quot; data-lang=&quot;text&quot;&gt;    &amp;lt;profile&amp;gt;
      &amp;lt;id&amp;gt;hadoop-1.0&amp;lt;/id&amp;gt;
      &amp;lt;activation&amp;gt;
        &amp;lt;property&amp;gt;
          &amp;lt;name&amp;gt;hadoop.profile&amp;lt;/name&amp;gt;
      &amp;lt;value&amp;gt;1.0&amp;lt;/value&amp;gt;
        &amp;lt;/property&amp;gt;

   &amp;lt;profile&amp;gt;
      &amp;lt;id&amp;gt;hadoop-2.0&amp;lt;/id&amp;gt;
      &amp;lt;activation&amp;gt;
        &amp;lt;property&amp;gt;
          &amp;lt;name&amp;gt;!hadoop.profile&amp;lt;/name&amp;gt;
        &amp;lt;/property&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;</description>
			<link>http://blog.javachen.com/hbase/2013/11/01/debug-hbase-in-eclipse</link>
			<guid>http://blog.javachen.com/hbase/2013/11/01/debug-hbase-in-eclipse</guid>
			<pubDate>Fri, 01 Nov 2013 00:00:00 +0800</pubDate>
		</item>

		<item>
			<title><![CDATA[编译hbase源代码并打补丁]]></title>
			<description>&lt;p&gt;写了一篇博客记录编译hbase源代码并打补丁的过程，如有不正确的，欢迎指出！&lt;/p&gt;

&lt;h1&gt;下载源代码&lt;/h1&gt;

&lt;p&gt;从&lt;a href=&quot;https://github.com/cloudera/hbase&quot;&gt;Cloudera github&lt;/a&gt;上下载最新分支源代码，例如：当前最新分支为cdh4-0.94.6_4.4.0&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;text language-text&quot; data-lang=&quot;text&quot;&gt;git clone git@github.com:cloudera/hbase.git -b cdh4-0.94.6_4.4.0 cdh4-0.94.6_4.4.0
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;说明：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;-b 指定下载哪个分支&lt;/li&gt;
&lt;li&gt;最后一个参数指定下载下来的文件名称&lt;/li&gt;
&lt;/ol&gt;

&lt;h1&gt;添加snappy压缩支持&lt;/h1&gt;

&lt;h2&gt;编译snappy&lt;/h2&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;text language-text&quot; data-lang=&quot;text&quot;&gt;svn checkout http://snappy.googlecode.com/svn/trunk/ snappy
cd snappy
sh autogen.sh
./configure 
sudo make install
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h2&gt;编译hadoop-snappy&lt;/h2&gt;

&lt;p&gt;降低gcc版本到4.4:&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;text language-text&quot; data-lang=&quot;text&quot;&gt;sudo yum install gcc-4.4
rm /usr/bin/gcc
ln -s /usr/bin/gcc-4.4 /usr/bin/gcc
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;建立libjvm软连接&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;text language-text&quot; data-lang=&quot;text&quot;&gt;sudo ln -s /usr/java/latest/jre/lib/amd64/server/libjvm.so  /usr/local/lib/
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;!-- more --&gt;

&lt;p&gt;下载并编译hadoop-snappy&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;text language-text&quot; data-lang=&quot;text&quot;&gt;svn checkout http://hadoop-snappy.googlecode.com/svn/trunk/ hadoop-snappy
cd hadoop-snappy
make package -Dsnappy.prefix=/usr/local/
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;安装jar包到本地仓库&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;text language-text&quot; data-lang=&quot;text&quot;&gt;mvn install:install-file -DgroupId=org.apache.hadoop -DartifactId=hadoop-snappy -Dversion=0.0.1-SNAPSHOT -Dpackaging=jar -Dfile=./target/hadoop-snappy-0.0.1-SNAPSHOT.jar
mvn install:install-file -DgroupId=org.apache.hadoop -DartifactId=hadoop-snappy -Dversion=0.0.1-SNAPSHOT -Dclassifier=Linux-amd64-64 -Dpackaging=tar -Dfile=./target/hadoop-snappy-0.0.1-SNAPSHOT-Linux-amd64-64.tar
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h1&gt;添加lzo压缩支持&lt;/h1&gt;

&lt;p&gt;暂不在此列出，请参考网上文章。&lt;/p&gt;

&lt;h1&gt;编译Protobuf&lt;/h1&gt;

&lt;p&gt;注意：目前只能装2.4.1版本的，装最新版本的可能会缺少文件。&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;text language-text&quot; data-lang=&quot;text&quot;&gt;wget https://protobuf.googlecode.com/files/protobuf-2.4.1.zip
unzip protobuf-2.4.1.zip
cd protobuf-2.4.1
./configure
make
sudo make install
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;测试是否安装成功，如果成功你会看到：&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;text language-text&quot; data-lang=&quot;text&quot;&gt;$ protoc
Missing input file.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;如果安装失败，你可能会看到：&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;text language-text&quot; data-lang=&quot;text&quot;&gt;$ protoc
protoc: error while loading shared libraries: libprotobuf.so.7: cannot open shared object file: No such file or directory
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h1&gt;编译hbase&lt;/h1&gt;

&lt;p&gt;进入到cdh4-0.94.6_4.4.0 目录，然后运行mvn基本命令。&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;text language-text&quot; data-lang=&quot;text&quot;&gt;cd cdh4-0.94.6_4.4.0
mvn clean install
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;忽略测试，请添加如下参数：&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;text language-text&quot; data-lang=&quot;text&quot;&gt;-DskipTests
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;添加MAVEN运行时jvm大小，请在mvn前面添加如下参数：&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;text language-text&quot; data-lang=&quot;text&quot;&gt;MAVEN_OPTS=&amp;quot;-Xmx2g&amp;quot;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;生成javadoc和文档，请添加如下参数：&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;text language-text&quot; data-lang=&quot;text&quot;&gt;javadoc:aggregate site assembly:single
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;生成release加入security和native包，请添加如下参数：&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;text language-text&quot; data-lang=&quot;text&quot;&gt;-Prelease,security,native
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;基于hadoop2.0进行编译，请添加如下参数：&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;text language-text&quot; data-lang=&quot;text&quot;&gt;-Dhadoop.profile=2.0
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;添加hadoop-snappy支持，请添加如下参数：&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;text language-text&quot; data-lang=&quot;text&quot;&gt;-Prelease,hadoop-snappy -Dhadoop-snappy.version=0.0.1-SNAPSHOT
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;如果你添加了一些java代码，在每个文件头没有添加license，则需要添加如下参数：&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;text language-text&quot; data-lang=&quot;text&quot;&gt;-Drat.numUnapprovedLicenses=200
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;综上，完整命令如下：&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;text language-text&quot; data-lang=&quot;text&quot;&gt;MAVEN_OPTS=&amp;quot;-Xmx2g&amp;quot; mvn clean install javadoc:aggregate site assembly:single -DskipTests -Prelease,security,native,hadoop-snappy -Drat.numUnapprovedLicenses=200 -Dhadoop.profile=2.0 -Dhadoop-snappy.version=0.0.1-SNAPSHOT
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h1&gt;生成patch&lt;/h1&gt;

&lt;p&gt;修改代码之后，在提交代码之前，运行如下命令生成patch：&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;text language-text&quot; data-lang=&quot;text&quot;&gt;git diff &amp;gt;../XXXXX.patch
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;如果你已经将该动文件加入到提交缓存区，即执行了如下代码：&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;text language-text&quot; data-lang=&quot;text&quot;&gt;git add .
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;你可以使用如下代码打补丁：&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;text language-text&quot; data-lang=&quot;text&quot;&gt;git diff --staged &amp;gt;../XXXXX.patch
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;如果在提交之后，想生成patch，执行如下命令：&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;text language-text&quot; data-lang=&quot;text&quot;&gt;git format-patch -1
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;code&gt;git format-patch&lt;/code&gt;的详细说明请参考：&lt;a href=&quot;http://devillived.net/forum/home.php?mod=space&amp;amp;uid=2&amp;amp;do=blog&amp;amp;id=211&quot;&gt;git patch操作&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;更多diff的命令如下：&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;text language-text&quot; data-lang=&quot;text&quot;&gt;git diff &amp;lt;file&amp;gt;   # 比较当前文件和暂存区文件差异
git diff
git diff &amp;lt;$id1&amp;gt; &amp;lt;$id2&amp;gt;    # 比较两次提交之间的差异
git diff &amp;lt;branch1&amp;gt;..&amp;lt;branch2&amp;gt;    # 在两个分支之间比较 
git diff --staged   # 比较暂存区和版本库差异
git diff --cached   # 比较暂存区和版本库差异
git diff --stat     # 仅仅比较统计信息
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h1&gt;打patch&lt;/h1&gt;

&lt;p&gt;打patch：&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;text language-text&quot; data-lang=&quot;text&quot;&gt;git apply ../XXXXX.patch
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;测试patch是否打成功：&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;text language-text&quot; data-lang=&quot;text&quot;&gt;git apply --check  ../add-aggregate-in-hbase-shell.patch
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;如果出现以下错误：&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;text language-text&quot; data-lang=&quot;text&quot;&gt;june@javachen.com /chan/workspace/hadoop/hbase $ git apply ../XXXXX.patch
fatal: git apply: bad git-diff - expected /dev/null on line 4
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;请安装dos2unix：&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;text language-text&quot; data-lang=&quot;text&quot;&gt;yum install dos2unix -y
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;然后，执行如下代码：&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;text language-text&quot; data-lang=&quot;text&quot;&gt;dos2unix ../add-aggregate-in-hbase-shell.patch
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;最后再尝试打补丁。&lt;/p&gt;

&lt;p&gt;注意：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;请注意，git apply 是一个事务性操作的命令，也就是说，要么所有补丁都打上去，要么全部放弃。&lt;/li&gt;
&lt;li&gt;对于传统的 diff 命令生成的补丁，则只能用 git apply 处理。对于 format-patch 制作的新式补丁，应当使用 git am 命令。&lt;/li&gt;
&lt;/ul&gt;

&lt;h1&gt;升级版本&lt;/h1&gt;

&lt;p&gt;当你fork了&lt;a href=&quot;https://github.com/cloudera/hbase&quot;&gt;Cloudera github&lt;/a&gt;代码之后，cloudera会继续更新代码、发布新的分支，如何将其最新的分支下载到自己的hbase仓库呢？例如，你的仓库中hbase最新分支为&lt;code&gt;cdh4-0.94.6_4.3.0&lt;/code&gt;，而cdh最新分支为&lt;code&gt;cdh4-0.94.6_4.4.0&lt;/code&gt;，现在如何将cdh上的分支下载到自己的参考呢？&lt;/p&gt;

&lt;p&gt;查看远程服务器地址和仓库名称：&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;text language-text&quot; data-lang=&quot;text&quot;&gt;june@javachen.com /chan/workspace/hadoop/hbase $ git remote -v
origin  git@github.com:javachen/hbase.git (fetch)
origin  git@github.com:javachen/hbase.git (push)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;添加远程仓库地址：&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;text language-text&quot; data-lang=&quot;text&quot;&gt;git remote add cdh git@github.com:cloudera/hbase.git
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;再一次查看远程服务器地址和仓库名称：&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;text language-text&quot; data-lang=&quot;text&quot;&gt;june@javachen.com /chan/workspace/hadoop/hbase $ git remote -v
cdh https://github.com/cloudera/hbase (fetch)
cdh https://github.com/cloudera/hbase (push)
origin  git@github.com:javachen/hbase.git (fetch)
origin  git@github.com:javachen/hbase.git (push)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;抓取远程仓库更新：&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;text language-text&quot; data-lang=&quot;text&quot;&gt;git fetch cdh
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;然后，再执行下面命令查看远程分支：&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;text language-text&quot; data-lang=&quot;text&quot;&gt;git branch -r
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;下载cdh上的分支：&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;text language-text&quot; data-lang=&quot;text&quot;&gt;git checkout cdh/cdh4-0.94.6_4.4.0
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;将其提交到自己的远程仓库：&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;text language-text&quot; data-lang=&quot;text&quot;&gt;git push origin cdh4-0.94.6_4.4.0:cdh4-0.94.6_4.4.0
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h1&gt;排错&lt;/h1&gt;

&lt;p&gt;如果在启动hbase的服务过程中出现如下日志：&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;text language-text&quot; data-lang=&quot;text&quot;&gt;2013-10-24 22:44:59,921 INFO org.apache.hadoop.hbase.util.VersionInfo: HBase Unknown
2013-10-24 22:44:59,921 INFO org.apache.hadoop.hbase.util.VersionInfo: Subversion Unknown -r Unknown
2013-10-24 22:44:59,921 INFO org.apache.hadoop.hbase.util.VersionInfo: Compiled by Unknown on Unknown
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;请查看src/saveVersion.sh文件的编码及换行符是否和你的操作系统一致。编码应该设置为UTF-8，如果你使用的是linux系统，则换行符应该为unix/linux换行符，不应该为window换行符。&lt;/p&gt;

&lt;h1&gt;参考文章&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;[1] &lt;a href=&quot;http://robbinfan.com/blog/34/git-common-command&quot;&gt;Git常用命令备忘&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;[2] &lt;a href=&quot;http://devillived.net/forum/home.php?mod=space&amp;amp;uid=2&amp;amp;do=blog&amp;amp;id=211&quot;&gt;git patch操作&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;[3] &lt;a href=&quot;http://blog.tsnrose.com/blog/2012/04/18/git-fetch/&quot;&gt;Git Fetch拉取他人分支&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;[4] &lt;a href=&quot;http://smilejay.com/2012/08/generate-a-patch-from-a-commit/&quot;&gt;git根据commit生成patch&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
			<link>http://blog.javachen.com/hbase/2013/10/28/compile-hbase-source-code-and-apply-patches</link>
			<guid>http://blog.javachen.com/hbase/2013/10/28/compile-hbase-source-code-and-apply-patches</guid>
			<pubDate>Mon, 28 Oct 2013 00:00:00 +0800</pubDate>
		</item>

		<item>
			<title><![CDATA[hive-server2中使用jdbc客户端用户运行mapreduce]]></title>
			<description>&lt;p&gt;最近做了个web系统访问hive数据库，类似于官方自带的hwi、安居客的&lt;a href=&quot;https://github.com/anjuke/hwi&quot;&gt;hwi改进版&lt;/a&gt;和大众点评的&lt;a href=&quot;http://blog.csdn.net/lalaguozhe/article/details/9614061&quot;&gt;polestar&lt;/a&gt;(&lt;a href=&quot;https://github.com/dianping/polestar&quot;&gt;github地址&lt;/a&gt;)系统，但是和他们的实现不一样，查询Hive语句走的不是cli而是通过jdbc连接hive-server2。为了实现mapreduce任务中资源按用户调度，需要hive查询自动绑定当前用户、将该用户传到yarn服务端并使mapreduce程序以该用户运行。本文主要是记录实现该功能过程中遇到的一些问题以及解决方法,如果你有更好的方法和建议，欢迎留言发表您的看法！&lt;/p&gt;

&lt;h1&gt;说明&lt;/h1&gt;

&lt;p&gt;集群环境使用的是cdh4.3，没有开启kerberos认证。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;写完这篇文章之后，在微博上收到&lt;a href=&quot;http://weibo.com/shanchao1?from=profile&amp;amp;wvr=5&amp;amp;loc=infdomain&quot;&gt;@单超eric&lt;/a&gt;的&lt;a href=&quot;http://weibo.com/1789178264/AeMItpBRk&quot;&gt;评论&lt;/a&gt;，发现cdh4.3中hive-server2已经实现&lt;a href=&quot;http://www.cloudera.com/content/cloudera-content/cloudera-docs/CDH4/latest/CDH4-Security-Guide/cdh4sg_topic_9_1.html#topic_9_1_unique_4&quot;&gt;Impersonation&lt;/a&gt;功能，再此对@单超eric的帮助表示感谢。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;so，你可以完全忽略本文后面的内容，直接看cloudera的HiveServer2 Impersonation是怎么做的。&lt;/p&gt;

&lt;h1&gt;hive-server2的启动&lt;/h1&gt;

&lt;p&gt;先从hive-server2服务的启动开始说起。&lt;/p&gt;

&lt;p&gt;如果你是以服务的方式启动hive-server2进程，则启动hive-server2的用户为hive,运行mapreduce的用户也为hive，启动脚本如下：&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;text language-text&quot; data-lang=&quot;text&quot;&gt;/etc/init.d/hive-server2 start
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;如果你以命令行方式启动hive-server2进程，则启动hive-server2的用户为root,运行mapreduce的用户也为root，启动脚本如下：&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;text language-text&quot; data-lang=&quot;text&quot;&gt;hive --service hiveserver2
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;为什么是上面的结论？这要从hive-server2的启动过程开始说明。&lt;/p&gt;

&lt;!-- more --&gt;

&lt;p&gt;查看HiveServer2.java的代码可以看到，hive-server2启动时会依次启动&lt;code&gt;cliService&lt;/code&gt;和&lt;code&gt;thriftCLIService&lt;/code&gt;，查看cliService的init()方法，可以看到如下代码：&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;text language-text&quot; data-lang=&quot;text&quot;&gt;public synchronized void init(HiveConf hiveConf) {
    this.hiveConf = hiveConf;

    sessionManager = new SessionManager();
    addService(sessionManager);
    try {
      HiveAuthFactory.loginFromKeytab(hiveConf);
      serverUserName = ShimLoader.getHadoopShims().
          getShortUserName(ShimLoader.getHadoopShims().getUGIForConf(hiveConf));
    } catch (IOException e) {
      throw new ServiceException(&amp;quot;Unable to login to kerberos with given principal/keytab&amp;quot;, e);
    } catch (LoginException e) {
      throw new ServiceException(&amp;quot;Unable to login to kerberos with given principal/keytab&amp;quot;, e);
    }
    super.init(hiveConf);
  }
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;从上面的代码可以看到在cliService初始化过程中会做登陆（从kertab中登陆）和获取用户名的操作：&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;text language-text&quot; data-lang=&quot;text&quot;&gt;ShimLoader.getHadoopShims().getUGIForConf(hiveConf)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;上面代码最终会调用HadoopShimsSecure类的getUGIForConf方法：&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;text language-text&quot; data-lang=&quot;text&quot;&gt;@Override
public UserGroupInformation getUGIForConf(Configuration conf) throws IOException {
  return UserGroupInformation.getCurrentUser();
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;UserGroupInformation.getCurrentUser()代码如下：&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;text language-text&quot; data-lang=&quot;text&quot;&gt; public synchronized
  static UserGroupInformation getCurrentUser() throws IOException {
    AccessControlContext context = AccessController.getContext();
    Subject subject = Subject.getSubject(context);
    if (subject == null || subject.getPrincipals(User.class).isEmpty()) {
      return getLoginUser();
    } else {
      return new UserGroupInformation(subject);
    }
  }
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;因为这时候服务刚启动，subject为空，故if分支会调用&lt;code&gt;getLoginUser()&lt;/code&gt;方法，其代码如下：&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;text language-text&quot; data-lang=&quot;text&quot;&gt;  public synchronized 
  static UserGroupInformation getLoginUser() throws IOException {
    if (loginUser == null) {
      try {
        Subject subject = new Subject();
        LoginContext login;
        if (isSecurityEnabled()) {
          login = newLoginContext(HadoopConfiguration.USER_KERBEROS_CONFIG_NAME,
              subject, new HadoopConfiguration());
        } else {
          login = newLoginContext(HadoopConfiguration.SIMPLE_CONFIG_NAME, 
              subject, new HadoopConfiguration());
        }
        login.login();
        loginUser = new UserGroupInformation(subject);
        loginUser.setLogin(login);
        loginUser.setAuthenticationMethod(isSecurityEnabled() ?
                                          AuthenticationMethod.KERBEROS :
                                          AuthenticationMethod.SIMPLE);
        loginUser = new UserGroupInformation(login.getSubject());
        String fileLocation = System.getenv(HADOOP_TOKEN_FILE_LOCATION);
        if (fileLocation != null) {
          // Load the token storage file and put all of the tokens into the
          // user. Don&amp;#39;t use the FileSystem API for reading since it has a lock
          // cycle (HADOOP-9212).
          Credentials cred = Credentials.readTokenStorageFile(
              new File(fileLocation), conf);
          loginUser.addCredentials(cred);
        }
        loginUser.spawnAutoRenewalThreadForUserCreds();
      } catch (LoginException le) {
        LOG.debug(&amp;quot;failure to login&amp;quot;, le);
        throw new IOException(&amp;quot;failure to login&amp;quot;, le);
      }
      if (LOG.isDebugEnabled()) {
        LOG.debug(&amp;quot;UGI loginUser:&amp;quot;+loginUser);
      }
    }
    return loginUser;
  }
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;因为是第一次调用getLoginUser(),故loginUser为空，接下来会创建LoginContext并调用其login方法，login方法最终会调用HadoopLoginModule的commit()方法。&lt;/p&gt;

&lt;p&gt;下图是从hive-server2启动到执行HadoopLoginModule的commit()方法的调用图：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2013/hive-server2-invoke.png&quot; alt=&quot;hive-server2启动过程&quot;&gt;&lt;/p&gt;

&lt;p&gt;获取登陆用户的关键代码就在commit()，逻辑如下：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;如果使用了kerberos，则为kerberos登陆用户。hive-server2中如何使用kerberos登陆，请查看官方文档。&lt;/li&gt;
&lt;li&gt;如果kerberos用户为空并且没有开启security，则从系统环境变量中取&lt;code&gt;HADOOP_USER_NAME&lt;/code&gt;的值&lt;/li&gt;
&lt;li&gt;如果环境变量中没有设置&lt;code&gt;HADOOP_USER_NAME&lt;/code&gt;，则使用系统用户，即启动hive-server2进程的用户。&lt;/li&gt;
&lt;/ul&gt;

&lt;h2&gt;小结&lt;/h2&gt;

&lt;p&gt;hive-server2启动过程中会做登陆操作并获取到登陆用户，启动之后再次调用&lt;code&gt;UserGroupInformation.getCurrentUser()&lt;/code&gt;取到的用户就为登陆用户了，这样会导致所有请求到hive-server2的hql最后都会以这个用户来运行mapreduce。&lt;/p&gt;

&lt;h1&gt;提交hive任务&lt;/h1&gt;

&lt;p&gt;现在来看hive任务是怎么提交到yarn服务端然后运行mapreduce的。&lt;/p&gt;

&lt;p&gt;为了调试简单，我在本机eclipse的hive源代码中配置&lt;code&gt;hive-site.xml、core-site.xml、mapred.xml、yarn-site.xml&lt;/code&gt;连接测试集群,添加缺少的yarn依赖并解决hive-builtins中报错的问题，然后运行HiveServer2类的main方法。&lt;em&gt;注意&lt;/em&gt;，我的电脑当前登陆用户为june，故启动hive-server2的用户为june。&lt;/p&gt;

&lt;p&gt;然后，在运行jdbc测试类，运行一个简单的sql语句，大概如下：&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;text language-text&quot; data-lang=&quot;text&quot;&gt;public static void test() {
    try {
        Class.forName(&amp;quot;org.apache.hive.jdbc.HiveDriver&amp;quot;);

        Connection conn = DriverManager.getConnection(
                &amp;quot;jdbc:hive2://june-mint:10000/default&amp;quot;, &amp;quot;&amp;quot;, &amp;quot;&amp;quot;);

        Statement stmt = conn.createStatement();

        ResultSet rs = stmt.executeQuery(&amp;quot;select count(1) from t&amp;quot;);

        while (rs.next())
            System.out.println(rs.getString(1));

        rs.close();
        stmt.close();
        conn.close();
    } catch (SQLException se) {
        se.printStackTrace();
    } catch (Exception e) {
        e.printStackTrace();
    }
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;查看yarn监控地址&lt;code&gt;http://192.168.56.101:8088/cluster&lt;/code&gt;，可以看到提交的mapreduce任务由june用户来运行。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2013/20131017-01.png&quot; alt=&quot;yarn cluster monitor page&quot;&gt;&lt;/p&gt;

&lt;p&gt;如何修改mapreduce任务的运行用户呢？如果了解hive提交mapreduce任务的过程的话，就应该知道hive任务会通过&lt;code&gt;org.apache.hadoop.mapred.JobClient&lt;/code&gt;来提交。在JobClient的init方法中有如下代码：&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;text language-text&quot; data-lang=&quot;text&quot;&gt;  public void init(JobConf conf) throws IOException {
    setConf(conf);
    cluster = new Cluster(conf);
    clientUgi = UserGroupInformation.getCurrentUser();
  }
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;JobClient类中提交mapreduce任务的代码如下，见submitJobInternal方法：&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;text language-text&quot; data-lang=&quot;text&quot;&gt;Job job = clientUgi.doAs(new PrivilegedExceptionAction&amp;lt;Job&amp;gt; () {
    @Override
    public Job run() throws IOException, ClassNotFoundException, 
      InterruptedException {
      Job job = Job.getInstance(conf);
      job.submit();
      return job;
    }
});
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;从前面知道，hive-server2启动中会进行登陆操作并且登陆用户为june，故clientUgi对应的登陆用户也为june，故提交的mapreduce任务也通过june用户来运行。&lt;/p&gt;

&lt;h1&gt;如何修改源代码&lt;/h1&gt;

&lt;p&gt;从上面代码可以知道，修改clientUgi的获取方式就可以改变提交任务的用户。UserGroupInformation中存在如下静态方法：&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;text language-text&quot; data-lang=&quot;text&quot;&gt;  public static UserGroupInformation createRemoteUser(String user) {
    if (user == null || &amp;quot;&amp;quot;.equals(user)) {
      throw new IllegalArgumentException(&amp;quot;Null user&amp;quot;);
    }
    Subject subject = new Subject();
    subject.getPrincipals().add(new User(user));
    UserGroupInformation result = new UserGroupInformation(subject);
    result.setAuthenticationMethod(AuthenticationMethod.SIMPLE);
    return result;
  }
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;故可以尝试使用该方法，修改JobClient的init方法如下：&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;text language-text&quot; data-lang=&quot;text&quot;&gt; public void init(JobConf conf) throws IOException {
    setConf(conf);
    cluster = new Cluster(conf);
    if(UserGroupInformation.isSecurityEnabled()){
        clientUgi = UserGroupInformation.getCurrentUser();
    }else{
        String user = conf.get(&amp;quot;myExecuteName&amp;quot;,&amp;quot;NoName&amp;quot;);
        clientUgi = UserGroupInformation.createRemoteUser(user);
    }
  }
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;上面代码是在没有开启security的情况下，从环境变量（myExecuteName）获取jdbc客户端指定的用户名，然后创建一个远程的UserGroupInformation。&lt;/p&gt;

&lt;h2&gt;为什么从环境变量中获取用户名称？&lt;/h2&gt;

&lt;ol&gt;
&lt;li&gt;在不考虑安全的情况下，可以由客户端任意指定用户。&lt;/li&gt;
&lt;li&gt;没有使用jdbc连接信息中的用户，是因为这样会导致每次获取jdbc连接的时候都要指定用户名，这样就没法使用已有的连接池。&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;编译代码、替换class文件，然后重新运行HiveServer2以及jdbc测试类，查看yarn监控地址&lt;code&gt;http://192.168.56.101:8088/cluster&lt;/code&gt;，截图如下：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2013/20131017-02.png&quot; alt=&quot;yarn cluster monitor page&quot;&gt;&lt;/p&gt;

&lt;p&gt;这时候mapreduce的运行用户变为NoName，这是因为从JobConf环境变量中找不到myExecuteName变量而使用默认值NoName的原因。&lt;/p&gt;

&lt;p&gt;查看hive-server2运行日志，会发现任务运行失败，关键异常信息如下：&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;text language-text&quot; data-lang=&quot;text&quot;&gt;Caused by: org.apache.hadoop.security.AccessControlException: Permission denied: user=NoName, access=WRITE, inode=&amp;quot;/tmp/hive-june/hive_2013-10-18_21-18-12_812_378750610917949668/_tmp.-ext-10001&amp;quot;:june:hadoop:drwxr-xr-x
    at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.check(FSPermissionChecker.java:224)
    at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.check(FSPermissionChecker.java:204)
    at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:149)
    at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkPermission(FSNamesystem.java:4705)
    at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkPermission(FSNamesystem.java:4687)
    at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkAncestorAccess(FSNamesystem.java:4661)
    at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.renameToInternal(FSNamesystem.java:2696)
    at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.renameToInt(FSNamesystem.java:2663)
    at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.renameTo(FSNamesystem.java:2642)
    at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.rename(NameNodeRpcServer.java:610)
    at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.rename
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;出现上述异常是因为，mapreduce任务在运行过程中会生成一些临时文件，而NoName用户对临时文件没有写的权限，这些临时文件属于june用户。查看hdfs文件如下：&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;text language-text&quot; data-lang=&quot;text&quot;&gt;[root@edh1 lib]# hadoop fs -ls /tmp/
Found 6 items
drwx------   - june hadoop          0 2013-10-15 01:33 /tmp/hadoop-yarn
drwxr-xr-x   - june hadoop          0 2013-10-16 06:52 /tmp/hive-june
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;code&gt;/tmp/hive-june&lt;/code&gt;是hive执行过程中保存在hdfs的路径，由&lt;code&gt;hive.exec.scratchdir&lt;/code&gt;定义，其默认值为&lt;code&gt;/tmp/hive-${user.name}&lt;/code&gt;，而且这个文件是在&lt;code&gt;org.apache.hadoop.hive.ql.Context&lt;/code&gt;类的构造方法中获取并在ExecDriver类的execute(DriverContext driverContext)方法中创建的。&lt;/p&gt;

&lt;p&gt;类似这样的权限问题还会出现在hdfs文件&lt;code&gt;重命名、删除临时目录的时候&lt;/code&gt;。为了避免出现这样的异常，需要修改&lt;code&gt;hive.exec.scratchdir&lt;/code&gt;为当前用户对应的临时目录路径，并使用当前登陆用户创建、重命名、删除临时目录。&lt;/p&gt;

&lt;p&gt;修改获取&lt;code&gt;hive.exec.scratchdir&lt;/code&gt;对应的临时目录代码如下，在Context类的够找方法中修改：&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;text language-text&quot; data-lang=&quot;text&quot;&gt;    String user = conf.get(myExecuteName，“”);

    if (user != null &amp;amp;&amp;amp; user.trim().length() &amp;gt; 0) {
      nonLocalScratchPath =
          new Path(&amp;quot;/tmp/hive-&amp;quot; + user, executionId);
    } else {
      nonLocalScratchPath =
          new Path(HiveConf.getVar(conf, HiveConf.ConfVars.SCRATCHDIR),
              executionId);
    }
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;找到这些操作对应的代码似乎太过复杂了，修改的地方也有很多，因为这里是使用的hive-server2，故在对应的jdbc代码中修改似乎会简单很多，例如修改HiveSessionImpl类的以下三个方法：&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;text language-text&quot; data-lang=&quot;text&quot;&gt;public OperationHandle executeStatement(String statement, Map&amp;lt;String, String&amp;gt; confOverlay) throws HiveSQLException{}

public void cancelOperation(final OperationHandle opHandle) throws HiveSQLException {}

public void closeOperation(final OperationHandle opHandle) throws HiveSQLException {}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;第一个方法是运行sql语句，第二个方法是取消运行，第三个方法是关闭连接。&lt;/p&gt;

&lt;p&gt;executeStatement中所做的修改如下，将&lt;code&gt;operation.run();&lt;/code&gt;改为：&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;text language-text&quot; data-lang=&quot;text&quot;&gt;    if (operation instanceof SQLOperation) {
        try {
          String user = hiveConf.getVar(ConfVars.HIVE_SERVER2_MAPREDUCE_USERNAME);
          ugi = UserGroupInformation.createRemoteUser(user);
          ugi.doAs(new PrivilegedExceptionAction&amp;lt;CommandProcessorResponse&amp;gt;() {
            @Override
            public CommandProcessorResponse run() throws HiveSQLException {
              operation.run();
              return null;
            }
          });
        } catch (IOException e) {
          e.printStackTrace();
        } catch (InterruptedException e) {
          e.printStackTrace();
        }
      } else {
        operation.run();
      }
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;这里添加了判断，当operation操作时，才执行下面代码，这是为了保证从hive环境变量中获取myExecuteName的值不为空时才创建UserGroupInformation。&lt;/p&gt;

&lt;p&gt;myExecuteName是新定义的hive变量，主要是用于jdbc客户端通过set语句设置myExecuteName的值为当前登陆用户名称，然后在执行sql语句。代码如下：&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;text language-text&quot; data-lang=&quot;text&quot;&gt;Statement stmt = conn.createStatement();

stmt.execute(&amp;quot;set myExecuteName=aaaa&amp;quot;);
ResultSet rs = stmt.executeQuery(&amp;quot;select count(1) from t&amp;quot;);

while (rs.next())
    System.out.println(rs.getString(1));
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h2&gt;小结&lt;/h2&gt;

&lt;p&gt;上面修改的类包括：&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;text language-text&quot; data-lang=&quot;text&quot;&gt;org.apache.hadoop.mapred.JobClient //从环境变量获取从jdbc客户端传过来的用户，即myExecuteName的值，然后以该值运行mapreduce用户
org.apache.hadoop.hive.ql.Context  //修改hive.exec.scratchdir的地址为从jdbc客户端传过来的用户对应的临时目录
org.apache.hive.service.cli.session.HiveSessionImpl //修改运行sql、取消操作、关闭连接对应的方法
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h1&gt;测试&lt;/h1&gt;

&lt;p&gt;是用javachen用户测试,hdfs上的临时目录如下：&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;text language-text&quot; data-lang=&quot;text&quot;&gt;[root@edh1 lib]# hadoop fs -ls /tmp/
Found 7 items
drwx------   - june         hadoop          0 2013-10-15 01:33 /tmp/hadoop-yarn
drwxr-xr-x   - javachen.com hadoop          0 2013-10-16 07:30 /tmp/hive-javachen.com
drwxr-xr-x   - june         hadoop          0 2013-10-16 06:52 /tmp/hive-june
drwxr-xr-x   - root         hadoop          0 2013-10-15 14:13 /tmp/hive-root
drwxrwxrwt   - yarn         mapred          0 2013-10-16 07:30 /tmp/logs
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;监控页面截图：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2013/20131017-03.png&quot; alt=&quot;yarn cluster monitor page&quot;&gt;&lt;/p&gt;

&lt;p&gt;除了简单测试之外，还需要测试修改后的代码是否影响源代码的运行以及hive cli的运行。&lt;/p&gt;

&lt;h1&gt;参考文章&lt;/h1&gt;

&lt;ol&gt;
&lt;li&gt;&lt;a href=&quot;https://cwiki.apache.org/confluence/display/Hive/Setting+up+HiveServer2&quot;&gt;HiveServer2 Impersonation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://www.cloudera.com/content/cloudera-content/cloudera-docs/CDH4/4.3.0/CDH4-Security-Guide/cdh4sg_topic_9_1.html&quot;&gt;CDH4 HiveServer2 Security Configuration&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h1&gt;Enjoy it ！&lt;/h1&gt;
</description>
			<link>http://blog.javachen.com/hive/2013/10/17/run-mapreduce-with-client-user-in-hive-server2</link>
			<guid>http://blog.javachen.com/hive/2013/10/17/run-mapreduce-with-client-user-in-hive-server2</guid>
			<pubDate>Thu, 17 Oct 2013 00:00:00 +0800</pubDate>
		</item>

		<item>
			<title><![CDATA[hive连接产生笛卡尔集]]></title>
			<description>&lt;p&gt;在使用hive过程中遇到这样的一个异常：&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;text language-text&quot; data-lang=&quot;text&quot;&gt;FAILED: ParseException line 1:18 Failed to recognize predicate &amp;#39;a&amp;#39;. Failed rule: &amp;#39;kwInner&amp;#39; in join type specifier
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;执行的hql语句如下：&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;text language-text&quot; data-lang=&quot;text&quot;&gt;[root@javachen.com ~]# hive -e &amp;#39;select a.* from t a, t b where a.id=b.id&amp;#39;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;从异常信息中很难看出出错原因，hive.log中也没有打印出详细的异常对战信息。改用jdbc连接hive-server2，可以看到hive-server2中提示如下异常信息：&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;text language-text&quot; data-lang=&quot;text&quot;&gt;13/10/17 09:57:48 ERROR ql.Driver: FAILED: ParseException line 1:18 Failed to recognize predicate &amp;#39;a&amp;#39;. Failed rule: &amp;#39;kwInner&amp;#39; in join type specifier

org.apache.hadoop.hive.ql.parse.ParseException: line 1:18 Failed to recognize predicate &amp;#39;a&amp;#39;. Failed rule: &amp;#39;kwInner&amp;#39; in join type specifier

    at org.apache.hadoop.hive.ql.parse.ParseDriver.parse(ParseDriver.java:446)
    at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:441)
    at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:349)
    at org.apache.hadoop.hive.ql.Driver.compileAndRespond(Driver.java:355)
    at org.apache.hive.service.cli.operation.SQLOperation.prepare(SQLOperation.java:95)
    at org.apache.hive.service.cli.operation.SQLOperation.prepare(SQLOperation.java:76)
    at org.apache.hive.service.cli.operation.SQLOperation.run(SQLOperation.java:114)
    at org.apache.hive.service.cli.session.HiveSessionImpl.executeStatement(HiveSessionImpl.java:194)
    at org.apache.hive.service.cli.CLIService.executeStatement(CLIService.java:155)
    at org.apache.hive.service.cli.thrift.ThriftCLIService.ExecuteStatement(ThriftCLIService.java:191)
    at org.apache.hive.service.cli.thrift.TCLIService$Processor$ExecuteStatement.getResult(TCLIService.java:1193)
    at org.apache.hive.service.cli.thrift.TCLIService$Processor$ExecuteStatement.getResult(TCLIService.java:1)
    at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)
    at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39)
    at org.apache.hive.service.cli.thrift.TSetIpAddressProcessor.process(TSetIpAddressProcessor.java:38)
    at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:206)
    at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    at java.lang.Thread.run(Thread.java:662)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;!-- more --&gt;

&lt;p&gt;从异常信息可以看到是在编译hql语句进行语法解析时出现了错误，到底为什么会出现&lt;code&gt;Failed rule: &amp;#39;kwInner&amp;#39; in join type specifier&lt;/code&gt;这样的异常信息呢？&lt;/p&gt;

&lt;p&gt;在eclipse中查找关键字并没有找到相应代码，在&lt;a href=&quot;http://svn.apache.org/repos/asf/hive/tags/release-0.10.0/ql/src/java/org/apache/hadoop/hive/ql/parse/Hive.g&quot;&gt;Hive.g&lt;/a&gt; 中查找关键字“kwInner”，可以看到如下内容：&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;text language-text&quot; data-lang=&quot;text&quot;&gt;joinToken
@init { msgs.push(&amp;quot;join type specifier&amp;quot;); }
@after { msgs.pop(); }
    :
      KW_JOIN                     -&amp;gt; TOK_JOIN
    | kwInner  KW_JOIN            -&amp;gt; TOK_JOIN
    | KW_CROSS KW_JOIN            -&amp;gt; TOK_CROSSJOIN
    | KW_LEFT  KW_OUTER KW_JOIN   -&amp;gt; TOK_LEFTOUTERJOIN
    | KW_RIGHT KW_OUTER KW_JOIN   -&amp;gt; TOK_RIGHTOUTERJOIN
    | KW_FULL  KW_OUTER KW_JOIN   -&amp;gt; TOK_FULLOUTERJOIN
    | KW_LEFT  KW_SEMI  KW_JOIN   -&amp;gt; TOK_LEFTSEMIJOIN
    ;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;从上面可以看出hive支持的连接包括：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;join&lt;/li&gt;
&lt;li&gt;inner join&lt;/li&gt;
&lt;li&gt;cross join (as of Hive 0.10)&lt;/li&gt;
&lt;li&gt;left outer join&lt;/li&gt;
&lt;li&gt;right outer join&lt;/li&gt;
&lt;li&gt;full outer join&lt;/li&gt;
&lt;li&gt;left semi join&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;kwInner为什么是小写呢，其含义是什么呢？搜索关键字，找到如下代码：&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;text language-text&quot; data-lang=&quot;text&quot;&gt;kwInner
:
{input.LT(1).getText().equalsIgnoreCase(&amp;quot;inner&amp;quot;)}? Identifier;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;上面的大概意思是找到输入左边的内容并判断其值在忽略大小写情况下是否等于inner，大概意思是hql语句中缺少inner关键字吧？修改下hql语句如下，然后执行：&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;text language-text&quot; data-lang=&quot;text&quot;&gt;[root@javachen.com ~]#  hive -e &amp;#39;select a.* from t a inner join t b where a.id=b.id&amp;#39;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;修改后的hql语句能够正常运行，并且变成了内连接。&lt;code&gt;在JION接连查询中没有ON连接key而通过WHERE条件语句会产生笛卡尔集。&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Hive本身是不支持笛卡尔集的，不能用&lt;code&gt;select T1.*, T2.* from table1, table2&lt;/code&gt;这种语法。但有时候确实需要用到笛卡尔集的时候，可以用下面的语法来实现同样的效果：&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;text language-text&quot; data-lang=&quot;text&quot;&gt;select T1.*, T2.* from table1 T1 join table2 T2 where 1=1;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;注意在Hive的Strict模式下不能用这种语法，因为这样会产生笛卡尔集，而这种模式禁止产生笛卡尔集。需要先用&lt;code&gt;set hive.mapred.mode=nonstrict;&lt;/code&gt;设为非strict模式就可以用了，或者将where改为on连接。&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;text language-text&quot; data-lang=&quot;text&quot;&gt;select T1.*, T2.* from table1 T1 join table2 T2 on  T1.id=T2.id;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h1&gt;关于Strict Mode&lt;/h1&gt;

&lt;p&gt;Hive中的严格模式可以防止用户发出（可以有问题）的查询无意中造成不良的影响。 将&lt;code&gt;hive.mapred.mode&lt;/code&gt;设置成strict可以禁止三种类型的查询： &lt;/p&gt;

&lt;p&gt;1）、在一个分区表上，如果没有在WHERE条件中指明具体的分区，那么这是不允许的，换句话说，不允许在分区表上全表扫描。这种限制的原因是分区表通常会持非常大的数据集并且可能数据增长迅速，对这样的一个大表做全表扫描会消耗大量资源，必须要再WHERE过滤条件中具体指明分区才可以执行成功的查询。&lt;/p&gt;

&lt;p&gt;2）、第二种是禁止执行有ORDER BY的排序要求但没有LIMIT语句的HiveQL查询。因为ORDER BY全局查询会导致有一个单一的reducer对所有的查询结果排序，如果对大数据集做排序，这将导致不可预期的执行时间，必须要加上limit条件才可以执行成功的查询。&lt;/p&gt;

&lt;p&gt;3）、第三种是禁止产生笛卡尔集。在JION接连查询中没有ON连接key而通过WHERE条件语句会产生笛卡尔集，需要改为JOIN...ON语句。&lt;/p&gt;

&lt;h1&gt;参考文章&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;[1] &lt;a href=&quot;http://flyingdutchman.iteye.com/blog/1871983&quot;&gt;深入学习《Programing Hive》：Tuning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;[2] &lt;a href=&quot;http://blog.hesey.net/2012/04/hive-tips.html&quot;&gt;Hive Tips&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
			<link>http://blog.javachen.com/hive/2013/10/17/cartesian-product-in-hive-inner-join</link>
			<guid>http://blog.javachen.com/hive/2013/10/17/cartesian-product-in-hive-inner-join</guid>
			<pubDate>Thu, 17 Oct 2013 00:00:00 +0800</pubDate>
		</item>

		<item>
			<title><![CDATA[最近的工作]]></title>
			<description>&lt;p&gt;最近一直在构思这篇博客的内容，到现在还是不知道从何下手。自从将博客从wordpress迁移到github上之后，就很少在博客写一些关于工作和生活的文章，所以想写一篇关于工作的博客，记录最近做过的事情以及一些当时的所思所想。&lt;/p&gt;

&lt;p&gt;最近半年多一直在做hadoop方面的工作，也就是接触hadoop才半年多时间。最开始接触hadoop是去年的11月21日，那天去Intel公司参加了两天的hadoop培训。培训的内容很多干货也有很多枯燥的东西，所以边听边瞌睡的听完了两天的培训内容。培训的ppt打印出来了，时不时地会翻看上面讲述的内容，然后在网上搜索些相关的资料。&lt;/p&gt;

&lt;p&gt;最先接触的hadoop发行版是Intel的IDH，刚开始使用IDH也就是用他的管理界面安装、部署hadoop集群，然后在8节点的集群上作hive两表关联的测试。测试结果不是很满意，但是对IDH倒是印象深刻。IDH的前端使用GWT开发，界面简洁，操作也比较方便，只是同步配置文件有时候非常慢，要等上一杯咖啡的时间。&lt;/p&gt;

&lt;!-- more --&gt;

&lt;p&gt;IDH的源代码不开源，所以遇到一些问题的时候，只能先自己摸索。我不喜欢闭源也不喜欢许可证以及混淆代码什么的。虽然java代码的混淆了，但是shell和puppet脚本还是能够很容易读懂的。参考IDH的部署安装脚本，我在试着用shell编写一个hadoop的安装部署脚本，这个工作还在慢慢进行中。也许在弄懂puppet的原理和代码之后，我会简化、改进IDH的脚本；也许会使用公司使用的saltstack来完善这个部署脚本。&lt;/p&gt;

&lt;p&gt;IDH实现了hive over hbase，这是一个很不错的特性，其基于hbase的协作器，代码实现并不复杂。IDH中有个hmon用于监控hadoop，我已经把这部分代码反编译了。&lt;/p&gt;

&lt;p&gt;在hadoop版本选择的过程中，体验了Cloudera的CDH。首先是CDH的压缩包手动安装hadoop集群，一点点的修改配置文件，直到最后集群能够成功启动，这种方式安装的hadoop集群不包含本地lib文件；然后又试着解压缩Cloudera-manager.bin文件，尝试在虚拟机中不连接网络的情况下通过Cloudera-manager来安装配置集群，在使用了一段时间之后，发现Cloudera-manager没有使用操作系统的service服务来管理hadoop组件的启动和停止，而是有自己一套实现来管理集群，再加上Cloudera-manager也不开源，故放弃了使用Cloudera-manager来安装集群的方式；最后，最后是使用rpm方式来安装hadoop，安装过程倒是不复杂，只是以后如果自己修改了源代码时候升级不是很方便了，Cloudera的github上有个cdh-package项目，这个其实就是apache的bigtop项目，试过通过这个来编译出hadoop的rpm包，没有成功。&lt;/p&gt;

&lt;p&gt;IDH通过本地文件来保存集群的配置信息，而CDH将这些信息保存到数据库了。CDH的Cloudera-manager的web界面基于bootstrap和jquery插件，ui做的很不错，通过反编译java代码，已经知道了其web界面的实现方式以及编译成功了部分java代码。CDH的Cloudera-manager和IDH-manager也很大不同，我想在这两个的基础上也实现一个hadoop的manager，这是我个人想法，还需要研究、整理出他们的实现思路，然后一点点的构思自己该怎么做。Cloudera-manager免费版简称CMF。&lt;/p&gt;

&lt;p&gt;差不多两个月前，公司想做个hive的查询界面，这个东西其实就是和hwi、hue差不多的个东西。基于Spring，我很快搭好了框架，然后把CMF其中的css和js都迁移过来了。这是我第一次接触bootstrap，稍微修改下代码一个前台框架就弄好了，CMF最主要是使用了require.js使javascript代码模块化，这东西改起来也很简单，我把CMF中大部分基础javascript代码都移到了我搭好的框架中。搭好这个框架没花多少时间，但后来公司不打算使用这一套框架，以后有时间我会继续基于这个框架开发个hadoop的管理界面。&lt;/p&gt;

&lt;p&gt;在使用ganglia监控hadoop的过程中，有时候需要找一个监控指标需要花好长时间，而且一个页面上显示的指标太多的时候，这个页面会反应不过来。hortonworks的HDP发行版中似乎对ganglia做了些修改，具体不知道改了什么。HDP发行版没有使用和研究过，只是下载了1.3和2.0两个版本的VM，然后看了看其中的hue，觉得做的很不错，只可惜是python写的。&lt;/p&gt;

&lt;p&gt;在使用hadoop的过程中，觉得hadoop的门槛对于用户来说还是有点高。用户需要学习hive语法，写出的sql语句通常都不是最优sql，如果有个sql优化器自动帮用户优化sql语句就好了。hbase用于监控业务日志，数据建模和编写代码查询数据对于业务人员来说难度也太大了，如果能够适度封装，让业务人员不用关心hadoop的细节，只需要编写sql语句就能查询数据该多好啊！&lt;/p&gt;

&lt;p&gt;其他工作：hive和mapreduce调优。&lt;/p&gt;

&lt;p&gt;上面是最近在做的一些事情，包括暂停没做的、正在做的以及还未做的。有时候觉得有些事情很简单，但没有时间、没有精力也没有自由一下子做完，有些时候人在江湖，身不由己。&lt;/p&gt;
</description>
			<link>http://blog.javachen.com/work/2013/09/08/recent-work</link>
			<guid>http://blog.javachen.com/work/2013/09/08/recent-work</guid>
			<pubDate>Sun, 08 Sep 2013 00:00:00 +0800</pubDate>
		</item>

		<item>
			<title><![CDATA[hive中如何确定map数]]></title>
			<description>&lt;p&gt;hive是基于Hadoop的一个数据仓库工具，可以将结构化的数据文件映射为一张数据库表，并提供完整的sql查询功能，可以将sql语句转换为MapReduce任务进行运行。当运行一个hql语句的时候，map数是如何计算出来的呢？有哪些方法可以调整map数呢？&lt;/p&gt;

&lt;!-- more --&gt;

&lt;h1&gt;hive默认的input format&lt;/h1&gt;

&lt;p&gt;在&lt;code&gt;cdh-4.3.0&lt;/code&gt;的hive中查看&lt;code&gt;hive.input.format&lt;/code&gt;值（为什么是&lt;code&gt;hive.input.format&lt;/code&gt;？）：&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;text language-text&quot; data-lang=&quot;text&quot;&gt;hive&amp;gt; set hive.input.format;
hive.input.format=org.apache.hadoop.hive.ql.io.CombineHiveInputFormat;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;可以看到默认值为CombineHiveInputFormat，如果你使用的是&lt;code&gt;IDH&lt;/code&gt;的hive，则默认值为：&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;text language-text&quot; data-lang=&quot;text&quot;&gt;hive&amp;gt; set hive.input.format;
hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;CombineHiveInputFormat类继承自HiveInputFormat，而HiveInputFormat实现了org.apache.hadoop.mapred.InputFormat接口，关于InputFormat的分析，可以参考&lt;a href=&quot;http://flyingdutchman.iteye.com/blog/1876400&quot;&gt;Hadoop深入学习：InputFormat组件&lt;/a&gt;.&lt;/p&gt;

&lt;h1&gt;InputFormat接口功能&lt;/h1&gt;

&lt;p&gt;简单来说，InputFormat主要用于描述输入数据的格式，提供了以下两个功能： &lt;/p&gt;

&lt;p&gt;1)、数据切分，按照某个策略将输入数据且分成若干个split，以便确定Map Task的个数即Mapper的个数，在MapReduce框架中，一个split就意味着需要一个Map Task; &lt;/p&gt;

&lt;p&gt;2)、为Mapper提供输入数据，即给定一个split(使用其中的RecordReader对象)将之解析为一个个的key/value键值对。 &lt;/p&gt;

&lt;p&gt;该类接口定义如下：&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;text language-text&quot; data-lang=&quot;text&quot;&gt;public interface InputFormat&amp;lt;K,V&amp;gt;{
    public InputSplit[] getSplits(JobConf job,int numSplits) throws IOException; 
    public RecordReader&amp;lt;K,V&amp;gt; getRecordReader(InputSplit split,JobConf job,Reporter reporter) throws IOException; 
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;其中，getSplit()方法主要用于切分数据，每一份数据由，split只是在逻辑上对数据分片，并不会在磁盘上将数据切分成split物理分片，实际上数据在HDFS上还是以block为基本单位来存储数据的。InputSplit只记录了Mapper要处理的数据的元数据信息，如起始位置、长度和所在的节点。&lt;/p&gt;

&lt;p&gt;MapReduce自带了一些InputFormat的实现类： &lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://dl2.iteye.com/upload/attachment/0085/0423/fa2e8c9f-f26a-3184-98e7-277c1b56fda1.jpg&quot; alt=&quot;InputFormat实现类&quot;&gt;&lt;/p&gt;

&lt;p&gt;hive中有一些InputFormat的实现类，如：&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;text language-text&quot; data-lang=&quot;text&quot;&gt;AvroContainerInputFormat
RCFileBlockMergeInputFormat
RCFileInputFormat
FlatFileInputFormat
OneNullRowInputFormat
ReworkMapredInputFormat
SymbolicInputFormat
SymlinkTextInputFormat
HiveInputFormat
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;HiveInputFormat的子类有：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2013/implement-of-hiveinputformat.png&quot; alt=&quot;HiveInputFormat的子类&quot;&gt;&lt;/p&gt;

&lt;h1&gt;HiveInputFormat&lt;/h1&gt;

&lt;p&gt;以HiveInputFormat为例，看看其getSplit()方法逻辑：&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;text language-text&quot; data-lang=&quot;text&quot;&gt;for (Path dir : dirs) {
  PartitionDesc part = getPartitionDescFromPath(pathToPartitionInfo, dir);
  // create a new InputFormat instance if this is the first time to see this
  // class
  Class inputFormatClass = part.getInputFileFormatClass();
  InputFormat inputFormat = getInputFormatFromCache(inputFormatClass, job);
  Utilities.copyTableJobPropertiesToConf(part.getTableDesc(), newjob);

  // Make filter pushdown information available to getSplits.
  ArrayList&amp;lt;String&amp;gt; aliases =
      mrwork.getPathToAliases().get(dir.toUri().toString());
  if ((aliases != null) &amp;amp;&amp;amp; (aliases.size() == 1)) {
    Operator op = mrwork.getAliasToWork().get(aliases.get(0));
    if ((op != null) &amp;amp;&amp;amp; (op instanceof TableScanOperator)) {
      TableScanOperator tableScan = (TableScanOperator) op;
      pushFilters(newjob, tableScan);
    }
  }

  FileInputFormat.setInputPaths(newjob, dir);
  newjob.setInputFormat(inputFormat.getClass());
  InputSplit[] iss = inputFormat.getSplits(newjob, numSplits / dirs.length);
  for (InputSplit is : iss) {
    result.add(new HiveInputSplit(is, inputFormatClass.getName()));
  }
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;上面代码主要过程是：&lt;/p&gt;

&lt;blockquote&gt;遍历每个输入目录，然后获得PartitionDesc对象，从该对象调用getInputFileFormatClass方法得到实际的InputFormat类，并调用其`getSplits(newjob, numSplits / dirs.length)`方法。
&lt;/blockquote&gt;

&lt;p&gt;按照上面代码逻辑，似乎hive中每一个表都应该有一个InputFormat实现类。在hive中运行下面代码，可以查看建表语句：&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;text language-text&quot; data-lang=&quot;text&quot;&gt;hive&amp;gt; show create table info; 
OK
CREATE  TABLE info(
  statist_date string, 
  statistics_date string, 
  inner_code string, 
  office_no string, 
  window_no string, 
  ticket_no string, 
  id_kind string, 
  id_no string, 
  id_name string, 
  area_center_code string)
ROW FORMAT DELIMITED 
  FIELDS TERMINATED BY &amp;#39;\;&amp;#39; 
  LINES TERMINATED BY &amp;#39;\n&amp;#39; 
STORED AS INPUTFORMAT 
  &amp;#39;org.apache.hadoop.mapred.TextInputFormat&amp;#39; 
OUTPUTFORMAT 
  &amp;#39;org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat&amp;#39;
LOCATION
  &amp;#39;hdfs://node:8020/user/hive/warehouse/info&amp;#39;
TBLPROPERTIES (
  &amp;#39;numPartitions&amp;#39;=&amp;#39;0&amp;#39;, 
  &amp;#39;numFiles&amp;#39;=&amp;#39;1&amp;#39;, 
  &amp;#39;transient_lastDdlTime&amp;#39;=&amp;#39;1378245263&amp;#39;, 
  &amp;#39;numRows&amp;#39;=&amp;#39;0&amp;#39;, 
  &amp;#39;totalSize&amp;#39;=&amp;#39;301240320&amp;#39;, 
  &amp;#39;rawDataSize&amp;#39;=&amp;#39;0&amp;#39;)
Time taken: 0.497 seconds
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;从上面可以看到info表的INPUTFORMAT为&lt;code&gt;org.apache.hadoop.mapred.TextInputFormat&lt;/code&gt;，TextInputFormat继承自FileInputFormat。FileInputFormat是一个抽象类，它最重要的功能是为各种InputFormat提供统一的getSplits()方法，该方法最核心的是文件切分算法和Host选择算法。&lt;/p&gt;

&lt;p&gt;算法如下：&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;text language-text&quot; data-lang=&quot;text&quot;&gt;long length = file.getLen();
long goalSize = totalSize / (numSplits == 0 ? 1 : numSplits);
long minSize = Math.max(job.getLong(org.apache.hadoop.mapreduce.lib.input.
FileInputFormat.SPLIT_MINSIZE, 1), minSplitSize);

long blockSize = file.getBlockSize();
long splitSize = computeSplitSize(goalSize, minSize, blockSize);
long bytesRemaining = length;
while (((double) bytesRemaining)/splitSize &amp;gt; SPLIT_SLOP) {
String[] splitHosts = getSplitHosts(blkLocations, 
    length-bytesRemaining, splitSize, clusterMap);
    splits.add(makeSplit(path, length-bytesRemaining, splitSize, 
               splitHosts));
    bytesRemaining -= splitSize;
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;hr&gt;

&lt;p&gt;&lt;code&gt;华丽的分割线&lt;/code&gt;：以下摘抄自&lt;a href=&quot;http://flyingdutchman.iteye.com/blog/1876400&quot;&gt;Hadoop深入学习：InputFormat组件&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;1）、文件切分算法&lt;/strong&gt; &lt;/p&gt;

&lt;p&gt;文件切分算法主要用于确定InputSplit的个数以及每个InputSplit对应的数据段，FileInputSplit以文件为单位切分生成InputSplit。有三个属性值来确定InputSplit的个数：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;goalSize：该值由totalSize/numSplits来确定InputSplit的长度，它是根据用户的期望的InputSplit个数计算出来的；numSplits为用户设定的Map Task的个数，默认为1。 &lt;/li&gt;
&lt;li&gt;minSize：由配置参数mapred.min.split.size（或者 &lt;code&gt;mapreduce.input.fileinputformat.split.minsize&lt;/code&gt;）决定的InputFormat的最小长度，默认为1。 &lt;/li&gt;
&lt;li&gt;blockSize：HDFS中的文件存储块block的大小，默认为64MB。&lt;/li&gt;
&lt;li&gt;numSplits=&lt;code&gt;mapred.map.tasks&lt;/code&gt; 或者 &lt;code&gt;mapreduce.job.maps&lt;/code&gt; &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;这三个参数决定一个InputFormat分片的最终的长度，计算方法如下： &lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;text language-text&quot; data-lang=&quot;text&quot;&gt;splitSize = max{minSize,min{goalSize,blockSize}} 
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;计算出了分片的长度后，也就确定了InputFormat的数目。 &lt;/p&gt;

&lt;p&gt;&lt;strong&gt;2）、host选择算法&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;InputFormat的切分方案确定后，接下来就是要确定每一个InputSplit的元数据信息。InputSplit元数据通常包括四部分，&lt;code&gt;&amp;lt;file,start,length,hosts&amp;gt;&lt;/code&gt;其意义为： &lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;file标识InputSplit分片所在的文件； &lt;/li&gt;
&lt;li&gt;InputSplit分片在文件中的的起始位置； &lt;/li&gt;
&lt;li&gt;InputSplit分片的长度； &lt;/li&gt;
&lt;li&gt;分片所在的host节点的列表。 &lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
InputSplit的host列表的算作策略直接影响到运行作业的本地性。&lt;br/&gt;

我们知道，由于大文件存储在HDFS上的block可能会遍布整个Hadoop集群，而一个InputSplit分片的划分算法可能会导致一个split分片对应多个不在同一个节点上的blocks，这就会使得在Map Task执行过程中会涉及到读其他节点上的属于该Task的block中的数据，从而不能实现数据本地性，而造成更多的网络传输开销。&lt;br/&gt;
 
一个InputSplit分片对应的blocks可能位于多个数据节点地上，但是基于任务调度的效率，通常情况下，不会把一个分片涉及的所有的节点信息都加到其host列表中，而是选择包含该分片的数据总量的最大的前几个节点，作为任务调度时判断是否具有本地性的主要凭证。&lt;br/&gt;
 
FileInputFormat使用了一个启发式的host选择算法：首先按照rack机架包含的数据量对rack排序，然后再在rack内部按照每个node节点包含的数据量对node排序，最后选取前N个(N为block的副本数)node的host作为InputSplit分片的host列表。当任务地调度Task作业时，只要将Task调度给host列表上的节点，就可以认为该Task满足了本地性。 &lt;br/&gt;

从上面的信息我们可以知道，当InputSplit分片的大小大于block的大小时，Map Task并不能完全满足数据的本地性，总有一本分的数据要通过网络从远程节点上读数据，故为了提高Map Task的数据本地性，减少网络传输的开销，应尽量是InputFormat的大小和HDFS的block块大小相同。
&lt;/blockquote&gt;

&lt;hr&gt;

&lt;h1&gt;CombineHiveInputFormat&lt;/h1&gt;

&lt;p&gt;&lt;code&gt;getSplits(JobConf job, int numSplits)&lt;/code&gt;代码运行过程如下：&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;text language-text&quot; data-lang=&quot;text&quot;&gt;init(job);
CombineFileInputFormatShim combine = ShimLoader.getHadoopShims().getCombineFileInputFormat();
    ShimLoader.loadShims(HADOOP_SHIM_CLASSES, HadoopShims.class);
        Hadoop23Shims
            HadoopShimsSecure.getCombineFileInputFormat()
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;CombineFileInputFormatShim继承了&lt;code&gt;org.apache.hadoop.mapred.lib.CombineFileInputFormat&lt;/code&gt;,CombineFileInputFormatShim的getSplits方法代码如下：&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;text language-text&quot; data-lang=&quot;text&quot;&gt;public InputSplitShim[] getSplits(JobConf job, int numSplits) throws IOException {
  long minSize = job.getLong(&amp;quot;mapred.min.split.size&amp;quot;, 0);

  // For backward compatibility, let the above parameter be used
  if (job.getLong(&amp;quot;mapred.min.split.size.per.node&amp;quot;, 0) == 0) {
    super.setMinSplitSizeNode(minSize);
  }

  if (job.getLong(&amp;quot;mapred.min.split.size.per.rack&amp;quot;, 0) == 0) {
    super.setMinSplitSizeRack(minSize);
  }

  if (job.getLong(&amp;quot;mapred.max.split.size&amp;quot;, 0) == 0) {
    super.setMaxSplitSize(minSize);
  }

  InputSplit[] splits = (InputSplit[]) super.getSplits(job, numSplits);

  InputSplitShim[] isplits = new InputSplitShim[splits.length];
  for (int pos = 0; pos &amp;lt; splits.length; pos++) {
    isplits[pos] = new InputSplitShim((CombineFileSplit)splits[pos]);
  }

  return isplits;
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;从上面代码可以看出，如果为CombineHiveInputFormat，则以下四个参数起作用：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;mapred.min.split.size 或者 mapreduce.input.fileinputformat.split.minsize&lt;/li&gt;
&lt;li&gt;mapred.max.split.size 或者 mapreduce.input.fileinputformat.split.maxsize&lt;/li&gt;
&lt;li&gt;mapred.min.split.size.per.rack 或者 mapreduce.input.fileinputformat.split.minsize.per.rack&lt;/li&gt;
&lt;li&gt;mapred.min.split.size.per.node 或者 mapreduce.input.fileinputformat.split.minsize.per.node&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;CombineFileInputFormatShim的getSplits方法最终会调用父类的getSplits方法，拆分算法如下：&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;text language-text&quot; data-lang=&quot;text&quot;&gt;long left = locations[i].getLength();
long myOffset = locations[i].getOffset();
long myLength = 0;
do {
    if (maxSize == 0) {
        myLength = left;
    } else {
    if (left &amp;gt; maxSize &amp;amp;&amp;amp; left &amp;lt; 2 * maxSize) {
      myLength = left / 2;
    } else {
      myLength = Math.min(maxSize, left);
    }
    }
    OneBlockInfo oneblock = new OneBlockInfo(path, myOffset,
      myLength, locations[i].getHosts(), locations[i]
          .getTopologyPaths());
    left -= myLength;
    myOffset += myLength;

    blocksList.add(oneblock);
} while (left &amp;gt; 0);
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h1&gt;hive中如何确定map数&lt;/h1&gt;

&lt;p&gt;总上总结如下：&lt;/p&gt;

&lt;p&gt;如果&lt;code&gt;hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat&lt;/code&gt;，则这时候的参数如下：&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;text language-text&quot; data-lang=&quot;text&quot;&gt;hive&amp;gt; set mapred.min.split.size;
mapred.min.split.size=1
hive&amp;gt; set mapred.map.tasks;
mapred.map.tasks=2
hive&amp;gt; set dfs.blocksize;
dfs.blocksize=134217728
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;上面参数中mapred.map.tasks为2，dfs.blocksize（使用的是cdh-4.3.0版本的hadoop，这里block和size之间没有逗号）为128M。&lt;/p&gt;

&lt;p&gt;假设有一个文件为200M，则按上面HiveInputFormat的split算法：&lt;/p&gt;

&lt;p&gt;1、文件总大小为200M，goalSize=200M /2 =100M，minSize=1 ，splitSize = max{1,min{100M,128M}} =100M&lt;/p&gt;

&lt;p&gt;2、200M / 100M &amp;gt;1.1,故第一块大小为100M&lt;/p&gt;

&lt;p&gt;3、剩下文件大小为100M，小于128M，故第二块大小为100M。&lt;/p&gt;

&lt;p&gt;如果&lt;code&gt;hive.input.format=org.apache.hadoop.hive.ql.io.CombineHiveInputFormat&lt;/code&gt;，则这时候的参数如下：&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;text language-text&quot; data-lang=&quot;text&quot;&gt;hive&amp;gt; set mapred.min.split.size;
mapred.min.split.size=1
hive&amp;gt; set mapred.max.split.size;
mapred.max.split.size=67108864
hive&amp;gt; set mapred.min.split.size.per.rack;
mapred.min.split.size.per.rack=1
hive&amp;gt; set mapred.min.split.size.per.node;
mapred.min.split.size.per.node=1
hive&amp;gt; set dfs.blocksize;
dfs.blocksize=134217728
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;上面参数中mapred.max.split.size为64M，dfs.blocksize（使用的是cdh-4.3.0版本的hadoop，这里block和size之间没有逗号）为128M。&lt;/p&gt;

&lt;p&gt;假设有一个文件为200M，则按上面CombineHiveInputFormat的split算法：&lt;/p&gt;

&lt;p&gt;1、128M &amp;lt; 200M &amp;lt;128M X 2，故第一个block大小为128M&lt;/p&gt;

&lt;p&gt;2、剩下文件大小为200M-128M=72M，72M &amp;lt; 128M,故第二块大小为72M&lt;/p&gt;

&lt;h1&gt;总结&lt;/h1&gt;

&lt;p&gt;网上有一些文章关于hive中如何控制map数的文章是否考虑的不够全面，没有具体情况具体分析。简而言之，当InputFormat的实现类为不同类时，拆分块算法都不一样，相关设置参数也不一样，需要具体分析。&lt;/p&gt;

&lt;h2&gt;1. map数不是越多越好&lt;/h2&gt;

&lt;p&gt;如果一个任务有很多小文件（远远小于块大小128m）,则每个小文件也会被当做一个块，用一个map任务来完成，而一个map任务启动和初始化的时间远远大于逻辑处理的时间，就会造成很大的资源浪费。
而且，同时可执行的map数是受限的。&lt;/p&gt;

&lt;h2&gt;2. 如何适当的增加map数？&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;将数据导入到hive前，手动将大文件拆分为小文件&lt;/li&gt;
&lt;li&gt;指定map数，使用insert或者create as select语句将一个表导入到另一个表，然后对另一张表做查询&lt;/li&gt;
&lt;/ul&gt;

&lt;h2&gt;3. 一些经验&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;合并小文件可以减少map数，但是会增加网络IO。&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;尽量使拆分块大小和hdfs的块大小接近，避免一个拆分块大小上的多个hdfs块位于不同数据节点，从而降低网络IO。&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;根据实际情况，控制map数量需要遵循两个原则：&lt;code&gt;使大数据量利用合适的map数&lt;/code&gt;；&lt;code&gt;使单个map任务处理合适的数据量。&lt;/code&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h1&gt;参考文章&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;[1] &lt;a href=&quot;http://f.dataguru.cn/thread-149820-1-1.html&quot;&gt;【hive】hive的查询注意事项以及优化总结&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;[2] &lt;a href=&quot;http://blog.sina.com.cn/s/blog_6ff05a2c010178qd.html&quot;&gt;Hadoop中map数的计算&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;[3] &lt;a href=&quot;http://blog.sina.com.cn/s/blog_6ff05a2c0101aqvv.html&quot;&gt;[Hive]从一个经典案例看优化mapred.map.tasks的重要性&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;[4] &lt;a href=&quot;http://superlxw1234.iteye.com/blog/1582880&quot;&gt;hive优化之------控制hive任务中的map数和reduce数&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;[5] &lt;a href=&quot;http://www.searchtb.com/2010/12/hadoop-job-tuning.html&quot;&gt;Hadoop Job Tuning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;[6] &lt;a href=&quot;http://www.tuicool.com/articles/77f2Af&quot;&gt;Hive配置项的含义详解（2）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;[7] &lt;a href=&quot;http://blog.csdn.net/lalaguozhe/article/details/9053645&quot;&gt;Hive小文件合并调研&lt;/a&gt;
&lt;a href=&quot;http://flyingdutchman.iteye.com/blog/1876400&quot;&gt;Hadoop深入学习：InputFormat组件&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
			<link>http://blog.javachen.com/hive/2013/09/04/how-to-decide-map-number</link>
			<guid>http://blog.javachen.com/hive/2013/09/04/how-to-decide-map-number</guid>
			<pubDate>Wed, 04 Sep 2013 00:00:00 +0800</pubDate>
		</item>

		<item>
			<title><![CDATA[我的jekyll配置和修改]]></title>
			<description>&lt;p&gt;主要记录使用jekyll搭建博客时的一些配置和修改。&lt;/p&gt;

&lt;p&gt;注意：&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;使用时请删除{和%以及{和{之间的空格。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h1&gt;预览文章&lt;/h1&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;text language-text&quot; data-lang=&quot;text&quot;&gt;source ~/.bash_profile
jekyll server
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h1&gt;添加about me 边栏&lt;/h1&gt;

&lt;p&gt;参考&lt;a href=&quot;http://www.the5fire.com/&quot;&gt;the5fire的技术博客&lt;/a&gt;在index.html页面加入如下代码：&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;text language-text&quot; data-lang=&quot;text&quot;&gt;&amp;lt;section&amp;gt;
&amp;lt;h4&amp;gt;About me&amp;lt;/h4&amp;gt;
&amp;lt;div&amp;gt;
 一个Java方案架构师，主要从事hadoop相关工作。&amp;lt;a href=&amp;quot;/about.html&amp;quot;&amp;gt;更多信息&amp;lt;/a&amp;gt; 
&amp;lt;br/&amp;gt;
&amp;lt;br/&amp;gt;
&amp;lt;strong&amp;gt;&amp;lt;font color=&amp;quot;red&amp;quot;&amp;gt;&amp;lt;a href=&amp;quot;/atom.xml&amp;quot; target=&amp;quot;_blank&amp;quot;&amp;gt;订阅本站&amp;lt;/a&amp;gt;&amp;lt;/font&amp;gt;&amp;lt;/strong&amp;gt;
&amp;lt;br/&amp;gt;&amp;lt;br/&amp;gt;
联系博主：javachen.june[a]gmail.com
&amp;lt;/div&amp;gt;
&amp;lt;/section&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;!-- more --&gt;

&lt;h1&gt;添加about页面&lt;/h1&gt;

&lt;p&gt;在根目录创建about.md并修改，注意：文件开头几行内容如下&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;text language-text&quot; data-lang=&quot;text&quot;&gt;title: About
layout: page
group: navigation
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h1&gt;设置固定链接&lt;/h1&gt;

&lt;p&gt;在 _config.yml 里，找到 permalink，设置如下：&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;text language-text&quot; data-lang=&quot;text&quot;&gt;permalink: /:categories/:year/:month/:day/:title 
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h1&gt;修改，markdown实现为redcarpet&lt;/h1&gt;

&lt;p&gt;首先通过gem安装redcarpet，然后修改_config.yml：&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;text language-text&quot; data-lang=&quot;text&quot;&gt;markdown: redcarpet
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h1&gt;首页添加最近文章&lt;/h1&gt;

&lt;p&gt;在index.html页面&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;text language-text&quot; data-lang=&quot;text&quot;&gt;&amp;lt;section&amp;gt;
&amp;lt;h4&amp;gt;Recent Posts&amp;lt;/h4&amp;gt;
&amp;lt;ul id=&amp;quot;recent_posts&amp;quot;&amp;gt;{ % for rpost in site.posts limit: 15 %}
&amp;lt;li class=&amp;quot;post&amp;quot;&amp;gt;
&amp;lt;a href=&amp;quot;&amp;quot;&amp;gt;&amp;lt;/a&amp;gt;
&amp;lt;/li&amp;gt;{ % endfor %}
&amp;lt;/ul&amp;gt;
&amp;lt;/section&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h1&gt;首页为每篇文章添加分类、标签、发表日期以及评论连接&lt;/h1&gt;

&lt;p&gt;在index.html页面找到&lt;code&gt;&amp;lt;h3&amp;gt;&amp;lt;a href=&amp;quot;{ { BASE_PATH }}{ { post.url }}&amp;quot;&amp;gt;{ { post.title }}&amp;lt;/a&amp;gt;&amp;lt;/h3&amp;gt;&lt;/code&gt;，在下面添加：&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;text language-text&quot; data-lang=&quot;text&quot;&gt; &amp;lt;div class=&amp;quot;c9&amp;quot;&amp;gt;
    Categories：
        { %for cg in post.categories % }
        &amp;lt;a href=&amp;quot;/categories.html#-ref&amp;quot;&amp;gt;&amp;lt;/a&amp;gt;
        { %if forloop.index &amp;lt; forloop.length % }
        ,
        { %endif%}
        { %endfor%}
    |
    Tags：
        { %for cg in post.tags %}
        &amp;lt;a href=&amp;quot;/tags.html#-ref&amp;quot;&amp;gt;&amp;lt;/a&amp;gt;
        { %if forloop.index &amp;lt; forloop.length %}
        ,
        { %endif%}
        { %endfor%}
    |
    Time：&amp;lt;time date=&amp;quot;{ { post.date|date: &amp;#39;%Y-%m-%d&amp;#39; }}&amp;quot;&amp;gt;&amp;lt;/time&amp;gt;
    &amp;lt;a href=&amp;#39;#comments&amp;#39; title=&amp;#39;分享文章、查看评论&amp;#39; style=&amp;quot;float:right;margin-right:.5em;&amp;quot;&amp;gt;Comments&amp;lt;/a&amp;gt;
&amp;lt;/div&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h1&gt;修改h1、h2等标题字体&lt;/h1&gt;

&lt;p&gt;主要是参考&lt;a href=&quot;http://www.ituring.com.cn/&quot;&gt;图灵社区&lt;/a&gt;的css，在&lt;code&gt;assets/themes/twitter/css/style.css&lt;/code&gt;中添加如下css代码：&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;text language-text&quot; data-lang=&quot;text&quot;&gt;h1,h2,h3,h4,h5,h6{margin:18px 0 9px;font-family:inherit;font-weight:normal;color:inherit;text-rendering:optimizelegibility;}h1 small,h2 small,h3 small,h4 small,h5 small,h6 small{font-weight:normal;color:#999999;}
h1{font-size:30px;line-height:36px;}h1 small{font-size:18px;}
h2{font-size:24px;line-height:36px;}h2 small{font-size:18px;}
h3{font-size:18px;line-height:27px;}h3 small{font-size:14px;}
h4,h5,h6{line-height:18px;}
h4{font-size:14px;}h4 small{font-size:12px;}
h5{font-size:12px;}
h6{font-size:11px;color:#999999;text-transform:uppercase;}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h1&gt;添加返回顶部功能&lt;/h1&gt;

&lt;p&gt;同样是参考了&lt;a href=&quot;http://www.ituring.com.cn/&quot;&gt;图灵社区&lt;/a&gt;的css和网上的一篇js实现。在&lt;code&gt;assets/themes/twitter/css/style.css&lt;/code&gt;：&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;text language-text&quot; data-lang=&quot;text&quot;&gt;.backToTop {
    display: block;
    width: 40px;
    height: 32px;
    font-size: 26px;
    line-height: 32px;
    font-family: verdana, arial;
    padding: 5px 0;
    background-color: #000;
    color: #fff;
    text-align: center;
    position: fixed;
    _position: absolute;
    right: 10px;
    bottom: 100px;
    _bottom: &amp;quot;auto&amp;quot;;
    cursor: pointer;
    opacity: .6;
    filter: Alpha(opacity=60);
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;在&lt;code&gt;assets/themes/twitter/js&lt;/code&gt;添加jquery和main.js，main.js内容如下：&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;text language-text&quot; data-lang=&quot;text&quot;&gt;jQuery.noConflict();
jQuery(document).ready(function(){
    var backToTopTxt = &amp;quot;▲&amp;quot;, backToTopEle = jQuery(&amp;#39;&amp;lt;div class=&amp;quot;backToTop&amp;quot;&amp;gt;&amp;lt;/div&amp;gt;&amp;#39;).appendTo(jQuery(&amp;quot;body&amp;quot;)).text(backToTopTxt).attr(&amp;quot;title&amp;quot;,&amp;quot;Back top top&amp;quot;).click(function() {
        jQuery(&amp;quot;html, body&amp;quot;).animate({ scrollTop: 0 }, 120);
    }), backToTopFun = function() {
        var st = jQuery(document).scrollTop(), winh = jQuery(window).height();
        (st &amp;gt; 200)? backToTopEle.show(): backToTopEle.hide();    
        //IE6下的定位
        if (!window.XMLHttpRequest) {
            backToTopEle.css(&amp;quot;top&amp;quot;, st + winh - 166); 
        }
    };

    backToTopEle.hide(); 
        jQuery(window).bind(&amp;quot;scroll&amp;quot;, backToTopFun);
    jQuery(&amp;#39;div.main a,div.pic a&amp;#39;).attr(&amp;#39;target&amp;#39;, &amp;#39;_blank&amp;#39;);
});
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h1&gt;添加文章版权说明&lt;/h1&gt;

&lt;p&gt;在&lt;code&gt;_includes/themes/twitter/post.html&lt;/code&gt;中文章主体下面添加如下代码：&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;text language-text&quot; data-lang=&quot;text&quot;&gt;&amp;lt;hr&amp;gt;
&amp;lt;div class=&amp;quot;copyright&amp;quot;&amp;gt;
&amp;lt;p&amp;gt;&amp;lt;strong&amp;gt;本文固定链接：&amp;lt;/strong&amp;gt;&amp;lt;a href=&amp;#39;{ {page.url}}&amp;#39;&amp;gt;http://blog.javachen.com/post/2013/08/31/my-jekyll-config&amp;lt;/a&amp;gt;&amp;lt;/p&amp;gt;
&amp;lt;p&amp;gt;&amp;lt;strong&amp;gt;原创文章,转载请注明出处：&amp;lt;/strong&amp;gt;&amp;lt;a href=&amp;#39;{ {page.url}}&amp;#39;&amp;gt;JavaChen Blog&amp;lt;/a&amp;gt;&amp;lt;/p&amp;gt;
&amp;lt;/div&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;并在&lt;code&gt;assets/themes/twitter/css/style.css&lt;/code&gt;中添加如下css代码：&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;text language-text&quot; data-lang=&quot;text&quot;&gt;.copyright {
margin: 10px 0;
padding: 10px 20px;
line-height: 1;
border-radius: 5px;
background: #f5f5f5;
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h1&gt;添加read more功能&lt;/h1&gt;

&lt;p&gt;参考&lt;a href=&quot;http://truongtx.me/2013/05/01/jekyll-read-more-feature-without-any-plugin/&quot;&gt;Jekyll - Read More without plugin&lt;/a&gt;，在index.html找到 ，然后修改为：&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;text language-text&quot; data-lang=&quot;text&quot;&gt;{ % if post.content contains &amp;quot;&amp;lt;!-- more --&amp;gt;&amp;quot; %}
{ { post.content | split:&amp;quot;&amp;lt;!-- more --&amp;gt;&amp;quot; | first % }}
&amp;lt;h4&amp;gt;&amp;lt;a href=&amp;#39;{ {post.url}}&amp;#39; title=&amp;#39;Read more...&amp;#39;&amp;gt;Read more...&amp;lt;/a&amp;gt;&amp;lt;/h4&amp;gt;
{ % else %}
{ { post.content}}
{ % endif %}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;然后，在文章中添加&lt;code&gt;&amp;lt;!-- more --&amp;gt;&lt;/code&gt;即可。&lt;/p&gt;

&lt;h1&gt;添加搜索栏&lt;/h1&gt;

&lt;p&gt;参考&lt;a href=&quot;http://truongtx.me/2012/12/28/jekyll-create-simple-search-box/&quot;&gt;Jekyll Bootstrap - Create Simple Search box&lt;/a&gt;，在&lt;code&gt;_includes/themes/twitter/default.html&lt;/code&gt;导航菜单下面添加：&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;text language-text&quot; data-lang=&quot;text&quot;&gt;&amp;lt;form class=&amp;quot;navbar-search pull-left&amp;quot; id=&amp;quot;search-form&amp;quot;&amp;gt;
  &amp;lt;input type=&amp;quot;text&amp;quot; id=&amp;quot;google-search&amp;quot; class=&amp;quot;search-query&amp;quot; placeholder=&amp;quot;Search&amp;quot;&amp;gt;
&amp;lt;/form
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;添加js：&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;text language-text&quot; data-lang=&quot;text&quot;&gt;jQuery(&amp;quot;#search-form&amp;quot;).submit(function(){
    var query = document.getElementById(&amp;quot;google-search&amp;quot;).value;
    window.open(&amp;quot;http://google.com/search?q=&amp;quot; + query+ &amp;quot;%20site:&amp;quot; + &amp;quot;http://blog.javachen.com&amp;quot;);
});
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h1&gt;其他&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;添加404页面&lt;/li&gt;
&lt;li&gt;使用多说评论&lt;/li&gt;
&lt;li&gt;修改博客主体为宽屏模式&lt;/li&gt;
&lt;/ul&gt;

&lt;h1&gt;TODO&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;添加语法高亮，参考&lt;a href=&quot;http://truongtx.me/2012/12/28/jekyll-bootstrap-syntax-highlighting/&quot;&gt;Jekyll - Syntax highlighting&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
			<link>http://blog.javachen.com/post/2013/08/31/my-jekyll-config</link>
			<guid>http://blog.javachen.com/post/2013/08/31/my-jekyll-config</guid>
			<pubDate>Sat, 31 Aug 2013 00:00:00 +0800</pubDate>
		</item>

		<item>
			<title><![CDATA[使用ZooKeeper实现配置同步]]></title>
			<description>&lt;h1&gt;前言&lt;/h1&gt;

&lt;p&gt;应用项目中都会有一些配置信息，这些配置信息数据量少，一般会保存到内存、文件或者数据库，有时候需要动态更新。当需要在多个应用服务器中修改这些配置文件时，需要做到快速、简单、不停止应用服务器的方式修改并同步配置信息到所有应用中去。本篇文章就是介绍如何使用ZooKeeper来实现配置的动态同步。&lt;/p&gt;

&lt;h1&gt;ZooKeeper&lt;/h1&gt;

&lt;p&gt;在《&lt;a href=&quot;&quot;&gt;hive Driver类运行过程&lt;/a&gt;》一文中可以看到hive为了支持并发访问引入了ZooKeeper来实现分布式锁。参考《&lt;a href=&quot;http://rdc.taobao.com/team/jm/archives/1232&quot;&gt;ZooKeeper典型应用场景一览&lt;/a&gt;》一文，ZooKeeper还可以用作其他用途，例如：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;数据发布与订阅（配置中心）&lt;/li&gt;
&lt;li&gt;负载均衡&lt;/li&gt;
&lt;li&gt;命名服务(Naming Service)&lt;/li&gt;
&lt;li&gt;分布式通知/协调&lt;/li&gt;
&lt;li&gt;集群管理与Master选举&lt;/li&gt;
&lt;li&gt;分布式锁&lt;/li&gt;
&lt;li&gt;分布式队列&lt;/li&gt;
&lt;/ul&gt;

&lt;!-- more --&gt;

&lt;p&gt;一些在线系统在运行中，需要在不停止程序的情况下能够动态调整某一个变量的值并且能够及时生效。特别是当部署了多台应用服务器的时候，需要能够做到在一台机器上修改配置文件，然后在同步到所有应用服务器。这时候使用ZooKeeper来实现就很合适了。&lt;/p&gt;

&lt;h1&gt;数据发布与订阅&lt;/h1&gt;

&lt;p&gt;发布与订阅模型，即所谓的配置中心，顾名思义就是发布者将数据发布到ZK节点上，供订阅者动态获取数据，实现配置信息的集中式管理和动态更新。例如全局的配置信息，服务式服务框架的服务地址列表等就非常适合使用。&lt;/p&gt;

&lt;p&gt;使用ZooKeeper的发布与订阅模型，可以将应用中用到的一些配置信息放到ZK上进行集中管理。这类场景通常是这样：应用在启动的时候会主动来获取一次配置，同时，在节点上注册一个Watcher，这样一来，以后每次配置有更新的时候，都会实时通知到订阅的客户端，从来达到获取最新配置信息的目的。这样的场景适合数据量很小，但是数据更新可能会比较快的需求。&lt;/p&gt;

&lt;h1&gt;配置存储方案&lt;/h1&gt;

&lt;p&gt;配置文件通常有如下几种保存方式：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;将配置信息保存在程序代码中
这种方案简单，但每次修改配置都要重新编译、部署应用程序。显然这种方案很不方便，也不可靠，更无法做到修改的实时生效。&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;将配置信息保存在xml文件或者属性文件中
在参数信息保存在xml或者属性文件中，当需要修改参数时，直接修改 xml 文件。这样无需重新编译，只需重新部署修改的文件即可。但然后对所有的应用进行重新部署。这样做的缺点显而易见，要往上百台机器上重新部署应用，简直是一个噩梦。同时该方案还有一个缺点，就是配置修改无法做到实时生效。修改后往往过一段时间才能生效。&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;将配置信息保存在数据库中
当需要修改参数时，直接修改数据库，然后重启分布式应用程序，或者刷新分布式应用的缓存。尽管这种做法比以上两种方案简单，但却面临着单点失效问题。如果数据库服务器停机，则分布式应用程序的配置信息将无法更新。另外这种方案的配置修改生效实时性虽然比第二种方案好些，但仍然不能达到某些情况下的要求。&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h1&gt;基于ZooKeeper的配置信息同步方案&lt;/h1&gt;

&lt;p&gt;如果使用ZooKeeper来实现，就可以直接把配置信息保存到ZooKeeper中，或者把属性文件内容保存到ZooKeeper中，当属性文件内容发生变化时，就通知监听者如应用程序去重新读取配置文件。&lt;/p&gt;

&lt;p&gt;在网上搜索了一下，很能找到好用的现成的代码实现。有的基于ZooKeeper来扩张jdk的hashmap来存储配置参数，如：&lt;a href=&quot;http://melin.iteye.com/blog/899435&quot;&gt;使用ZooKeeper实现静态数据中心化配置管理&lt;/a&gt;，也有人直接实现了一个基于java并发框架的工具包，如：&lt;a href=&quot;https://github.com/openUtility/menagerie&quot;&gt;menagerie&lt;/a&gt;。&lt;/p&gt;

&lt;hr&gt;

&lt;p&gt;&lt;code&gt;注意&lt;/code&gt;:以下部分文字和图来自：&lt;a href=&quot;http://www.code365.org/wp-content/uploads/2012/02/%E5%9F%BA%E4%BA%8EZooKeeper%E7%9A%84%E9%85%8D%E7%BD%AE%E4%BF%A1%E6%81%AF%E5%AD%98%E5%82%A8%E6%96%B9%E6%A1%88%E7%9A%84%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%AE%9E%E7%8E%B01.pdf&quot;&gt;基于ZooKeeper的配置信息存储方案的设计与实现1.pdf&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;基于ZooKeeper的特性,借助ZooKeeper可以实现一个可靠的、简单的、修改配置能够实时生效的配置信息存储方案,整体的设计方案如图：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2013/zookeeper-01.jpg&quot; alt=&quot;基于zookeeper的方案&quot;&gt;&lt;/p&gt;

&lt;p&gt;整个配置信息存储方案由三部分组成:ZooKeeper服务器集群、配置管理程序、分布式应用程序。&lt;/p&gt;

&lt;p&gt;ZooKeeper服务器集群存储配置信息,在服务器上创建一个保存数据的节点(创建节点操作);配置管理程序提供一个配置管理的UI界面或者命令行方式,用户通过配置界面修改ZooKeeper服务器节点上配置信息(设置节点数据操作);分布式应用连接到ZooKeeper集群上(创建ZooKeeper客户端操作),监听配置信息的变化(使用获取节点数据操作,并注册一个watcher)。&lt;/p&gt;

&lt;p&gt;当配置信息发生变化时,分布式应用会更新程序中使用配置信息。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2013/zookeeper-02.jpg&quot; alt=&quot;修改配置的时许图&quot;&gt;&lt;/p&gt;

&lt;h1&gt;源代码&lt;/h1&gt;

&lt;p&gt;找到一个淘宝工程师写的实现方式，待整理下之后，提交到github上去。&lt;/p&gt;

&lt;h1&gt;优点&lt;/h1&gt;

&lt;p&gt;借助 ZooKeeper我们实现的配置信息存储方案具有的优点如下:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;简单。尽管前期搭建ZooKeeper服务器集群较为麻烦,但是实现该方案后,修改配置整个过程变得简单很多。用户只要修改配置,无需进行其他任何操作,配置自动生效。&lt;/li&gt;
&lt;li&gt;可靠。ZooKeeper服务集群具有无单点失效的特性,使整个系统更加可靠。即使ZooKeeper 集群中的一台机器失效,也不会影响整体服务,更不会影响分布式应用配置信息的更新。&lt;/li&gt;
&lt;li&gt;实时。ZooKeeper的数据更新通知机制,可以在数据发生变化后,立即通知给分布式应用程序,具有很强的变化响应能力。&lt;/li&gt;
&lt;/ol&gt;

&lt;h1&gt;总结&lt;/h1&gt;

&lt;p&gt;本文参考了网上的一些文章，给出了基于ZooKeeper的配置信息同步方案,解决了传统配置信息同步方案的缺点如实时性差、可靠性差、复杂等。&lt;/p&gt;

&lt;h1&gt;参考文章&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;http://rdc.taobao.com/team/jm/archives/1232&quot;&gt;ZooKeeper典型应用场景一览&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://melin.iteye.com/blog/899435&quot;&gt;使用ZooKeeper实现静态数据中心化配置管理&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://github.com/openUtility/menagerie&quot;&gt;menagerie&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://www.code365.org/wp-content/uploads/2012/02/%E5%9F%BA%E4%BA%8EZooKeeper%E7%9A%84%E9%85%8D%E7%BD%AE%E4%BF%A1%E6%81%AF%E5%AD%98%E5%82%A8%E6%96%B9%E6%A1%88%E7%9A%84%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%AE%9E%E7%8E%B01.pdf&quot;&gt;基于ZooKeeper的配置信息存储方案的设计与实现1.pdf&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
			<link>http://blog.javachen.com/hadoop/2013/08/23/publish-proerties-using-zookeeper</link>
			<guid>http://blog.javachen.com/hadoop/2013/08/23/publish-proerties-using-zookeeper</guid>
			<pubDate>Fri, 23 Aug 2013 00:00:00 +0800</pubDate>
		</item>

		<item>
			<title><![CDATA[hive Driver类运行过程]]></title>
			<description>&lt;h1&gt;概括&lt;/h1&gt;

&lt;p&gt;从《&lt;a href=&quot;hive/2013/08/21/hive-CliDriver/&quot;&gt;hive cli的入口类&lt;/a&gt;》中可以知道hive中处理hive命令的处理器一共有以下几种：&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;text language-text&quot; data-lang=&quot;text&quot;&gt;（1）set       SetProcessor，设置修改参数,设置到SessionState的HiveConf里。 
（2）dfs       DfsProcessor，使用hadoop的FsShell运行hadoop的命令。 
（3）add       AddResourceProcessor，添加到SessionState的resource_map里，运行提交job的时候会写入Hadoop的Distributed Cache。 
（4）delete    DeleteResourceProcessor，从SessionState的resource_map里删除。 
（5）其他       Driver 
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Driver类的主要作用是用来编译并执行hive命令，然后返回执行结果。这里主要分析Driver类的运行逻辑。&lt;/p&gt;

&lt;!-- more --&gt;

&lt;h1&gt;分析&lt;/h1&gt;

&lt;p&gt;Driver类入口如下：&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;text language-text&quot; data-lang=&quot;text&quot;&gt;Driver.run(String command) // 处理一条命令 
{ 
    int ret = compile(command);  // 分析命令，生成Task。 
    ret = execute();  // 运行Task。 
} 
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;运行命令之前，先编译命令，然后在运行任务。&lt;/p&gt;

&lt;h2&gt;compile方法过程&lt;/h2&gt;

&lt;p&gt;1、创建Context上下文&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;text language-text&quot; data-lang=&quot;text&quot;&gt;command = new VariableSubstitution().substitute(conf,command);
ctx = new Context(conf);
ctx.setTryCount(getTryCount());
ctx.setCmd(command);
ctx.setHDFSCleanup(true);
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;2、创建ParseDriver对象，然后解析命令、生成AST树。语法和词法分析内容，不是本文重点故不做介绍。&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;text language-text&quot; data-lang=&quot;text&quot;&gt;ParseDriver pd = new ParseDriver();
ASTNode tree = pd.parse(command, ctx);
tree = ParseUtils.findRootNonNullToken(tree);
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;简单归纳来说，解析程包括如下：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;词法分析，生成AST树，ParseDriver完成。 &lt;/li&gt;
&lt;li&gt;分析AST树，AST拆分成查询子块，信息记录在QB，这个QB在下面几个阶段都需要用到，SemanticAnalyzer.doPhase1完成。 &lt;/li&gt;
&lt;li&gt;从metastore中获取表的信息，SemanticAnalyzer.getMetaData完成。 &lt;/li&gt;
&lt;li&gt;生成逻辑执行计划，SemanticAnalyzer.genPlan完成。 &lt;/li&gt;
&lt;li&gt;优化逻辑执行计划，Optimizer完成，ParseContext作为上下文信息进行传递。 &lt;/li&gt;
&lt;li&gt;生成物理执行计划，SemanticAnalyzer.genMapRedTasks完成。 &lt;/li&gt;
&lt;li&gt;物理计划优化，PhysicalOptimizer完成，PhysicalContext作为上下文信息进行传递。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;3、读取环境变量，如果配置了语法分析的hook，参数为：&lt;code&gt;hive.semantic.analyzer.hook&lt;/code&gt;，则:先用反射得到&lt;code&gt;AbstractSemanticAnalyzerHook&lt;/code&gt;的集合，调用&lt;code&gt;hook.preAnalyze(hookCtx, tree)&lt;/code&gt;方法,然后再调用&lt;code&gt;sem.analyze(tree, ctx)&lt;/code&gt;方法，该方法才是用来作语法分析的,最后再调用&lt;code&gt;hook.postAnalyze(hookCtx, tree)&lt;/code&gt;方法执行一些用户定义的后置操作；&lt;/p&gt;

&lt;p&gt;否则，直接调用&lt;code&gt;sem.analyze(tree, ctx)&lt;/code&gt;进行语法分析。&lt;/p&gt;

&lt;p&gt;4、校验执行计划：&lt;code&gt;sem.validate()&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;5、创建查询计划QueryPlan。&lt;/p&gt;

&lt;p&gt;6、初始化FetchTask。&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;text language-text&quot; data-lang=&quot;text&quot;&gt;if (plan.getFetchTask() != null) {
   plan.getFetchTask().initialize(conf, plan, null);
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;7、授权校验工作。&lt;/p&gt;

&lt;h2&gt;run方法过程&lt;/h2&gt;

&lt;p&gt;1、运行HiveDriverRunHook的前置方法preDriverRun&lt;/p&gt;

&lt;p&gt;2、运行&lt;code&gt;compile(command)&lt;/code&gt;方法，并根据返回值判断是否该释放Hive锁。hive中可以配置&lt;code&gt;hive.support.concurrency&lt;/code&gt;值为true并设置zookeeper的服务器地址和端口，基于zookeeper实现分布式锁以支持hive的多并发访问。这部分内容不是本文重点故不做介绍。&lt;/p&gt;

&lt;p&gt;3、调用execute()方法执行任务。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;先运行ExecuteWithHookContext的前置hook方法，ExecuteWithHookContext类型有三种：前置、运行失败、后置。&lt;/li&gt;
&lt;li&gt;然后创建DriverContext用于维护正在运行的task任务，正在运行的task任务会添加到队列runnable中去。&lt;/li&gt;
&lt;li&gt;其次，在while循环中遍历队列中的任务，然后启动任务让其执行。&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;text language-text&quot; data-lang=&quot;text&quot;&gt;    while (runnable.peek() != null &amp;amp;&amp;amp; running.size() &amp;lt; maxthreads) {
      Task&amp;lt;? extends Serializable&amp;gt; tsk = runnable.remove();
      launchTask(tsk, queryId, noName, running, jobname, jobs, driverCxt);
    }
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;在launchTask方法中，先判断是否支持并发执行，如果支持则调用线程的start()方法，否则调用&lt;code&gt;tskRun.runSequential()&lt;/code&gt;方法顺序执行，只有当是MapReduce任务时，才执行并发执行：&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;text language-text&quot; data-lang=&quot;text&quot;&gt;    if (HiveConf.getBoolVar(conf, HiveConf.ConfVars.EXECPARALLEL) &amp;amp;&amp;amp; tsk.isMapRedTask()) {
          // Launch it in the parallel mode, as a separate thread only for MR tasks
          tskRun.start();
    } else {
          tskRun.runSequential();
    }
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;最后任务的运行，交给具体的Task去执行了。&lt;/li&gt;
&lt;li&gt;如果任务运行失败，则会创建一个备份任务，重新加入队列，然后再次运行；如果备份任务运行完成，则运行ExecuteWithHookContext的hook方法，这时候的hook为失败类型的hook。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;4、运行HiveDriverRunHook的后置方法postDriverRun&lt;/p&gt;

&lt;h2&gt;hive中支持的hook&lt;/h2&gt;

&lt;p&gt;上面分析中，提到了hive的hook机制，hive中一共存在以下几种hook。&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;text language-text&quot; data-lang=&quot;text&quot;&gt;hive.semantic.analyzer.hook
hive.exec.filter.hook
hive.exec.driver.run.hooks
hive.server2.session.hook
hive.exec.pre.hooks
hive.exec.post.hooks
hive.exec.failure.hooks
hive.client.stats.publishers
hive.metastore.ds.connection.url.hook
hive.metastore.init.hooks
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;通过hook机制，可以在运行前后做一些用户想做的事情。如：你可以在语法分析的hook中对hive的操作做一些超级管理员级别的权限判断；你可以对hive-server2做一些session级别的控制。&lt;/p&gt;

&lt;p&gt;cloudera的github仓库&lt;a href=&quot;https://github.com/cloudera/access&quot;&gt;access&lt;/a&gt;中关于hive的访问控制就是使用了hive的hook机制。&lt;/p&gt;

&lt;p&gt;twitter的mapreduce可视化项目监控项目&lt;a href=&quot;https://github.com/twitter/ambrose&quot;&gt;ambrose&lt;/a&gt;也利用了hive的hook机制，有兴趣的话，你可以去看看其是如何使用hive的hook并且你也可以扩增hook做些自己想做的事情。&lt;/p&gt;

&lt;h1&gt;总结&lt;/h1&gt;

&lt;p&gt;本文主要介绍了hive运行过程，其中简单提到了hive语法词法解析以及hook机制，没有详细分析。&lt;/p&gt;

&lt;p&gt;hive Driver类的执行过程如下：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2013/hive-driver.jpg&quot; alt=&quot;hive-driver&quot;&gt;&lt;/p&gt;

&lt;h1&gt;参考文章&lt;/h1&gt;

&lt;ol&gt;
&lt;li&gt;&lt;a href=&quot;http://www.cnblogs.com/end/archive/2012/12/19/2825320.html&quot;&gt;hive 初始化运行流程&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://github.com/cloudera/access&quot;&gt;Cloudera access&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://github.com/twitter/ambrose&quot;&gt;twitter ambrose&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
</description>
			<link>http://blog.javachen.com/hive/2013/08/22/hive-Driver</link>
			<guid>http://blog.javachen.com/hive/2013/08/22/hive-Driver</guid>
			<pubDate>Thu, 22 Aug 2013 00:00:00 +0800</pubDate>
		</item>

	</channel>
</rss>
