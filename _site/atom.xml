<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
 
 <title>JavaChen</title>
 <link href="blog.javachen.com/atom.xml" rel="self"/>
 <link href="blog.javachen.com/"/>
 <updated>2012-11-16T17:09:53+08:00</updated>
 <id>http://blog.javachen.com/</id>
 <author>
   <name>javachen</name>
   <email>june.chan@foxmail.com</email>
 </author>
 
 
 <entry>
   <title>使用Octopress将博客从wordpress迁移到GitHub上</title>
   <link href="http://blog.javachen.com/2012/06/03/migrate-blog-form-wordpress-to-github-with-octopress.html"/>
   <updated>2012-06-03T14:00:00+08:00</updated>
   <id>http://blog.javachen.com/2012/06/03/migrate-blog-form-wordpress-to-github-with-octopress</id>
   <content type="html">&lt;h2&gt;Step1 - 在本机安装Octopress&lt;/h2&gt;

&lt;p&gt;首先，必须先在本机安装配置&lt;a href=&quot;http://git-scm.com/&quot;&gt;Git&lt;/a&gt;和&lt;a href=&quot;https://rvm.beginrescueend.com/rvm/install/&quot;&gt;Ruby&lt;/a&gt;,Octopress需要Ruby版本至少为1.9.2。你可以使用&lt;a href=&quot;http://rvm.beginrescueend.com/&quot;&gt;RVM&lt;/a&gt;或&lt;a href=&quot;https://github.com/sstephenson/rbenv&quot;&gt;rbenv&lt;/a&gt;安装ruby，安装方法见Octopress官方文档：&lt;a href=&quot;http://octopress.org/docs/setup/&quot;&gt;http://octopress.org/docs/setup/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;我使用rvm安装：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;rvm install 1.9.2 &amp;amp;&amp;amp; rvm use 1.9.2
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;安装完之后可以查看ruby版本：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ruby --version
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;结果为：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ruby 1.9.2p320 (2012-04-20 revision 35421) [x86_64-linux]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后需要从github下载Octopress：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;git clone git://github.com/imathis/octopress.git octopress
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;因为我fork了Octopress，并在配置文件上做了一些修改，故我从我的仓库地址下载Octopress，命令如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;git clone git@github.com:javachen/octopress.git
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;运行上面的代码后，你会看到：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Cloning into 'octopress'...
remote: Counting objects: 6579, done.
remote: Compressing objects: 100% (2361/2361), done.
remote: Total 6579 (delta 3773), reused 6193 (delta 3610)
Receiving objects: 100% (6579/6579), 1.34 MiB | 35 KiB/s, done.
Resolving deltas: 100% (3773/3773), done.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;接下来进入octopress：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;cd octopress
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;接下来安装依赖：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;gem install bundler
rbenv rehash    # If you use rbenv, rehash to be able to run the bundle command
bundle install
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;安装Octopress默认的主题：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;rake install
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;你也可以安装自定义的主题，blog为主题名称：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;rake install['blog']
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;至此，Octopress所需的环境已经搭建成功。&lt;/p&gt;

&lt;h2&gt;Step2 - 连接GitHub Pages&lt;/h2&gt;

&lt;p&gt;首先，你得有一个GitHub的帐号，并且已经创建了一个新的Repository。如果你准备用自己的域名的话，Repository的名称可以随便取，不过正常人在正常情况下，一般都是以域名取名的。如果你没有自己的域名，GitHub是提供二级域名使用的，但是你得把Repository取名为&lt;code&gt;你的帐号.github.com&lt;/code&gt;，并且，部署的时候会占用你的master分支。&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Tips：&lt;/em&gt;
如果用自己的一级域名，记得把source/CNAME文件内的域名改成你的一级域名，还有在dns管理中把域名的A Record指向IP：207.97.227.245；
如果用自己的二级域名，记得把source/CNAME文件内的域名改成你的二级域名，还有在dns管理中把域名的CNAME Record指向网址：charlie.github.com；&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;echo 'your-domain.com' &amp;gt;&amp;gt; source/CNAME
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如果用GitHub提供的二级域名，记得把source/CNAME删掉。&lt;/p&gt;

&lt;p&gt;完成上述准备工作后，运行：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;rake setup_github_pages
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;它会提示你输入有读写权限的Repository Url，这个在GitHub上可以找到。Url形如：https://github.com/javachen/javachen.github.com.git，javachen.github.com是我的Repository的名称。&lt;/p&gt;

&lt;h2&gt;Step3 - 配置你的博客&lt;/h2&gt;

&lt;p&gt;需要配置博客url、名称、作者、rss等信息。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;url: http://javachen.github.com
title: JavaChen on Java
subtitle: Just some random thoughts about technology,Java and life.
author: javachen
simple_search: http://google.com/search
description:

date_format: &quot;%Y年%m月%d日&quot;

subscribe_rss: /atom.xml
subscribe_email:
email:

# 如果你使用的是一个子目录，如http://site.com/project，则设置为'root: /project'
root: /
# 文章标题格式
permalink: /:year/:month/:day/:title/
source: source
destination: public
plugins: plugins
code_dir: downloads/code
# 分类存放路径
category_dir: categories
markdown: rdiscount
pygments: false # default python pygments have been replaced by pygments.rb
&lt;/code&gt;&lt;/pre&gt;

&lt;h2&gt;Step4 - 部署&lt;/h2&gt;

&lt;p&gt;先把整个项目静态化，然后再部署到GitHub：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;rake generate
rake deploy
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;当你看到“Github Pages deploy complete”后，就表示你大功已成。Enjoy!&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Tips：&lt;/em&gt;
Octopress提供的所有rake方法，可以运行&lt;code&gt;rake -T&lt;/code&gt;查看。
如果在执行上述命令中ruby报错，则需要一一修复错误，这一步是没有接触过ruby的人比较苦恼的。&lt;/p&gt;

&lt;h2&gt;Step5 - 从Wordpress迁移到Octopress&lt;/h2&gt;

&lt;h3&gt;备份&lt;/h3&gt;

&lt;h4&gt;备份评论内容&lt;/h4&gt;

&lt;p&gt;Octopress由于是纯静态，所以没有办法存储用户评论了，我们可以使用DISQUS提供的“云评论”服务。首先安装DISQUS的WordPress插件，在插件设置中我们可以将现有的评论内容导入到DISQUS中。DISQUS处理导入数据的时间比较长，往往需要24小时甚至以上的时间。&lt;/p&gt;

&lt;h4&gt;备份文章内容&lt;/h4&gt;

&lt;p&gt;在WordPress后台我们可以将整站数据备份成一个.xml文件下载下来。同时，我原先文章中的图片都是直接在Wordpress后台上传的，所以要把服务器上&lt;code&gt;wp-content/uploads&lt;/code&gt;下的所有文件备份下来。&lt;/p&gt;

&lt;h3&gt;迁移&lt;/h3&gt;

&lt;h4&gt;迁移文章&lt;/h4&gt;

&lt;p&gt;jekyll本身提供了一个从WordPress迁移文章的工具，不过对中文实在是不太友好。这里我使用了YORKXIN的修改版本。将上面备份的wordpress.xml放到Octopress根目录，把脚本放到新建的utils目录中，然后运行：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ruby -r &quot;./utils/wordpressdotcom.rb&quot; -e &quot;Jekyll::WordpressDotCom.process&quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;于是转换好的文章都放进source目录了。&lt;/p&gt;

&lt;h4&gt;迁移URL&lt;/h4&gt;

&lt;p&gt;迁移URL，便是要保证以前的文章链接能够自动重定向到新的链接上。这样既能保证搜索引擎的索引不受影响，也是一项对读者负责任的行为是吧。不过这是一项挺麻烦的事情。&lt;/p&gt;

&lt;p&gt;幸好我当初建立WordPress的时候就留下了后路。原先网站的链接是这样的：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;http://XXXXXXXXX.com/[year]/[month]/[the-long-long-title].html
http://XXXXXXXXX.com/page/xx/
http://XXXXXXXXX.com/category/[category-name]/
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这样的格式是比较容易迁移的。如果原先的文章URL是带有数字ID的话，只能说声抱歉了。到_config.yml里面设置一下新站点的文章链接格式，跟原先的格式保持一致：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;permalink: /:year/:month/:title/
category_dir: category
pagination_dir:  # 留空
&lt;/code&gt;&lt;/pre&gt;

&lt;h4&gt;迁移评论&lt;/h4&gt;

&lt;p&gt;既然做好了301，那么迁移评论就显得非常简单了。登录DISQUS后台，进入站点管理后台的“Migrate Threads”栏目，那里有一个“Redirect Crawler”的功能，便是自动跟随301重定向，将评论指向新的网址。点一下那个按钮就大功告成。&lt;/p&gt;

&lt;h4&gt;迁移图片&lt;/h4&gt;

&lt;p&gt;可以参考&lt;a href=&quot;http://log4d.com/2012/05/image-host/&quot;&gt;使用独立图床子域名&lt;/a&gt;&lt;/p&gt;

&lt;h2&gt;Step6 - 再次部署&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;rake generate
rake deploy
&lt;/code&gt;&lt;/pre&gt;

&lt;h2&gt;参考文章&lt;/h2&gt;

&lt;ol&gt;
&lt;li&gt;&lt;a href=&quot;http://octopress.org/docs/setup/&quot;&gt;http://octopress.org/docs/setup/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://octopress.org/docs/deploying/&quot;&gt;http://octopress.org/docs/deploying/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://mrzhang.me/blog/blog-equals-github-plus-octopress.html&quot;&gt;http://mrzhang.me/blog/blog-equals-github-plus-octopress.html&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://blog.dayanjia.com/2012/04/migration-to-octopress-from-wordpress/&quot;&gt;http://blog.dayanjia.com/2012/04/migration-to-octopress-from-wordpress/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://log4d.com/2012/05/image-host/&quot;&gt;http://log4d.com/2012/05/image-host/&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

<strong>原创文章，转载请注明：</strong>转载自：<a href='http://blog.javachen.com/2012/06/03/migrate-blog-form-wordpress-to-github-with-octopress.html'>使用Octopress将博客从wordpress迁移到GitHub上</a></content>
 </entry>
 
 <entry>
   <title>Kettle dependency management</title>
   <link href="http://blog.javachen.com/Pentaho/2012/04/13/kettle-dependency-management.html"/>
   <updated>2012-04-13T00:00:00+08:00</updated>
   <id>http://blog.javachen.com/Pentaho/2012/04/13/kettle-dependency-management</id>
   <content type="html">&lt;p&gt;pentaho的项目使用了ant和ivy解决项目依赖,所以必须编译源码需要ivy工具.直接使用ivy编译pentaho的bi server项目,一直没有编译成功.&lt;br /&gt;
使用ivy编译kettle的源代码却是非常容易的事情.&lt;/p&gt;

&lt;p&gt;该篇文章翻译并参考了Will Gorman在pentaho的wiki上添加的&lt;a href=&quot;http://wiki.pentaho.com/display/EAI/Kettle+dependency+management&quot; target=&quot;_blank&quot;&gt;Kettle dependency management&lt;/a&gt;,文章标题没作修改.&lt;br /&gt;
编写此文,是为了记录编译kettle源码的方法和过程.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;以下是对原文的一个简单翻译.&lt;/strong&gt;
将kettle作为一个产品发行是一个很有趣的事情.有很多来自于pentaho其他项目(其中有一些有依赖于kettle)的jar包被导入到kettle.这些jar包必须在发行的时候构建并且加入到kettle中.如果一个核心的库被更新了,我们必须将其导入到kettle中(如果有必要).bi服务器,pentaho报表以及pentaho元数据编辑器都将kettle作为一个服务/引擎资源而被构建的.自从我们已经将这些jar导入到我们的源码仓库,这些项目必须使用ivy明确列出kettle以及他的依赖.当kettle的依赖变化的时候,我们必须审查libext文件是否需要更新.&lt;/p&gt;

&lt;p&gt;pentaho创建了一系列的脚本来自动化的安装ivy,解决jar(或者是artifacts),构建并发行artifacts.kettle已经升级使用subfloor(简单的意味着build.xml继承自subfloor的构建脚本).subfloor使用ivy从pentaho仓库()或者ibiblio maven2仓库来获取跟新jar.ibiblio仓库用于大多数第三方的jar文件(如apache-commons).pentaho仓库用于在线的pentaho项目或者一些比在ibiblio的三方库.为了解决kettle的依赖,我们不得不在ivy.xml里创建一个清单.这个文件明确地列出每一个没有传递依赖的jar文件.这意味着libext文件的映射在ivy.xml中是一对一的.
&lt;!--more--&gt;
&lt;strong&gt;关于Ivy&lt;/strong&gt;
&lt;a href=&quot;http://ant.apache.org/ivy/&quot; target=&quot;_blank&quot;&gt;Apache Ivy™&lt;/a&gt;是一个流行的致力于灵活性和简单性的依赖管理工具.更多的参考:&lt;a href=&quot;http://ant.apache.org/ivy/features.html&quot; target=&quot;_blank&quot;&gt;enterprise features&lt;/a&gt;, &lt;a href=&quot;http://ant.apache.org/ivy/testimonials.html&quot; target=&quot;_blank&quot;&gt;what people say about it&lt;/a&gt;, 以及 &lt;a href=&quot;http://ant.apache.org/ivy/history/latest-milestone/index.html&quot; target=&quot;_blank&quot;&gt;how it can improve your build system&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;在kettle中使用ivyIDE&lt;/strong&gt;
首先,从svn上下载kettle的源代码:
&lt;pre&gt;
svn://source.pentaho.org/svnkettleroot/Kettle/trunk
&lt;/pre&gt;
如果你想在Eclipse上使用&lt;a href=&quot;http://ant.apache.org/ivy/ivyde/download.cgi&quot; target=&quot;_blank&quot;&gt;ivyde plugin&lt;/a&gt;.&lt;br /&gt;
请参考相关文章安装该插件.&lt;/p&gt;

&lt;p&gt;如果你不想使用ivyde,你可以简单快速并且容易的开始并编译代码.&lt;br /&gt;
1.执行&lt;code&gt;ant resolve&lt;/code&gt;,这个命令将会创建一个叫做resolved-libs的文件夹.&lt;br /&gt;
2.使用下面命令更新classpath &lt;br /&gt;
  a.手动的添加这些jar文件到你的ide的classpath&lt;br /&gt;
  b.执行ant create-dot-classpath,将会修改你的.classpath文件(注意刷新项目以使改变生效)&lt;br /&gt;
注意:kettle项目中的构建脚本会自动安装ivy插件.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;构建Kettle&lt;/strong&gt;
你可以下载kettle源代码然后立即执行&lt;code&gt;ant distrib&lt;/code&gt;命令&lt;br /&gt;
或者你可以在ide中导入下载的kettle工程,然后按照你的操作系统(默认的是Windows 32-bit)版本修改依赖的swt.jar文件.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;ivy中未完成的&lt;/strong&gt;
&lt;strong&gt;pentaho-database-&lt;/strong&gt;这是一个依赖kettle-db的常用项目,但又被kettle-ui使用.这样会导致循环依赖,将来可能会将其引入到kettle项目或是从该项目中去掉对kettle的依赖.
&lt;strong&gt;swt-&lt;/strong&gt;swt文件目前没有包括在ivy.xml文件中
&lt;strong&gt;library configurations-&lt;/strong&gt;每一个kettle库(kettle-db,kettle-core等等)应该在ivy.xml中有他自己的依赖.这些库应该继承一些特定的依赖,而取代继承整个kettle依赖.
&lt;strong&gt;checked-in plugins-&lt;/strong&gt;当前引入的插件如;DummyJob, DummyPlugin, S3CsvInput, ShapeFileReader3,versioncheck应该都移到ivy的plugin配置中.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;参考文章&lt;/strong&gt;
&lt;a href=&quot;http://wiki.pentaho.com/display/EAI/Kettle+dependency+management&quot; target=&quot;_blank&quot;&gt;Kettle dependency management&lt;/a&gt;
&lt;/p&gt;
<strong>原创文章，转载请注明：</strong>转载自：<a href='http://blog.javachen.com/Pentaho/2012/04/13/kettle-dependency-management.html'>Kettle dependency management</a></content>
 </entry>
 
 <entry>
   <title>Getting Started Using the Cassandra CLI</title>
   <link href="http://blog.javachen.com/NoSql/2012/04/09/getting-started-using-the-cassandra-cli.html"/>
   <updated>2012-04-09T00:00:00+08:00</updated>
   <id>http://blog.javachen.com/NoSql/2012/04/09/getting-started-using-the-cassandra-cli</id>
   <content type="html">&lt;p&gt;这仅仅是一个Cassandra CLI使用方法的清单。&lt;br /&gt;
Cassandra CLI 客户端用于处理集群中基本的数据定义（DDL）和数据维护（DML）。其处于&lt;code&gt;/usr/bin/cassandra-cli&lt;/code&gt;，如果是试用包安装，或者是&lt;code&gt;$CASSANDRA_HOME/bin/cassandra-cli&lt;/code&gt;，如果使用二进制文件安装。&lt;/p&gt;

&lt;p&gt;&lt;h1&gt;Starting the CLI&lt;/h1&gt;
使用&lt;code&gt;cassandra-cli&lt;/code&gt; &lt;code&gt;-host&lt;/code&gt; &lt;code&gt;-port&lt;/code&gt; 命令启动 Cassandra CLI，他将会连接&lt;code&gt;cassandra.yaml&lt;/code&gt;文件中定义的集群名称，默认为“&lt;em&gt;Test Cluster&lt;/em&gt;”。&lt;br /&gt;
如果你有一个但节点的集群，则使用以下命令：
&lt;pre&gt; 
$ cassandra-cli -host localhost -port 9160
&lt;/pre&gt;
如果想连接多节点集群中的一个节点，可以使用以下命令:
&lt;pre&gt;
$ cassandra-cli -host 110.123.4.5 -port 9160
&lt;/pre&gt;
或者，可以直接执行以下命令：
&lt;pre&gt;
$ cassandra-cli
&lt;/pre&gt;
登录成功之后，可以看到：
&lt;pre&gt;
Welcome to cassandra CLI.
Type 'help;' or '?' for help. Type 'quit;' or 'exit;' to quit.
&lt;/pre&gt;
你必须指定连接一个节点：
&lt;pre&gt;
[default@unknown]connect localhost/9160;
&lt;/pre&gt;
&lt;!--more--&gt;
&lt;h1&gt;Creating a Keyspace&lt;/h1&gt;
&lt;pre&gt;
[default@unknown] CREATE KEYSPACE demo;
&lt;/pre&gt;
下面的一个例子，创建一个叫demo的Keyspace,并且复制因子为1，使用&lt;code&gt;SimpleStrategy&lt;/code&gt;复制替换策略。
&lt;pre&gt;
[default@unknown] CREATE KEYSPACE demo with 
        placement_strategy ='org.apache.cassandra.locator.SimpleStrategy' 
        and strategy_options = [{replication_factor:1}];
&lt;/pre&gt;
你可以使用&lt;code&gt;SHOW KEYSPACES&lt;/code&gt;来查看所有系统的和你创建的Keyspace&lt;/p&gt;

&lt;p&gt;&lt;h1&gt;Use a keyspace&lt;/h1&gt;
&lt;pre&gt;
[default@unknown] USE demo;
&lt;/pre&gt;&lt;/p&gt;

&lt;p&gt;&lt;h1&gt;Creating a Column Family&lt;/h1&gt;
&lt;pre&gt;
[default@demo] CREATE COLUMN FAMILY users
WITH comparator = UTF8Type
AND key_validation_class=UTF8Type
AND column_metadata = [
{column_name: full_name, validation_class: UTF8Type}
{column_name: email, validation_class: UTF8Type}
{column_name: state, validation_class: UTF8Type}
{column_name: gender, validation_class: UTF8Type}
{column_name: birth_year, validation_class: LongType}
];
&lt;/pre&gt;
我们使用demo keyspace创建了一个column family，其名称为users，并包括5个静态列：full_name，email,state,gender,birth_year.comparator, key_validation_class和validation_class，用于设置；列名称，行key的值，列值的编码。comparator还定义了列名称的排序方式。&lt;br /&gt;
下面命令创建一个名称为 blog_entry的动态column family，我们不需要定义列，而由应用程序稍后定义。
&lt;pre&gt;
[default@demo] CREATE COLUMN FAMILY blog_entry WITH comparator = TimeUUIDType AND key_validation_class=UTF8Type AND default_validation_class = UTF8Type;
&lt;/pre&gt;&lt;/p&gt;

&lt;p&gt;&lt;h1&gt;Creating a Counter Column Family&lt;/h1&gt;
&lt;pre&gt;
[default@demo] CREATE COLUMN FAMILY page_view_counts WITH 
          default_validation_class=CounterColumnType 
          AND key_validation_class=UTF8Type AND comparator=UTF8Type;
&lt;/pre&gt;
插入一行和计数列：
&lt;pre&gt;
[default@demo] INCR page_view_counts['www.datastax.com'][home] BY 0;
&lt;/pre&gt;
增加计数：
&lt;pre&gt;
[default@demo] INCR page_view_counts['www.datastax.com'][home] BY 1;
&lt;/pre&gt;&lt;/p&gt;

&lt;p&gt;&lt;h1&gt;Inserting Rows and Columns&lt;/h1&gt;
以下命令以一个特点的行key值插入列到users中
&lt;pre&gt;
[default@demo] SET users['bobbyjo']['full_name']='Robert Jones';
[default@demo] SET users['bobbyjo']['email']='bobjones@gmail.com';
[default@demo] SET users['bobbyjo']['state']='TX';
[default@demo] SET users['bobbyjo']['gender']='M';
[default@demo] SET users['bobbyjo']['birth_year']='1975';
&lt;/pre&gt;
更新数据： set users['bobbyjo']['full_name'] = 'Jack';&lt;br /&gt;
获取数据： get users['bobbyjo'];&lt;br /&gt;
get命令用法参考：&lt;a href=&quot;http://wiki.apache.org/cassandra/API#get_slice&quot; target=&quot;_blank&quot;&gt;API#get_slice&lt;/a&gt;
查询数据： get users where gender= 'M';&lt;br /&gt;
下面命令在 blog_entry中创建了一行，其行key为“yomama”，并指定了一列：timeuuid()的值为 'I love my new shoes!'
&lt;pre&gt;
[default@demo] SET blog_entry['yomama'][timeuuid()] = 'I love my new shoes!';
&lt;/pre&gt;&lt;/p&gt;

&lt;p&gt;&lt;h1&gt;Reading Rows and Columns&lt;/h1&gt;
使用List命令查询记录，默认查询100条记录
&lt;pre&gt;
[default@demo] LIST users;
&lt;/pre&gt;
Cassandra 默认以16进制数组的格式存储数据 为了返回可读的数据格式，可以指定编码：
&lt;li&gt;ascii&lt;/li&gt;
&lt;li&gt;bytes&lt;/li&gt;
&lt;li&gt;integer (a generic variable-length integer type)&lt;/li&gt;
&lt;li&gt;lexicalUUID&lt;/li&gt;
&lt;li&gt;long&lt;/li&gt;
&lt;li&gt;utf8&lt;/li&gt;
例如：
&lt;pre&gt;
[default@demo] GET users[utf8('bobby')][utf8('full_name')];
&lt;/pre&gt;
你也可以使用&lt;code&gt;ASSUME&lt;/code&gt;命令指定编码，例如，指定行key，行名称，行值显示ascii码格式：
&lt;pre&gt;
[default@demo] ASSUME users KEYS AS ascii;
[default@demo] ASSUME users COMPARATOR AS ascii;
[default@demo] ASSUME users VALIDATOR AS ascii;
&lt;/pre&gt;&lt;/p&gt;

&lt;p&gt;&lt;h1&gt;Setting an Expiring Column&lt;/h1&gt;
例如，假设我们正在跟踪我们的用户，到期后10天的优惠券代码。我们可以定义coupon_code的列和设置该列的过期日期。例如：
&lt;pre&gt;
[default@demo] SET users['bobbyjo'] [utf8('coupon_code')] = utf8('SAVE20') WITH ttl=864000;
&lt;/pre&gt;
自该列被设置值之后，经过10天或864,000秒后，其值将被标记为删除，不再由读操作返回。然而，请注意，直到Cassandra的处理过程完成，该值才会从硬盘中删除。&lt;/p&gt;

&lt;p&gt;&lt;h1&gt;Indexing a Column&lt;/h1&gt;
给birth_year添加一个二级索引：
&lt;pre&gt;
[default@demo] UPDATE COLUMN FAMILY users 
            WITH comparator = UTF8Type AND column_metadata = 
            [{column_name: birth_year, validation_class: LongType, index_type: KEYS}];
&lt;/pre&gt;
由于该列被索引了，所以可以直接通过该列查询：
&lt;pre&gt;
[default@demo] GET users WHERE birth_date = 1969;
&lt;/pre&gt;&lt;/p&gt;

&lt;p&gt;&lt;h1&gt;Deleting Rows and Columns&lt;/h1&gt;
删除yomama索引的coupon_code列：
&lt;pre&gt;
[default@demo] DEL users ['yomama']['coupon_code'];
[default@demo] GET users ['yomama'];
&lt;/pre&gt;
或者删除整行：
&lt;pre&gt;
[default@demo] DEL users ['yomama'];
&lt;/pre&gt;&lt;/p&gt;

&lt;p&gt;&lt;h1&gt;Dropping Column Families and Keyspaces&lt;/h1&gt;
&lt;pre&gt;
[default@demo] DROP COLUMN FAMILY users;
[default@demo] DROP KEYSPACE demo;
&lt;/pre&gt;&lt;/p&gt;

&lt;p&gt;&lt;h1&gt;For help&lt;/h1&gt;
&lt;pre&gt;
[default@unknown]help;
&lt;/pre&gt;
查看某一个命令的详细说明：
&lt;pre&gt;
[default@unknown] help SET;
&lt;/pre&gt;&lt;/p&gt;

&lt;p&gt;&lt;h1&gt;To Quit&lt;/h1&gt;
&lt;pre&gt;
[default@unknown]quit;
&lt;/pre&gt;&lt;/p&gt;

&lt;p&gt;&lt;h1&gt;To Execute Script&lt;/h1&gt;
&lt;pre&gt;
bin/cassandra-cli -host localhost -port 9160 -f script.txt
&lt;/pre&gt;&lt;/p&gt;

&lt;p&gt;&lt;h1&gt;参考文章&lt;/h1&gt;
1.&lt;a href=&quot;http://www.datastax.com/docs/0.8/dml/using_cli&quot; target=&quot;_blank&quot;&gt;Getting Started Using the Cassandra CLI&lt;/a&gt;

2.&lt;a href=&quot;http://wiki.apache.org/cassandra/CassandraCli&quot; target=&quot;_blank&quot;&gt;CassandraCli&lt;/a&gt;&lt;/p&gt;
<strong>原创文章，转载请注明：</strong>转载自：<a href='http://blog.javachen.com/NoSql/2012/04/09/getting-started-using-the-cassandra-cli.html'>Getting Started Using the Cassandra CLI</a></content>
 </entry>
 
 <entry>
   <title>哈希表</title>
   <link href="http://blog.javachen.com/Java/2012/03/26/hash-and-hash-functions.html"/>
   <updated>2012-03-26T00:00:00+08:00</updated>
   <id>http://blog.javachen.com/Java/2012/03/26/hash-and-hash-functions</id>
   <content type="html">&lt;p&gt;&lt;strong&gt;定义 &lt;/strong&gt;
一般的线性表、树，数据在结构中的相对位置是&lt;code&gt;随机&lt;/code&gt;的，即和记录的关键字之间不存在确定的关系，因此，在结构中查找记录时需进行一系列和关键字的比较。这一类查找方法建立在“比较“的基础上，查找的效率依赖于查找过程中所进行的比较次数。 若想能直接找到需要的记录，必须在记录的存储位置和它的关键字之间建立一个确定的对应关系f，使每个关键字和结构中一个唯一的存储位置相对应，这就是哈希表。&lt;/p&gt;

&lt;p&gt;&lt;code&gt;哈希表&lt;/code&gt;又称散列表。
&lt;em&gt;哈希表存储的基本思想是&lt;/em&gt;：以数据表中的每个记录的关键字 k为自变量，通过一种函数H(k)计算出函数值。把这个值解释为一块连续存储空间（即&lt;code&gt;数组空间&lt;/code&gt;）的单元地址（即&lt;code&gt;下标&lt;/code&gt;），将该记录存储到这个单元中。在此称该函数H为哈希函数或散列函数。按这种方法建立的表称为&lt;code&gt;哈希表&lt;/code&gt;或&lt;code&gt;散列表&lt;/code&gt;。&lt;br /&gt;
哈希表是一种数据结构，它可以提供快速的插入操作和查找操作。&lt;br /&gt;
哈希表是基于&lt;code&gt;数组结构&lt;/code&gt;实现的，所以它也存在一些&lt;em&gt;缺点&lt;/em&gt;： 数组创建后难于扩展，某些哈希表被基本填满时，性能下降得非常严重。 这个问题是哈希表不可避免的，即&lt;code&gt;冲突现象&lt;/code&gt;：对不同的关键字可能得到同一哈希地址。 所以在以下情况下可以优先考虑使用哈希表： &lt;em&gt;不需要有序遍历数据，并且可以提前预测数据量的大小&lt;/em&gt;。&lt;/p&gt;

&lt;p&gt;&lt;!--more--&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;冲突&lt;/strong&gt;
理想情况下，哈希函数在关键字和地址之间建立了一个一一对应关系，从而使得查找只需一次计算即可完成。由于关键字值的某种随机性，使得这种一一对应关系难以发现或构造。因而可能会出现不同的关键字对应一个存储地址。即k1≠k2，但H(k1)=H(k2)，这种现象称为冲突。&lt;br /&gt;
把这种具有不同关键字值而具有相同哈希地址的对象称&lt;code&gt;同义词&lt;/code&gt;。 在大多数情况下，冲突是不能完全避免的。这是因为所有可能的关键字的集合可能比较大，而对应的地址数则可能比较少。&lt;br /&gt;
对于哈希技术，主要研究两个问题：&lt;br /&gt;
（1）如何设计哈希函数以使冲突尽可能少地发生。&lt;br /&gt;
（2）发生冲突后如何解决。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;哈希函数的构造方法&lt;/strong&gt;
构造好的哈希函数的方法，应能使冲突尽可能地少，因而应具有较好的随机性。这样可使一组关键字的散列地址均匀地分布在整个地址空间。根据关键字的结构和分布的不同，可构造出许多不同的哈希函数。
&lt;strong&gt;1．直接定址法&lt;/strong&gt;
&lt;code&gt;直接定址法&lt;/code&gt;是以关键字k本身或关键字加上某个数值常量c作为哈希地址的方法。&lt;br /&gt;
该哈希函数H(k)为：&lt;br /&gt;
H(k)=k+c (c≥0)&lt;br /&gt;
这种哈希函数计算简单，并且不可能有冲突发生。当关键字的分布基本连续时，可使用直接定址法的哈希函数。否则，若关键字分布不连续将造成内存单元的大量浪费&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;2．除留余数法&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;取关键字k除以哈希表长度m所得余数作为哈希函数地址的方法。即：&lt;br /&gt;
H(k)=k％m&lt;br /&gt;
这是一种较简单、也是较常见的构造方法。&lt;br /&gt;
这种方法的关键是选择好哈希表的长度m。使得数据集合中的每一个关键字通过该函数转化后映射到哈希表的任意地址上的概率相等。&lt;br /&gt;
理论研究表明，在m取值为素数（质数）时，冲突可能性相对较少。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;3．平方取中法&lt;/strong&gt;
取关键字平方后的中间几位作为哈希函数地址（若超出范围时，可再取模）。&lt;br /&gt;
设有一组关键字ABC，BCD,CDE，DEF，……其对应的机内码如表所示。假定地址空间的大小为1000，编号为0-999。现按平方取中法构造哈希函数，则可取关键字机内码平方后的中间三位作为存储位置。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;4．折叠法&lt;/strong&gt;
这种方法适合在关键字的位数较多，而地址区间较小的情况。&lt;br /&gt;
将关键字分隔成位数相同的几部分。然后将这几部分的叠加和作为哈希地址（若超出范围，可再取模）。&lt;br /&gt;
例如，假设关键字为某人身份证号码430104681015355，则可以用4位为一组进行叠加。即有5355+8101+1046+430=14932，舍去高位。 则有H(430104681015355)=4932 为该身份证关键字的哈希函数地址。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;5．数值分析法&lt;/strong&gt;
若事先知道所有可能的关键字的取值时，可通过对这些关键字进行分析，发现其变化规律，构造出相应的哈希函数。&lt;br /&gt;
例：对如下一组关键字通过分析可知：每个关键字从左到右的第l，2，3位和第6位取值较集中，不宜作哈希地址。 剩余的第4，5，7和8位取值较分散，可根据实际需要取其中的若干位作为哈希地址。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;6. 随机数法&lt;/strong&gt;
选择一个随机函数，取关键字的随机函数值为它的哈希地址，即H(key)＝random(key)，其中random为随机函数。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;7. 斐波那契（Fibonacci）散列法&lt;/strong&gt;
平方散列法的缺点是显而易见的，所以我们能不能找出一个理想的乘数，而不是拿value本身当作乘数呢？答案是肯定的。&lt;br /&gt;
1，对于16位整数而言，这个乘数是40503&lt;br /&gt;
2，对于32位整数而言，这个乘数是2654435769&lt;br /&gt;
3，对于64位整数而言，这个乘数是11400714819323198485&lt;br /&gt;
这几个“理想乘数”是如何得出来的呢？这跟一个法则有关，叫黄金分割法则，而描述黄金分割法则的最经典表达式无疑就是著名的斐波那契数列，如果你还有兴趣，就到网上查找一下“斐波那契数列”等关键字，我数学水平有限，不知道怎么描述清楚为什么，另外斐波那契数列的值居然和太阳系八大行星的轨道半径的比例出奇吻合，很神奇，对么？&lt;br /&gt;
对我们常见的32位整数而言，公式：&lt;br /&gt;
index = (value * 2654435769) &amp;gt;&amp;gt; 28&lt;br /&gt;
如果用这种斐波那契散列法的话，那我上面的图就变成这样了：&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;冲突的解决方法&lt;/strong&gt;
假设哈希表的地址范围为0～m-l，当对给定的关键字k，由哈希函数H(k)算出的哈希地址为i（0≤i≤m-1）的位置上已存有记录，这种情况就是&lt;code&gt;冲突现象&lt;/code&gt;。 处理冲突就是为该关键字的记录找到另一个“空”的哈希地址。即通过一个新的哈希函数得到一个新的哈希地址。如果仍然发生冲突，则再求下一个，依次类推。直至新的哈希地址不再发生冲突为止。&lt;br /&gt;
常用的处理冲突的方法有开放地址法、链地址法两大类
&lt;strong&gt;1．开放定址法&lt;/strong&gt;
用开放定址法处理冲突就是当冲突发生时，形成一个地址序列。沿着这个序列逐个探测，直到找出一个“空”的开放地址。将发生冲突的关键字值存放到该地址中去。&lt;br /&gt;
如 Hi=(H(k)+d（i）) % m, i=1，2，…k (k 其中H(k)为哈希函数，m为哈希表长，d为增量函数，d(i)=dl，d2…dn-l。&lt;br /&gt;
增量序列的取法不同，可得到不同的开放地址处理冲突探测方法。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;1）线性探测法&lt;/strong&gt;
线性探测法是从发生冲突的地址（设为d）开始，依次探查d+l，d+2，…m-1（当达到表尾m-1时，又从0开始探查）等地址，直到找到一个空闲位置来存放冲突处的关键字。&lt;br /&gt;
若整个地址都找遍仍无空地址，则产生溢出。&lt;br /&gt;
线性探查法的数学递推描述公式为：&lt;br /&gt;
d0=H(k)&lt;br /&gt;
di=(di-1+1)% m (1≤i≤m-1)&lt;/p&gt;

&lt;p&gt;【例】已知哈希表地址区间为0～10，给定关键字序列（20，30，70，15，8，12，18，63，19）。哈希函数为H(k)=k％ll，采用线性探测法处理冲突，则将以上关键字依次存储到哈希表中。试构造出该哈希表，并求出等概率情况下的平均查找长度。&lt;br /&gt;
假设数组为A, 本题中各元素的存放过程如下：&lt;br /&gt;
H(20)=9，可直接存放到A[9]中去。&lt;br /&gt;
H(30)=8，可直接存放到A[8]中去。&lt;br /&gt;
H(70)=4，可直接存放到A[4]中去。&lt;br /&gt;
H(15)=4，冲突；&lt;br /&gt;
d0=4&lt;br /&gt;
d1=(4+1)%11=5，将15放入到A[5]中。&lt;br /&gt;
H(8)=8，冲突；&lt;br /&gt;
d0=8&lt;br /&gt;
d1=(8+1)%11=9，仍冲突；&lt;br /&gt;
d2=(8+2)%11=10，将8放入到A[10]中。&lt;/p&gt;

&lt;p&gt;在等概率情况下成功的平均查找长度为：&lt;br /&gt;
（1*5+2+3+4+6）/9 =20/9&lt;br /&gt;
利用线性探查法处理冲突容易造成关键字的&lt;code&gt;堆积&lt;/code&gt;问题。这是因为当连续n个单元被占用后，再散列到这些单元上的关键字和直接散列到后面一个空闲单元上的关键字都要占用这个空闲单元，致使该空闲单元很容易被占用，从而发生非同义冲突。造成平均查找长度的增加。&lt;br /&gt;
为了克服堆积现象的发生，可以用下面的方法替代线性探查法。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;（2）平方探查法&lt;/strong&gt;
设发生冲突的地址为d，则平方探查法的探查序列为：d+12，d+22，…直到找到一个空闲位置为止。&lt;br /&gt;
平方探查法的数学描述公式为：&lt;br /&gt;
d0=H(k)&lt;br /&gt;
di=(d0+i2) % m (1≤i≤m-1)&lt;br /&gt;
在等概率情况下成功的平均查找长度为：&lt;br /&gt;
（1*4+2*2+3+4+6）/9 =21/9&lt;br /&gt;
平方探查法是一种较好的处理冲突的方法，可以避免出现堆积问题。它的缺点是不能探查到哈希表上的所有单元，但至少能探查到一半单元。&lt;br /&gt;
例如，若表长m=13，假设在第3个位置发生冲突，则后面探查的位置依次为4、7、12、6、2、0，即可以探查到一半单元。&lt;br /&gt;
若解决冲突时，探查到一半单元仍找不到一个空闲单元。则表明此哈希表太满，需重新建立哈希表。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;2．链地址法&lt;/strong&gt;
用链地址法解决冲突的方法是：把所有关键字为同义词的记录存储在一个线性链表中，这个链表称为同义词链表。并将这些链表的表头指针放在数组中（下标从0到m-1）。这类似于图中的邻接表和树中孩子链表的结构。&lt;br /&gt;
由于在各链表中的第一个元素的查找长度为l，第二个元素的查找长度为2，依此类推。因此，在等概率情况下成功的平均查找长度为：&lt;br /&gt;
(1*5+2*2+3*l+4*1)／9=16／9&lt;/p&gt;

&lt;p&gt;虽然链地址法要多费一些存储空间，但是彻底解决了“堆积”问题，大大提高了查找效率。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;3. 再哈希法&lt;/strong&gt;：&lt;br /&gt;
Hi=R Hi(key)，&lt;br /&gt;
R Hi均是不同的哈希函数，即在同义词产生地址冲突时计算另一个哈希函数地址，直到冲突不再发生。这种方法不易产生&lt;code&gt;聚集&lt;/code&gt;，但增加了计算的时间。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;4.建立一个公共溢出区&lt;/strong&gt;
这也是处理冲突的一种方法。&lt;br /&gt;
假设哈希函数的值域为[0，m-1]，则设向量HashTable[0…m-1]为基本表，每个分量存放一个记录，另设立向量OverTable[0．．v]为溢出表。所有关键字和基本表中关键字为同义词的记录，不管它们由哈希函数得到的哈希地址是什么，一旦发生冲突，都填入溢出表。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;哈希表的查找及性能分析&lt;/strong&gt;
&lt;code&gt;哈希法&lt;/code&gt;是利用关键字进行计算后直接求出存储地址的。当哈希函数能得到均匀的地址分布时，不需要进行任何比较就可以直接找到所要查的记录。但实际上不可能完全避免冲突，因此查找时还需要进行探测比较。&lt;br /&gt;
在哈希表中，虽然冲突很难避免，但发生冲突的可能性却有大有小。这主要与三个因素有关。
&lt;strong&gt;第一:与装填因子有关&lt;/strong&gt;
所谓装填因子是指哈希表中己存入的元素个数n与哈希表的大小m的比值，即f=n/m。&lt;br /&gt;
当f越小时，发生冲突的可能性越小，越大（最大为1）时，发生冲突的可能性就越大。
&lt;strong&gt;第二:与所构造的哈希函数有关&lt;/strong&gt;
若哈希函数选择得当，就可使哈希地址尽可能均匀地分布在哈希地址空间上，从而减少冲突的发生。否则，若哈希函数选择不当，就可能使哈希地址集中于某些区域，从而加大冲突的发生。
&lt;strong&gt;第三:与解决冲突的哈希冲突函数有关&lt;/strong&gt;
哈希冲突函数选择的好坏也将减少或增加发生冲突的可能性。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;思考&lt;/strong&gt;
&lt;em&gt;哈希算法的基本思想是什么？&lt;/em&gt;
&lt;em&gt;哈希算法的存储效率主要取决于什么？&lt;/em&gt;
&lt;em&gt;哈希算法解决冲突的方式有哪些？&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;java 哈希表实现&lt;/strong&gt;
java中哈希表的实现有多个，比如hashtable，hashmap，currenthashmap，也有其他公司实现的，如apache的FashHashmap,google的mapmarker,high-lib的NonBlockingHashMap,其中差别是：&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;hastable&lt;/strong&gt;:线程同步，比较慢
&lt;strong&gt;hashmap&lt;/strong&gt;：线程不同步，不同步时候读写最快（但是不能保证读到最新数据），加同步修饰的时候， 读写比较慢
&lt;strong&gt;currenthashmap&lt;/strong&gt;:线程同步，默认分成16块，写入的时候只锁要写入的快，读取一般不锁块，只有读到空的时候，才锁块，性能比较高，处于hashmap同步和不同步之间。
&lt;strong&gt;fashhashmap&lt;/strong&gt;:apache collection 将HashMap封装，读取的时候copy一个新的，写入比较慢（尤其是存入比较多对象每写一次都要复制一个对象，超级慢），读取快
&lt;strong&gt;NoBlockingHashMap&lt;/strong&gt;： high_scale_lib实现写入慢，读取较快
&lt;strong&gt;MiltigetHashMap&lt;/strong&gt;，MapMaker google collection，和CurrentHashMap性能相当，功能比较全，可以设置超时，重复的可以保存成list&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;参考文章&lt;/strong&gt;
&lt;a href=&quot;http://course.onlinesjtu.com/mod/page/view.php?id=423&quot; target=&quot;_blank&quot;&gt; http://course.onlinesjtu.com/mod/page/view.php?id=423&lt;/a&gt;
&lt;a href=&quot;http://www.cnblogs.com/bigshuai/articles/2398116.html&quot; target=&quot;_blank&quot;&gt; http://www.cnblogs.com/bigshuai/articles/2398116.html&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;扩展阅读&lt;/strong&gt;
Hash碰撞的拒绝式服务攻击 &lt;a href=&quot;http://blog.jobbole.com/11454/&quot; target=&quot;_blank&quot;&gt;http://blog.jobbole.com/11454/&lt;/a&gt;

Berkeley DB Hash、Btree、Queue、Recno选择&lt;a href=&quot; http://www.webzone8.com/article/560.html&quot; target=&quot;_blank&quot;&gt; http://www.webzone8.com/article/560.html&lt;/a&gt;

Java Hashtable &lt;a href=&quot;http://javapapers.com/core-java/java-hashtable/#&amp;amp;slider1=1&quot; target=&quot;_blank&quot;&gt;http://javapapers.com/core-java/java-hashtable/#&amp;amp;slider1=1&lt;/a&gt;

Java Hashtable分析 &lt;a href=&quot;http://kantery.iteye.com/blog/441755&quot; target=&quot;_blank&quot;&gt;http://kantery.iteye.com/blog/441755&lt;/a&gt;&lt;/p&gt;
<strong>原创文章，转载请注明：</strong>转载自：<a href='http://blog.javachen.com/Java/2012/03/26/hash-and-hash-functions.html'>哈希表</a></content>
 </entry>
 
 <entry>
   <title>如何在kettle4.2上面实现cassandra的输入与输出</title>
   <link href="http://blog.javachen.com/Pentaho/2012/03/23/how-to-implement-cassandra-input-and-output-in-kettle4-2.html"/>
   <updated>2012-03-23T00:00:00+08:00</updated>
   <id>http://blog.javachen.com/Pentaho/2012/03/23/how-to-implement-cassandra-input-and-output-in-kettle4-2</id>
   <content type="html">&lt;p&gt;这是在QQ群里有人问到的一个问题.&lt;br /&gt;
如何在pdi-ce-4.2.X-stable上面实现cassandra的输入与输出,或是实现hadoop,hbase,mapreduce,mongondb的输入输出?&lt;/p&gt;

&lt;p&gt;在kettle中实现cassandra的输入与输出有以下两种方式:&lt;br /&gt;
第一种方式:自己编写cassandra输入输出组件&lt;br /&gt;
第二种方式:使用别人编写好的插件,将其集成进来&lt;/p&gt;

&lt;p&gt;当然还有第三种方法,直接使用4.3版本的pdi.&lt;br /&gt;
第一种方法需要对cassandra很熟悉编写插件才可以做到,第二种方法可以通过拷贝pdi-ce-big-data-4.3.0-preview中的文件来完成.&lt;/p&gt;

&lt;p&gt;在pdi-ce-big-data-4.3.0-preview&lt;a href=&quot;http://ci.pentaho.com/job/pentaho-big-data-plugin/lastSuccessfulBuild/artifact/pentaho-big-data-plugin/dist/&quot; target=&quot;_blank&quot;&gt;(下载页面&lt;/a&gt;)版本中可以看到kettle开始支持cassandra的输入和输出.&lt;br /&gt;
故我们可以将4.3版本中的cassandra相关文件拷贝到4.2.1中.我使用的是pdi-ce-4.2.1-stable.&lt;br /&gt;
在pdi-ce-big-data-4.3.0-preview/plugins目录下有以下目录或文件:
&lt;!--more--&gt;
&lt;pre&gt;
.
|-- databases
|-- hour-partitioner.jar
|-- jobentries
|-- kettle-gpload-plugin
|-- kettle-hl7-plugin
|-- kettle-palo-plugin
|-- pentaho-big-data-plugin
|-- repositories
|-- spoon
|-- steps
`-- versioncheck
&lt;/pre&gt;&lt;/p&gt;

&lt;p&gt;pentaho-big-data-plugin目录是kettle对大数据的集成与支持,我们只需要将该目录拷贝到pdi-ce-4.2.1-stable/plugins目录下即可.最后的结构如下
&lt;pre&gt;
.
|-- databases
|-- hour-partitioner.jar
|-- jobentries
|   `-- DummyJob
|       |-- DPL.png
|       |-- dummyjob.jar
|       `-- plugin.xml
|-- pentaho-big-data-plugin
|   |-- lib
|   |   |-- apache-cassandra-1.0.0.jar
|   |   |-- apache-cassandra-thrift-1.0.0.jar
|   |   |-- aws-java-sdk-1.0.008.jar
|   |   |-- commons-cli-1.2.jar
|   |   |-- guava-r08.jar
|   |   |-- hbase-comparators-TRUNK-SNAPSHOT.jar
|   |   |-- jline-0.9.94.jar
|   |   |-- libthrift-0.6.jar
|   |   |-- mongo-java-driver-2.7.2.jar
|   |   |-- pig-0.8.1.jar
|   |   |-- xpp3_min-1.1.4c.jar
|   |   `-- xstream-1.3.1.jar
|   `-- pentaho-big-data-plugin-TRUNK-SNAPSHOT.jar
|-- repositories
|-- spoon
|-- steps
|   |-- DummyPlugin
|   |   |-- DPL.png
|   |   |-- dummy.jar
|   |   `-- plugin.xml
|   |-- S3CsvInput
|   |   |-- jets3t-0.7.0.jar
|   |   |-- plugin.xml
|   |   |-- S3CIN.png
|   |   `-- s3csvinput.jar
|   `-- ShapeFileReader3
|       |-- plugin.xml
|       |-- SFR.png
|       `-- shapefilereader3.jar
`-- versioncheck
    |-- kettle-version-checker-0.2.0.jar
    `-- lib
        `-- pentaho-versionchecker.jar&lt;/pre&gt;&lt;/p&gt;

&lt;p&gt;13 directories, 29 files
&lt;/p&gt;

&lt;p&gt;启动pdi-ce-4.2.1-stable之后,打开一个转换,在核心对象窗口就可以看到Big Data步骤目录了.
&lt;div class=&quot;pic&quot;&gt;
&lt;a href=&quot;http://ww4.sinaimg.cn/mw600/48e24b4cjw1dr9zaa66nbj.jpg&quot; target=&quot;_blank&quot;&gt;
&lt;img alt=&quot;&quot; src=&quot;http://ww4.sinaimg.cn/mw600/48e24b4cjw1dr9zaa66nbj.jpg&quot; title=&quot;pdi big data plugin in kette 4.2&quot; class=&quot;aligncenter&quot; width=&quot;600&quot; height=&quot;375&quot; /&gt;
&lt;/a&gt;
&lt;/div&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;获取pentaho-big-data-plugin源码&lt;/strong&gt;
如果想在eclipse中查看或修改pentaho-big-data-plugin源码,该怎么做呢?&lt;br /&gt;
你可以从&lt;a href=&quot;http://ci.pentaho.com/job/pentaho-big-data-plugin/lastSuccessfulBuild/artifact/pentaho-big-data-plugin/dist/pentaho-big-data-plugin-TRUNK-SNAPSHOT-sources.zip&quot; target=&quot;_blank&quot;&gt;这里&lt;/a&gt;下载到源码,然后将src下的文件拷贝到你的pdi-ce-4.2.1-stable源码工程中.&lt;/p&gt;

&lt;p&gt;然后,需要在kettle-steps.xml中注册步骤节点&lt;br /&gt;
例如,下面是MongoDbInput步骤的注册方法,请针对不同插件的不同类路径加以修改.
&lt;pre&gt;
&lt;step id=&quot;MongoDbInput&quot;&gt;
&lt;description&gt;i18n:org.pentaho.di.trans.step:BaseStep.TypeLongDesc.MongoDbInput
&lt;classname&gt;org.pentaho.di.trans.steps.mongodbinput.MongoDbInputMeta
&lt;category&gt;i18n:org.pentaho.di.trans.step:BaseStep.Category.Input
&lt;tooltip&gt;i18n:org.pentaho.di.trans.step:BaseStep.TypeTooltipDesc.MongoDbInput
&lt;iconfile&gt;ui/images/mongodb-input.png
&lt;/iconfile&gt;&lt;/tooltip&gt;&lt;/category&gt;&lt;/classname&gt;&lt;/description&gt;&lt;/step&gt;
&lt;/pre&gt;&lt;/p&gt;

&lt;p&gt;&lt;div class=&quot;note&quot;&gt;
&lt;h&gt;注意:&lt;br /&gt;
由于pdi-ce-4.2.1-stable中存在hive组件,故添加pentaho-big-data-plugin插件之后有可能会出现找不到类的情况,这是由于jar重复版本不一致导致的,按照异常信息,找到重复的jar并按情况删除一个jar包即可.
&lt;/h&gt;&lt;/div&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;扩展阅读:&lt;/strong&gt;
Pentaho Big Data Plugin &lt;a href=&quot;http://wiki.pentaho.com/display/BAD/Getting+Started+for+Java+Developers&quot; target=&quot;_blank&quot;&gt;http://wiki.pentaho.com/display/BAD/Getting+Started+for+Java+Developers&lt;/a&gt;
pentaho-big-data-plugin ci
&lt;a href=&quot;http://ci.pentaho.com/job/pentaho-big-data-plugin/lastSuccessfulBuild/artifact/pentaho-big-data-plugin/dist/&quot; target=&quot;_blank&quot;&gt;http://ci.pentaho.com/job/pentaho-big-data-plugin/lastSuccessfulBuild/artifact/pentaho-big-data-plugin/dist/&lt;/a&gt;
Pentaho Community Edition (CE) downloads &lt;a href=&quot;http://wiki.pentaho.com/display/BAD/Downloads&quot; target=&quot;_blank&quot;&gt;http://wiki.pentaho.com/display/BAD/Downloads&lt;/a&gt;&lt;/p&gt;
<strong>原创文章，转载请注明：</strong>转载自：<a href='http://blog.javachen.com/Pentaho/2012/03/23/how-to-implement-cassandra-input-and-output-in-kettle4-2.html'>如何在kettle4.2上面实现cassandra的输入与输出</a></content>
 </entry>
 
 <entry>
   <title>Kettle运行作业之前的初始化过程</title>
   <link href="http://blog.javachen.com/Pentaho/2012/02/22/the-init-process-before-job-execution.html"/>
   <updated>2012-02-22T00:00:00+08:00</updated>
   <id>http://blog.javachen.com/Pentaho/2012/02/22/the-init-process-before-job-execution</id>
   <content type="html">&lt;p&gt;本文主要描述Kettle是如何通过GUI调用代码启动线程执行作业的。&lt;/p&gt;

&lt;p&gt;之前用英文写了一篇文章《&lt;a href=&quot;http://blog.javachen.com/2012/02/the-execution-process-of-kettles-job/&quot; target=&quot;_blank&quot;&gt;The execution process of kettle’s job&lt;/a&gt;》 ，这篇文章只是用于英语写技术博客的一个尝试。由于很久没有使用英语写作了，故那篇文章只是简单的通过UML的序列图描述kettle运行job的一个java类调用过程。将上篇文章的序列图和这篇文章联系起来，会更加容易理解本文。&lt;/p&gt;

&lt;p&gt;在Spoon界面点击运行按钮，Spoon GUI会调用Spoon.runFile()方法，这可以从xul文件（ui/menubar.xul）中的描述看出来。关于kettle中的xul的使用，不是本文重点故不在此说明。&lt;/p&gt;

&lt;p&gt;&lt;pre lang=&quot;java&quot;&gt;
public void runFile() {
	executeFile(true, false, false, false, false, null, false);
}&lt;/pre&gt;&lt;/p&gt;

&lt;p&gt;public void executeFile(boolean local, boolean remote, boolean cluster,&lt;br /&gt;
		boolean preview, boolean debug, Date replayDate, boolean safe) {&lt;br /&gt;
	TransMeta transMeta = getActiveTransformation();&lt;br /&gt;
	if (transMeta != null)&lt;br /&gt;
		executeTransformation(transMeta, local, remote, cluster, preview,&lt;br /&gt;
				debug, replayDate, safe);&lt;/p&gt;

&lt;p&gt;	JobMeta jobMeta = getActiveJob();&lt;br /&gt;
	if (jobMeta != null)&lt;br /&gt;
		executeJob(jobMeta, local, remote, replayDate, safe, null, 0);&lt;br /&gt;
}&lt;/p&gt;

&lt;p&gt;public void executeJob(JobMeta jobMeta, boolean local, boolean remote,&lt;br /&gt;
		Date replayDate, boolean safe, String startCopyName, int startCopyNr) {&lt;br /&gt;
	try {&lt;br /&gt;
		delegates.jobs.executeJob(jobMeta, local, remote, replayDate, safe,&lt;br /&gt;
				startCopyName, startCopyNr);&lt;br /&gt;
	} catch (Exception e) {&lt;br /&gt;
		new ErrorDialog(shell, &quot;Execute job&quot;,&lt;br /&gt;
				&quot;There was an error during job execution&quot;, e);&lt;br /&gt;
	}&lt;br /&gt;
}
&lt;/p&gt;

&lt;p&gt;runFile()方法内部调用executeFile()方法，executeFile方法有以下几个参数：&lt;br /&gt;
local：是否本地运行&lt;br /&gt;
remote：是否远程运行&lt;br /&gt;
cluster：是否集群环境运行&lt;br /&gt;
preview：是否预览&lt;br /&gt;
debug：是否调试&lt;br /&gt;
replayDate：回放时间&lt;br /&gt;
safe：是否安全模式&lt;/p&gt;

&lt;p&gt;executeFile方法会先获取当前激活的转换，如果获取结果不为空，则执行该转换；否则获取当前激活的作业，执行该作业。 本文主要讨论作业的执行过程，关于转换的执行过程，之后单独一篇文章进行讨论。&lt;/p&gt;

&lt;p&gt;executeJob委托SpoonJobDelegate执行其内部的executeJob方法，注意，其将JobMeta传递给了executeJob方法。SpoonJobDelegate还保存着对Spoon的引用。&lt;/p&gt;

&lt;p&gt;SpoonJobDelegate的executeJob方法主要完成以下操作：&lt;br /&gt;
1.设置Spoon的执行配置JobExecutionConfiguration类，该类设置变量、仓库、是否执行安全模式、日志等级等等。&lt;br /&gt;
2.获得当前Job对应的图形类JobGraph。&lt;br /&gt;
3.将执行配置类JobExecutionConfiguration的变量、参数、命令行参数设置给jobMeta。&lt;br /&gt;
4.如果本地执行，则调用jobGraph.startJob(executionConfiguration)，如果远程执行，则委托给SpoonSlaveDelegate执行。&lt;/p&gt;

&lt;p&gt;JobExecutionConfiguration类是保存job执行过程中的一些配置，该类会在Spoon、JobGraph类之间传递。&lt;/p&gt;

&lt;p&gt;本文只讨论本地执行的情况，故往下查看jobGraph.startJob(executionConfiguration)方法。该方法被synchronized关键字修饰。&lt;/p&gt;

&lt;p&gt;JobGraph类包含当前Spoon类的引用、以及对Job的引用。初始情况，Job的引用应该为null。该类会做以下操作：&lt;br /&gt;
1.如果job为空或者没有运行或者没有激活，则先保存，然后往下执行作业。&lt;br /&gt;
2.在仓库不为空的时候，通过仓库加载Job获得一个运行时的JobMeta对象，名称为runJobMeta；否则，通过文件名称直接new一个JobMeta对象，名称也为runJobMeta。&lt;br /&gt;
3.通过仓库和runJobMeta对象构建一个Job对象，并将jobMeta对象（此对象通过JobGraph构造方法传入）的变量、参数共享给Job对象。&lt;br /&gt;
4.Job对象添加JobEntry监听器、Job监听器。&lt;br /&gt;
5.调用Job的start方法，启动线程开始执行一个job。&lt;/p&gt;

&lt;p&gt;Job继承自Thread类，该类的run方法内部会递归执行该作业内部的作业项，限于篇幅，本文不做深究。&lt;/p&gt;
<strong>原创文章，转载请注明：</strong>转载自：<a href='http://blog.javachen.com/Pentaho/2012/02/22/the-init-process-before-job-execution.html'>Kettle运行作业之前的初始化过程</a></content>
 </entry>
 
 <entry>
   <title>The execution process of kettle’s job</title>
   <link href="http://blog.javachen.com/Pentaho/2012/02/21/the-execution-process-of-kettles-job.html"/>
   <updated>2012-02-21T00:00:00+08:00</updated>
   <id>http://blog.javachen.com/Pentaho/2012/02/21/the-execution-process-of-kettles-job</id>
   <content type="html">&lt;p&gt;How to execute a kettle job in Spoon GUI or command line after we create a job in Spoon GUI? In Spoon GUI,the main class is &quot;org.pentaho.di.ui.spoon.Spoon.java&quot;.This class handles the main window of the Spoon graphical transformation editor.Many operations about a job or transformation such as run,debug,preview,zoomIn,etc,are all in this class.This post just writes about the code execution process.&lt;/p&gt;

&lt;p&gt;When we start a job or transformation,Spoon invokes the method runFile(),and then is distributed to executeTransformation() or executeJob().At now,we mainly study about executeJob() method.&lt;/p&gt;

&lt;p&gt;This is a simple sequence diagram below.It contains several classes for Starting to execute a job using execute(int nr, Result result) in Job.java.We can see the relation of these classes from it.&lt;/p&gt;

&lt;p&gt;&lt;div class=&quot;pic&quot;&gt;
&lt;a href=&quot;http://blog.javachen.com/files/2012/02/spoon-execute-sequence.jpg&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;http://blog.javachen.com/files/2012/02/spoon-execute-sequence-300x180.jpg&quot; alt=&quot;&quot; title=&quot;spoon execute sequence&quot; width=&quot;300&quot; height=&quot;180&quot; class=&quot;aligncenter size-medium wp-image-2511&quot; /&gt;&lt;/a&gt;
&lt;/div&gt;&lt;/p&gt;

&lt;p&gt;What is the detail process of job execution? You should look into the Job.run() method for detail information.&lt;/p&gt;
<strong>原创文章，转载请注明：</strong>转载自：<a href='http://blog.javachen.com/Pentaho/2012/02/21/the-execution-process-of-kettles-job.html'>The execution process of kettle’s job</a></content>
 </entry>
 
 <entry>
   <title>kettle中定义错误处理</title>
   <link href="http://blog.javachen.com/Pentaho/2012/02/17/step-error-handling-in-kettle.html"/>
   <updated>2012-02-17T00:00:00+08:00</updated>
   <id>http://blog.javachen.com/Pentaho/2012/02/17/step-error-handling-in-kettle</id>
   <content type="html">&lt;p&gt;在kettle执行的过程中，如果遇到错误，kettle会停止运行。在某些时候，并不希望kettle停止运行，这时候可以使用错误处理（Step Error Handling）。错误处理允许你配置一个步骤来取代出现错误时停止运行一个转换，出现错误的记录行将会传递给另一个步骤。在Step error handling settings对话框里，需要设置启用错误处理。&lt;/p&gt;

&lt;p&gt;下面例子中读取postgres数据库中的a0表数据，然后输出到a1表：
&lt;div class=&quot;pic&quot;&gt;
&lt;img alt=&quot;&quot; src=&quot;http://ww2.sinaimg.cn/mw600/48e24b4cjw1dq56wck3m7j.jpg&quot; class=&quot;alignnone&quot; width=&quot;600&quot; height=&quot;172&quot; /&gt;
&lt;/div&gt;&lt;/p&gt;

&lt;p&gt;a1表结构如下：
&lt;pre lang=&quot;sql&quot;&gt;
CREATE TABLE a1
(
  a double precision,
  id integer NOT NULL,
  CONSTRAINT id_pk PRIMARY KEY (id ),
  CONSTRAINT id_unin UNIQUE (id )
)
&lt;/pre&gt;&lt;/p&gt;

&lt;p&gt;从表结构可以看出，a1表中id为主键、唯一。
&lt;!--more--&gt;&lt;/p&gt;

&lt;p&gt;a0表数据预览：
&lt;div class=&quot;pic&quot;&gt;
&lt;img alt=&quot;&quot; src=&quot;http://ww4.sinaimg.cn/mw600/48e24b4cjw1dq56wcr6c2j.jpg&quot; class=&quot;alignnone&quot; width=&quot;553&quot; height=&quot;403&quot; /&gt;
&lt;/div&gt;&lt;/p&gt;

&lt;p&gt;现在a1表数据为空，执行上面的转换，执行成功之后，a1表数据和a0表数据一致。&lt;br /&gt;
再次执行，上面的转换会报错，程序停止运行，会报主键重复的异常。&lt;/p&gt;

&lt;p&gt;现在，我想报错之后，程序继续往下执行，并记录错误的记录的相关信息，这时候可以使用“定义错误处理”的功能。&lt;br /&gt;
在“表输出”的步骤上右键选择“定义错误处理”，弹出如下对话框。
&lt;div class=&quot;pic&quot;&gt;
&lt;img src=&quot;http://ww3.sinaimg.cn/mw600/48e24b4cjw1dq56wd5ckwj.jpg&quot; alt=&quot;&quot; /&gt;
&lt;/div&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;相关字段说明：&lt;/strong&gt;
目标步骤：指定处理错误的步骤&lt;br /&gt;
启用错误处理？：设置是否启用错误处理&lt;br /&gt;
错误数列名：出错的记录个数&lt;br /&gt;
错误描述列名：描述错误信息的列名称&lt;br /&gt;
错误列的列名：出错列的名称&lt;br /&gt;
错误编码列名：描述错误的代码的列名&lt;br /&gt;
允许的最大错误数：允许的最大错误数，超过此数，不在处理错误&lt;br /&gt;
允许的最大错误百分比：&lt;br /&gt;
在计算百分百前最少要读入的行数：&lt;/p&gt;

&lt;p&gt;添加错误处理后的转换如下：
&lt;div class=&quot;pic&quot;&gt;
&lt;img src=&quot;http://ww4.sinaimg.cn/mw600/48e24b4cjw1dq56wdntipj.jpg&quot; alt=&quot;&quot; /&gt;
&lt;/div&gt;&lt;/p&gt;

&lt;p&gt;记录错误信息的字段列表如下，可以看出，errorNum、errorDesc、errorName、errorCode都是在定义错误处理时候填入的列名称，a、id来自于输入的记录的列。
&lt;div class=&quot;pic&quot;&gt;
&lt;img src=&quot;http://ww2.sinaimg.cn/mw600/48e24b4cjw1dq56wdvk6uj.jpg&quot; alt=&quot;&quot; /&gt;
&lt;/div&gt;&lt;/p&gt;

&lt;p&gt;记录的错误信息如下：
&lt;div class=&quot;pic&quot;&gt;
&lt;img src=&quot;http://ww4.sinaimg.cn/mw600/48e24b4cjw1dq56we2sn2j.jpg&quot; alt=&quot;&quot; /&gt;
&lt;/div&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;分析&lt;/strong&gt;
可以看到,错误日志里只是记录了出错的行里面的信息，并没有记录当前行所在的表名称以及执行时间等等，如果能够对此进行扩展，则该错误日志表才能更有实际意义。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;说明&lt;/strong&gt;
1.错误日志的错误码含义（如：TOP001）含义见参考文章2.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;参考文章&lt;/strong&gt;
1.&lt;a href=&quot;http://wiki.pentaho.com/display/EAI/.09+Transformation+Steps#.09TransformationSteps-StepErrorHandling&quot; target=&quot;_blank&quot;&gt;Step Error Handling&lt;/a&gt;

2.&lt;a href=&quot;http://wiki.pentaho.com/display/COM/Step+error+handling+codes&quot; target=&quot;_blank&quot;&gt;Step error handling codes&lt;/a&gt;&lt;/p&gt;
<strong>原创文章，转载请注明：</strong>转载自：<a href='http://blog.javachen.com/Pentaho/2012/02/17/step-error-handling-in-kettle.html'>kettle中定义错误处理</a></content>
 </entry>
 
 <entry>
   <title>JSF中EL表达式之this扩展</title>
   <link href="http://blog.javachen.com/JSF/2012/02/14/this-expression-of-jsf-el.html"/>
   <updated>2012-02-14T00:00:00+08:00</updated>
   <id>http://blog.javachen.com/JSF/2012/02/14/this-expression-of-jsf-el</id>
   <content type="html">&lt;p&gt;本篇文章来自以前公司的一套jsf+seam+Hibernate的一套框架，其对jsf进行了一些改进，其中包括:EL表达式中添加this，通过jsf的渲染实现权限控制到按钮等等。JSF表达式中添加this，主要是为了在facelets页面使用this关键字引用（JSF自动查找）到当前页面对应的pojo类，详细说明见下午。因为，本文的文章是公司同事整理的，本文作者仅仅是将其分享出来，供大家参考思路，如果有什么不妥的话，请告知。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;EL表达式this扩展&lt;/strong&gt;
在业务系统中，大量页面具有大量区域是相似或者相同的，或者可能根据某些局部特征的变化具有一定的变化，jsf中通过facelet模板功能可以达到一定程度的页面重用，从而减轻开发人员编辑和拷贝一些页面代码，达到重用的目的。然而，她们具有如下限制：&lt;br /&gt;
1.Java语言作为一种典型的OO语言，通过抽象、继承等功能，可以大量重用已经实现或者在父类中已经存在的属性和方法等。模板技术作为一种静态加载和内容替换，无法充分利用面向对象的继承功能&lt;br /&gt;
2.由于Jsf/jsp框架采用视图和动作分离的模型，多个相似功能在不同的页面实现中由于页面对应点动作类不同因而必须使用复制的方法；&lt;br /&gt;
3.模板中使用EL表达式与后台动作类交互，这种交互是基于绝对名称的，不同的网页对应的动作类是完全不同的，因此很难重用和利用面向对象的特征。&lt;/p&gt;

&lt;p&gt;我们需要一种新的功能，实现：&lt;br /&gt;
1.模板的应用特种可以参照OO的继承特种，即模板的对模板的引用可以看成一种继承，这种继承可以和java的OO是一致的&lt;br /&gt;
2.多个页面和多个独立java后台程序相同部分完全可以抽离出来，不依赖它们是否继承关系、只需保证他们具有相同的属性或者方法&lt;br /&gt;
3.动态映射功能，即在满足上述基础上可以实现页面和后台实现类的属性和方法的自动映射&lt;br /&gt;
4.兼容标准的EL表达式&lt;/p&gt;

&lt;p&gt;我们将上述功能处理为“this”表达式。其功能模型为：
&lt;div class=&quot;pic&quot;&gt;
&lt;a href=&quot;http://blog.javachen.com/files/2012/02/this-expression-of-el.jpg&quot;&gt;&lt;img src=&quot;http://blog.javachen.com/files/2012/02/this-expression-of-el-300x168.jpg&quot; alt=&quot;&quot; title=&quot;this expression of el&quot; width=&quot;300&quot; height=&quot;168&quot; class=&quot;aligncenter size-medium wp-image-2496&quot; /&gt;&lt;/a&gt;
&lt;/div&gt;
页面A和页面B分别引用了通用功能T,内含this相关的El表达式，通过分析处理，分别映射到对应的页面动作类的属性A.name和B.name。A和B可以从相同的基类C派生而来，只需C类实现了name属性即可，A类和B类也可以毫不相关，但是它们具有相同的属性name。
&lt;!--more--&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;动作类和页面的一致性保证&lt;/strong&gt;
为了有效实现this表达式，我们实现如下映射规则：&lt;br /&gt;
1.名称为小写方式，不管页面如何命名，对应的后台类的jsf标识符都转换为小写&lt;br /&gt;
2.页面和相应的后台类以相同命名方式，页面的目录转化为后台类的包名，名称通过点分隔包名，如根目录的a.xhtml对应的后台类名称为A.java，其唯一jsf标识名称为“a”，test/b.xhtml的后台类为test/B.java，其唯一jsf标识为“test.b”&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;“this”EL表达式算法&lt;/strong&gt;
算法流程如下图：
&lt;div class=&quot;pic&quot;&gt;
&lt;a href=&quot;http://blog.javachen.com/files/2012/02/this-expression-flow-of-el.jpg&quot;&gt;&lt;img src=&quot;http://blog.javachen.com/files/2012/02/this-expression-flow-of-el-300x226.jpg&quot; alt=&quot;&quot; title=&quot;this expression flow of el&quot; width=&quot;300&quot; height=&quot;226&quot; class=&quot;aligncenter size-medium wp-image-2497&quot; /&gt;&lt;/a&gt;
&lt;/div&gt;&lt;/p&gt;
<strong>原创文章，转载请注明：</strong>转载自：<a href='http://blog.javachen.com/JSF/2012/02/14/this-expression-of-jsf-el.html'>JSF中EL表达式之this扩展</a></content>
 </entry>
 
 <entry>
   <title>Ext读取xml文件生成动态表格和表单(续)</title>
   <link href="http://blog.javachen.com/Extjs/2012/01/31/ext_readxml_in_bjsasc_wuzi_continue.html"/>
   <updated>2012-01-31T00:00:00+08:00</updated>
   <id>http://blog.javachen.com/Extjs/2012/01/31/ext_readxml_in_bjsasc_wuzi_continue</id>
   <content type="html">&lt;p&gt;很多人向我要《&lt;a href=&quot;http://blog.javachen.com/2009/10/ext_readxml_in_bjsasc_wuzi&quot; target=&quot;_blank&quot;&gt;Ext读取xml文件生成动态表格和表单&lt;/a&gt;》一文的源代码，故花了些时间将源代码整理出来，并重新编写此文，分享当时的技术思路。&lt;/p&gt;

&lt;p&gt;《Ext读取xml文件生成动态表格和表单》一文需要的文件有：&lt;br /&gt;
1.html文件，此处以SASC.search.MtrUse.html为例&lt;br /&gt;
2.Extjs相关文件,见SASC.search.MtrUse.html文件中的引用&lt;br /&gt;
3.工具类，DomUtils.js&lt;br /&gt;
4.核心js类:SASC.extjs.search.MtrUse.js&lt;br /&gt;
5.java代码&lt;/p&gt;

&lt;p&gt;详细html和js代码见相关文件，这里先描述思路。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;首先&lt;/strong&gt;
通过一个事件打开一个弹出窗口，该窗口的url指向SASC.search.MtrUse.html文件，并附带参数xmlFile，xmlFile的值为xml文件名称，其存于服务器的某一路径下面。如：“../SASC.search.MtrUse.html?xmlFile=PC_MTRREPLACE_IMP.xml” .PC_MTRREPLACE_IMP.xml文件的放置路径见DomUtils.js文件中的说明。&lt;/p&gt;

&lt;p&gt;在这里，前台会读取该xml生成ext界面，后天会从xml文件读取sql语句等信息，详细信息见java代码。&lt;br /&gt;
进入SASC.search.MtrUse.html页面，执行ext的初始化方法时，会先通过当前页面的url中获取xmlFile参数的值（调用getForwardXmlUrl(getQsValue('xmlFile'))），得到xml文件的服务器路径，然后通过javascript的解析该xml文件，渲染出ext界面,这部分代码见SASC.extjs.search.MtrUse.js文件内的initStoreData(xmlObj) 方法。&lt;br /&gt;
需要说明的是，xml文件是按照一定规律编写的，详细的参考xml文件内容，以及解析xml文件的相关方法。你可以重新定义该xml的结构，然后修改解析xml文件的方法。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;然后&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;初始化完ext界面之后，会获取表格数据，这部分使用了struts，这不是本文重点，故不做介绍。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;最后&lt;/strong&gt;
相关文件打包见：
&lt;a href=&quot;http://vdisk.weibo.com/s/2enQS&quot; target=&quot;_blank&quot;&gt;http://vdisk.weibo.com/s/2enQS&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;说明&lt;/strong&gt;
如果还有什么不懂，欢迎email：javachen.june#gmail.com&lt;/p&gt;
<strong>原创文章，转载请注明：</strong>转载自：<a href='http://blog.javachen.com/Extjs/2012/01/31/ext_readxml_in_bjsasc_wuzi_continue.html'>Ext读取xml文件生成动态表格和表单(续)</a></content>
 </entry>
 
 <entry>
   <title>使用kettle数据迁移添加主键和索引</title>
   <link href="http://blog.javachen.com/Pentaho/2012/01/05/add-primary-keys-and-indexes-when-migrating-datas-whith-kettle.html"/>
   <updated>2012-01-05T00:00:00+08:00</updated>
   <id>http://blog.javachen.com/Pentaho/2012/01/05/add-primary-keys-and-indexes-when-migrating-datas-whith-kettle</id>
   <content type="html">&lt;p&gt;Kettle是一款国外开源的etl工具，纯java编写，绿色无需安装，主要用于&lt;strong&gt;数据抽取、转换、装载&lt;/strong&gt;。kettle兼容了市面上几十种数据库，故用kettle来做数据库的迁移视乎是个不错的选择。&lt;/p&gt;

&lt;p&gt;kettle的数据抽取主要在于抽取数据，而没有考虑数据库的&lt;strong&gt;函数、存储过程、视图、表结构以及索引、约束&lt;/strong&gt;等等，而这些东西恰恰都是数据迁移需要考虑的事情。当然，如果在不考虑数据库中的函数、存储过程、视图的情况下，使用kettle进行数据的迁移还算是一个可行的方案。&lt;/p&gt;

&lt;p&gt;这篇文章主要是讲述在使用kettle进行数据库的迁移的时候如何迁移主键和索引，为什么要迁移主键和索引？异构数据库之间的迁移很难无缝的实现自定义函数、存储过程、视图、表结构、索引、约束以及数据的迁移，所以多数情况下只需要异构数据库之间类型兼容、数据一致就可以了。但是在有些情况下需要对输出表进行查询以及数据比对的时候，&lt;strong&gt;需要有主键和索引方便对比和加快查询速度&lt;/strong&gt;。
&lt;p style=&quot;text-align: left;&quot;&gt;先来看看kettle中的一些组件。&lt;/p&gt;

&lt;p&gt;下图是kettle中的一个表输出组件。
&lt;div class=&quot;pic&quot;&gt;
&lt;a href=&quot;http://blog.javachen.com/files/2012/01/kettle-table-out.png&quot;&gt;&lt;img class=&quot;size-medium wp-image-2480 aligncenter&quot; title=&quot;kettle-table-out&quot; src=&quot;http://blog.javachen.com/files/2012/01/kettle-table-out-269x300.png&quot; alt=&quot;kettle中的表输出组件&quot; width=&quot;269&quot; height=&quot;300&quot; /&gt;&lt;/a&gt;
&lt;/div&gt;&lt;/p&gt;

&lt;p&gt;在该组件里可以指定表名、字段等信息，并且还可以建表的sql语句。打开建表的sql语句，你可以看到该语句里只指定了字段名称和类型，没有指定主外键、约束、和索引。显然，该组件只是完成了数据的输出并没有将表的主键迁移过去。
&lt;!--more--&gt;
下图是kettle中纬度更新/查询的组件。
&lt;div class=&quot;pic&quot;&gt;
&lt;a href=&quot;http://blog.javachen.com/files/2012/01/kettle-look-up.png&quot;&gt;&lt;img class=&quot;size-medium wp-image-2481 aligncenter&quot; title=&quot;kettle-look-up&quot; src=&quot;http://blog.javachen.com/files/2012/01/kettle-look-up-292x300.png&quot; alt=&quot;kettle中纬度更新/查询的组件&quot; width=&quot;292&quot; height=&quot;300&quot; /&gt;&lt;/a&gt;
&lt;/div&gt;
该组件可以指定输出表名、映射字段、纬度字段、并且指定主键（图中翻译为关键字段），该组件比表输出组件多了一个功能，即指定主键。&lt;/p&gt;
从上面两个组件中可以看出，kettle实际上预留了设置主键的接口，具体的接口说明需要查看api或者源代码，只是kettle没有智能的查处输入表的主键字段，而是需要用户在kettle ui界面指定一个主键名称。&lt;/p&gt;

&lt;p&gt;如果现在想使用kettle实现&lt;strong&gt;异构数据库的数据以及主键和索引的迁移&lt;/strong&gt;，有没有一个完整方便的解决方案呢？我能想到的解决方案如下：
&lt;strong&gt;1.&lt;/strong&gt;使用kettle向导中的多表复制菜单进行数据库的迁移，这只能实现数据的迁移还需要额外的方法添加主键和索引，你可以手动执行一些脚步添加约束。
&lt;strong&gt;2.&lt;/strong&gt;针对源数据库中的每一张表创建一个转换，转换中使用纬度更新/查询组件，在该主键中指定主键。创建完所有的转换之后，创建一个作业将这些转换串联起来即可。
&lt;strong&gt;3.&lt;/strong&gt;扩展kettle向导中的多表复制菜单里的功能，在该功能创建的作业中添加一些节点用于添加输出表的主键和索引。这些节点可以是执行sql语句的主键，故只需要通过jdbc代码获取添加主键和索引的sql语句。&lt;/p&gt;

&lt;p&gt;方案1需要单独执行脚步实现添加主键和索引，创建或生成这些脚步需要些时间；方案2需要针对每个表认为的指定主键，工作量大，而且无法实现添加索引；方案3最容易实现和扩展。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;下面是方案3的具体的实现。&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;首先需要在每一个表的建表语句节点和复制数据节点之后添加一个执行sql语句的节点，该节点用于添加主键和索引。&lt;br /&gt;
多表复制向导的核心代码在src-db/org.pentaho.di.ui.spoon.delegates.SpoonJobDelegate.java的public void ripDBWizard()方法中。该方法如下：
&lt;pre lang=&quot;java&quot;&gt;public void ripDBWizard(final int no) {
	final List databases = spoon.getActiveDatabases();
	if (databases.size() == 0)
		return;&lt;/pre&gt;&lt;/p&gt;

&lt;p&gt;	final RipDatabaseWizardPage1 page1 = new RipDatabaseWizardPage1(&quot;1&quot;,&lt;br /&gt;
			databases);&lt;br /&gt;
	final RipDatabaseWizardPage2 page2 = new RipDatabaseWizardPage2(&quot;2&quot;);&lt;br /&gt;
	final RipDatabaseWizardPage3 page3 = new RipDatabaseWizardPage3(&quot;3&quot;,&lt;br /&gt;
			spoon.getRepository());&lt;br /&gt;
	Wizard wizard = new Wizard() {&lt;br /&gt;
		public boolean performFinish() {&lt;br /&gt;
			try {&lt;br /&gt;
				JobMeta jobMeta = ripDBByNo(no, databases,&lt;br /&gt;
					page3.getJobname(), page3.getRepositoryDirectory(),&lt;br /&gt;
					page3.getDirectory(), page1.getSourceDatabase(),&lt;br /&gt;
					page1.getTargetDatabase(), page2.getSelection());&lt;/p&gt;

&lt;p&gt;				if (jobMeta == null)&lt;br /&gt;
					return false;&lt;/p&gt;

&lt;p&gt;				if (page3.getRepositoryDirectory() != null) {&lt;br /&gt;
					spoon.saveToRepository(jobMeta, false);&lt;br /&gt;
				} else {&lt;br /&gt;
					spoon.saveToFile(jobMeta);&lt;br /&gt;
				}&lt;/p&gt;

&lt;p&gt;				addJobGraph(jobMeta);&lt;br /&gt;
				return true;&lt;br /&gt;
			} catch (Exception e) {&lt;br /&gt;
				new ErrorDialog(spoon.getShell(), &quot;Error&quot;,&lt;br /&gt;
						&quot;An unexpected error occurred!&quot;, e);&lt;br /&gt;
				return false;&lt;br /&gt;
			}&lt;br /&gt;
		}&lt;/p&gt;

&lt;p&gt;		public boolean canFinish() {&lt;br /&gt;
			return page3.canFinish();&lt;br /&gt;
		}&lt;br /&gt;
	};&lt;/p&gt;

&lt;p&gt;	wizard.addPage(page1);&lt;br /&gt;
	wizard.addPage(page2);&lt;br /&gt;
	wizard.addPage(page3);&lt;/p&gt;

&lt;p&gt;	WizardDialog wd = new WizardDialog(spoon.getShell(), wizard);&lt;br /&gt;
	WizardDialog.setDefaultImage(GUIResource.getInstance().getImageWizard());&lt;br /&gt;
	wd.setMinimumPageSize(700, 400);&lt;br /&gt;
	wd.updateSize();&lt;br /&gt;
	wd.open();&lt;br /&gt;
}
该方法主要是创建一个向导，该向导中包括三个向导页，第一个向导页用于&lt;strong&gt;选择数据库连接&lt;/strong&gt;：源数据库和目标数据库连接；第二个向导页用于&lt;strong&gt;选表&lt;/strong&gt;；第三个向导页用于&lt;strong&gt;指定作业保存路径&lt;/strong&gt;。在向导完成的时候，即performFinish()方法里，会根据选择的数据源和表生成一个作业，即JobMeta对象。&lt;br /&gt;
创建Jobmeta的方法为：
&lt;pre lang=&quot;java&quot;&gt;public JobMeta ripDB(final List databases,final String jobname, final
    RepositoryDirectoryInterface repdir,final String directory, final DatabaseMeta
    sourceDbInfo,final DatabaseMeta targetDbInfo, final String[] tables){
 //此处省略若干代码
}&lt;/pre&gt;
该方法主要逻辑在下面代码内：
&lt;pre lang=&quot;java&quot;&gt;IRunnableWithProgress op = new IRunnableWithProgress() {
	public void run(IProgressMonitor monitor)
	 throws InvocationTargetException, InterruptedException {
           //此处省略若干代码
        }
}&lt;/pre&gt;
上面代码中有以下代码用于遍历所选择的表生成作业中的一些节点：
&lt;pre lang=&quot;java&quot;&gt;for (int i = 0; i &amp;lt; tables.length &amp;amp;&amp;amp; !monitor.isCanceled(); i++) {
    //此处省略若干代码
}&lt;/pre&gt;
针对每一张表先会创建一个JobEntrySQL节点，然后创建一个转换JobEntryTrans，可以在创建转换之后再创建一个JobEntrySQL节点，该节点用于添加主键和索引。&lt;br /&gt;
这部分的代码如下：
&lt;pre lang=&quot;java&quot;&gt;String pksql = JdbcDataMetaUtil.exportPkAndIndex(
		sourceDbInfo, sourceCon, tables[i],
		targetDbInfo, targetCon, tables[i]);&lt;/pre&gt;&lt;/p&gt;

&lt;p&gt;if (!Const.isEmpty(pksql)) {&lt;br /&gt;
	location.x += 300;&lt;br /&gt;
	JobEntrySQL jesql = new JobEntrySQL(&lt;br /&gt;
		BaseMessages.getString(PKG,&quot;Spoon.RipDB.AddPkAndIndex&quot;)&lt;br /&gt;
			+ tables[i] + &quot;]&quot;);&lt;br /&gt;
	jesql.setDatabase(targetDbInfo);&lt;br /&gt;
	jesql.setSQL(pksql);&lt;br /&gt;
	jesql.setDescription(BaseMessages.getString(PKG,&lt;br /&gt;
			&quot;Spoon.RipDB.AddPkAndIndex&quot;)&lt;br /&gt;
			+ tables[i]&lt;br /&gt;
			+ &quot;]&quot;);&lt;/p&gt;

&lt;p&gt;	JobEntryCopy jecsql = new JobEntryCopy();&lt;br /&gt;
	jecsql.setEntry(jesql);&lt;br /&gt;
	jecsql.setLocation(new Point(location.x, location.y));&lt;br /&gt;
	jecsql.setDrawn();&lt;br /&gt;
	jobMeta.addJobEntry(jecsql);&lt;/p&gt;

&lt;p&gt;	// Add the hop too...&lt;br /&gt;
	JobHopMeta jhi = new JobHopMeta(previous, jecsql);&lt;br /&gt;
	jobMeta.addJobHop(jhi);&lt;br /&gt;
	previous = jecsql;&lt;br /&gt;
}
获取添加主键和索引的sql语句，主要是采用jdbc的方式读取两个数据库，判断源数据库的表中是否存在主键和索引，如果有则返回添加主键或索引的sql语句。这部分代码封装在JdbcDataMetaUtil类中。&lt;br /&gt;
该代码见：&lt;a href=&quot;https://gist.github.com/1564353.js&quot; target=&quot;_blank&quot;&gt;https://gist.github.com/1564353.js&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;最后的效果图如下：
&lt;div class=&quot;pic&quot;&gt;
&lt;a href=&quot;http://blog.javachen.com/files/2012/01/kettle-add-primary-key-and-indexes.png&quot;&gt;&lt;img class=&quot;aligncenter size-medium wp-image-2483&quot; title=&quot;kettle-add-primary-key-and-indexes&quot; src=&quot;http://blog.javachen.com/files/2012/01/kettle-add-primary-key-and-indexes-300x79.png&quot; alt=&quot;&quot; width=&quot;300&quot; height=&quot;79&quot; /&gt;&lt;/a&gt;
&lt;/div&gt;&lt;/p&gt;

&lt;p&gt;&lt;div class=&quot;infor&quot;&gt;说明：&lt;br /&gt;
1.以上代码使用的是jdbc的方法获取主键或索引，不同的数据库的jdbc驱动实现可能不同而且不同数据库的语法可能不同，故上面代码可能有待完善。&lt;br /&gt;
2.如果一个数据库中存在多库并且这多个库中有相同的表，使用上面的代码针对一个表名会查出多个主键或索引。这一点也是可以改善的&lt;/div&gt;&lt;/p&gt;
<strong>原创文章，转载请注明：</strong>转载自：<a href='http://blog.javachen.com/Pentaho/2012/01/05/add-primary-keys-and-indexes-when-migrating-datas-whith-kettle.html'>使用kettle数据迁移添加主键和索引</a></content>
 </entry>
 
 <entry>
   <title>kettle进行数据迁移遇到的问题</title>
   <link href="http://blog.javachen.com/Pentaho/2012/01/04/some-problems-about-migrating-database-datas-with-kettle.html"/>
   <updated>2012-01-04T00:00:00+08:00</updated>
   <id>http://blog.javachen.com/Pentaho/2012/01/04/some-problems-about-migrating-database-datas-with-kettle</id>
   <content type="html">&lt;p&gt;使用kettle进行oracle或db2数据导入到mysql或postgres数据库过程中遇到以下问题，以下只是一个简单描述，详细的说明以及所做的代码修改没有提及。下面所提到的最新的pdi程序是我修改kettle源码并编译之后的版本。&lt;/p&gt;

&lt;p&gt;1. &lt;strong&gt;同时运行两个pdi程序&lt;/strong&gt;，例如：一个为oracle到mysql，另一个为oracle到postgres，其中一个停止运行
&lt;strong&gt;原因：&lt;/strong&gt;从oracle迁移到mysql创建的作业和转换文件和oracle到postgres的作业和转换保存到一个路径，导致同名称的转换相互之间被覆盖，故在运行时候会出现混乱。
&lt;strong&gt;解决办法：&lt;/strong&gt;将新建的作业和转换分别保存在两个不同的路径，最好是新建两个不同路径的仓库，关于如何新建仓库，请参考《kettle使用说明》文档。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;2. 关键字的问题。&lt;/strong&gt;Oracle初始化到mysql，关键字前面会加上前缀“MY_”。如果在建表的时候出现错误，则需要检查表的字段中是否有关键字。
&lt;strong&gt;解决办法：&lt;/strong&gt;出差的表单独进行处理，新建一个转换，实现关键字段该名称然后初始化出错的表。具体操作参见文档。&lt;/p&gt;

&lt;p&gt;oracle中的字段名从中可以有#号，但是到mysql会报错
&lt;strong&gt;解决办法：&lt;/strong&gt;字段改名称，去掉#号&lt;/p&gt;

&lt;p&gt;3. &lt;strong&gt;Db2初始化到mysql或是postgres出错&lt;/strong&gt;
&lt;strong&gt;原因：&lt;/strong&gt;1）db2数据库连接用户没有权限访问出错的表；2）出错的表名存在小写字母
&lt;strong&gt;解决办法：&lt;/strong&gt;使用更新后的pdi程序，更新后的程序会将db2的表名使用双引号括起来。&lt;/p&gt;

&lt;p&gt;4. Oracle到mysql和pg时日期类型数据值有偏差
&lt;strong&gt;原因：&lt;/strong&gt;从oracle中读取日期类型的数据时候，读取结果与oracle数据库中的数据已经存在偏差。少数记录使用oracle10g的驱动读取数据少一个小时，用oracle11g的驱动会多一个小时，该问题尚待oracle工程师给出解决方案。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;5. 主键&lt;/strong&gt;从ORACLE导入不到MYSQL和POSTGRES
&lt;strong&gt;原因：&lt;/strong&gt;pdi程序中没有对主键进行处理
&lt;strong&gt;解决办法：&lt;/strong&gt;使用更新的pdi程序，执行Tools-Wizzard-Copy Tables Extension...功能添加主键；执行Tools-Wizzard-Copy Tables Data Only...功能可以只复制数据&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;6. Oracle中存在ascii字符导入到postgres时候报错&lt;/strong&gt;：ERROR: invalid byte sequence for encoding &quot;UTF8&quot;: 0x00
&lt;strong&gt;原因：&lt;/strong&gt;PostgreSQL内部采用C语言风格的字符串（以0x00）表示结尾，因而不允许字符串中包括0x00，建议在转换时先对字符串类型的数据进行清洗，也就是增加一个节点用于删除字符串数据中的特殊字符0x00。
&lt;strong&gt;解决办法:&lt;/strong&gt;使用新的pdi程序。&lt;br /&gt;
在kettle的DataBase类中修改PreparedStatement.setString(int index,String value)方法传入的参数，将value的值trim之后在setString&lt;/p&gt;

&lt;p&gt;7. &lt;strong&gt;异构数据库之间的类型兼容问题&lt;/strong&gt;。日期类型和时间类型的数据初始化到mysql或postgres中都为时间类型的数据，导致数据对比时候数据不一致。
&lt;strong&gt;原因：&lt;/strong&gt;Pdi程序中的类型转换采用的是向上兼容的方式，故日期和时间类型都转换为时间类型数据。
&lt;strong&gt;解决办法：&lt;/strong&gt;针对与db2数据初始化到mysql和postgres，该问题在最新的pdi程序中已经处理。因为oracle中的日期类型字段既可以存日期又可以存时间，故没针对oracle数据做出处理。&lt;/p&gt;

&lt;p&gt;8. Db2中没有主键的数据初始化到mysql和postgres需要添加&lt;strong&gt;索引&lt;/strong&gt;
&lt;strong&gt;解决办法：&lt;/strong&gt;使用最新的pdi程序，最新的pdi程序会添加主键和索引。&lt;/p&gt;

&lt;p&gt;9. Db2中decimal（n,m）类型的数据初始化到postgres数据库被&lt;strong&gt;四舍五入&lt;/strong&gt;。
&lt;strong&gt;原因：&lt;/strong&gt;Db2中decimal（n,m）类型的数据初始化到postgres中的类型不对。
&lt;strong&gt;解决办法：&lt;/strong&gt;使用最新的pdi程序。&lt;/p&gt;

&lt;p&gt;10. 导数据中途时没有报错，直接软件退出
&lt;strong&gt;原因：&lt;/strong&gt;1）jvm内存溢出，需要修改jvm参数；2）pdi程序报swt错误
&lt;strong&gt;解决办法：&lt;/strong&gt;修改jvm参数&lt;/p&gt;

&lt;p&gt;11.&lt;strong&gt;初次使用kettle做db2的初始化会报错&lt;/strong&gt;
&lt;strong&gt;原因：&lt;/strong&gt;kettle中的db2的jdbc驱动与使用的db2版本不对应。
&lt;strong&gt;解决办法：&lt;/strong&gt;从db2的安装目录下拷贝jdbc驱动到kettle目录（libext/JDBC）下&lt;/p&gt;
<strong>原创文章，转载请注明：</strong>转载自：<a href='http://blog.javachen.com/Pentaho/2012/01/04/some-problems-about-migrating-database-datas-with-kettle.html'>kettle进行数据迁移遇到的问题</a></content>
 </entry>
 
 <entry>
   <title>在eclipse中构建Pentaho BI Server工程</title>
   <link href="http://blog.javachen.com/Pentaho/2011/09/28/build-pentaho-bi-server-source-code-in-eclipse.html"/>
   <updated>2011-09-28T00:00:00+08:00</updated>
   <id>http://blog.javachen.com/Pentaho/2011/09/28/build-pentaho-bi-server-source-code-in-eclipse</id>
   <content type="html">&lt;p&gt;首先需要说明的是，Pentaho BI Server源代码在&lt;em&gt;svn://source.pentaho.org/svnroot/bi-platform-v2/trunk/&lt;/em&gt;，并且用ivy构建。ivy没有用过也不熟悉，故不打算从这里使用ivy构建源码。&lt;br /&gt;
当然，您可以参考&lt;a href=&quot;http://wiki.pentaho.com/display/ServerDoc2x/Building+and+Debugging+Pentaho+with+Eclipse&quot; target=&quot;_blank&quot;&gt;官方文档&lt;/a&gt;构建源码。&lt;/p&gt;

&lt;p&gt;Pentaho BI Server打包后的文件存于&lt;a href=&quot;http://sourceforge.net/projects/pentaho/files/Business%20Intelligence%20Server/&quot; target=&quot;_blank&quot;&gt;这里&lt;/a&gt;，其中包括（本文使用的是3.9.0版本）：biserver-ce-3.9.0-stable.zip，bi-platform-3.9.0-stable-sources.zip，biserver-ce-3.9.0-stable-javadoc.zip。
&lt;!--more--&gt;
将biserver-ce-3.9.0-stable.zip解压之后执行&lt;em&gt;biserver-ce/start-pentaho.bat&lt;/em&gt;（或是再linux环境下：&lt;em&gt;biserver-ce/start-pentaho.sh&lt;/em&gt;），即可成功启动biserver。现在我想将这个工程导入到eclipse然后调式跟踪代码，怎么做呢？&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;以下操作是在eclipse3.7+tomcat 6.20的环境中进行的。&lt;/strong&gt;
在eclipse中创建一个web项目，名称为pentaho，然后将&lt;em&gt;biserver-ce/tomcat/webapps&lt;/em&gt;下的&lt;code&gt;pentaho-style&lt;/code&gt;和&lt;code&gt;sw-style&lt;/code&gt;拷贝到你的tomcat 6服务器的webapps目录下，将pentaho文件下的所有文件拷贝到工程下的WebContent目录下。由于biserver需要访问pentaho-solutions下的文件，故还需要修改&lt;code&gt;WEB-INF/web.xml&lt;/code&gt;文件你的以下配置，用于指定pentaho-solutions的路径：
&lt;pre&gt;
&lt; context-param &gt;
	&lt; param-name &gt;solution-path&lt; /param-name&gt;
	&lt; param-value &gt;/home/june.chan/opt/biserver-ce/pentaho-solutions&lt; /param-value&gt;
&lt; /context-param &gt;
&lt;/pre&gt;
现在即可部署项目，运行&lt;code&gt;biserver-ce/data/start_hypersonic.bat&lt;/code&gt;（用于启动数据库），然后启动tomcat，就可以通过&lt;em&gt;http://localhost:8080/pentaho&lt;/em&gt;访问biserver。如果启动报错，需要将hsqldb-1.8.0.7.jar包，拷贝到应用路径下（&lt;em&gt;\tomcat-pci-test\biserver-ce\tomcat\webapps\pentaho\WEB-INF\lib&lt;/em&gt;）。&lt;br /&gt;
现在可以看到biserver的登录页面，但是还是没有看到biserver的源代码。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;接下来，构建源代码。&lt;/strong&gt;
在biserver-ce/tomcat/webapps/pentaho/WEB-INF/lib下面有很多名称为pentaho-bi-platform-########-3.9.0-stable.jar的jar文件，这些即是biserver源码编译之后的class文件。在bi-platform-3.9.0-stable-sources.zip压缩文件你即可以看到这些class文件的源代码。将这些src包解压然后拷贝到之前新建的pentaho工程的src目录下。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;font color=&quot;red&quot;&gt;需要注意的是：&lt;/font&gt;&lt;/strong&gt;
1.这些src jar包你只报告java文件，不包括配置文件：log4j配置文件，hibernate配置和实体映射文件，ehcache配置文件&lt;br /&gt;
2.上面的配置文件需要到biserver-ce/tomcat/webapps/pentaho/WEB-INF/lib目录下的pentaho-bi-platform-########-3.9.0-stable.jar文件中寻找。&lt;br /&gt;
3.
&lt;pre&gt;	
&lt;ul&gt;
&lt;li&gt;biserver-ce/tomcat/webapps/pentaho/WEB-INF/lib/pentaho-bi-platform-engine-security-3.9.0-stable.jar文件中有ldap的配置文件，&lt;/li&gt;
   	&lt;li&gt;biserver-ce/tomcat/webapps/pentaho/WEB-INF/lib/pentaho-bi-platform-engine-services-3.9.0-stable.jar文件中有ehcache的配置文件，&lt;/li&gt;
	&lt;li&gt;biserver-ce/tomcat/webapps/pentaho/WEB-INF/lib/pentaho-bi-platform-plugin-actions-3.9.0-stable.jar文件中有log4j的配置文件，&lt;/li&gt;
&lt;li&gt; biserver-ce/tomcat/webapps/pentaho/WEB-INF/lib/pentaho-bi-platform-repository-3.9.0-stable.jar文件中有hibernate配置文件，&lt;/li&gt;
&lt;li&gt;biserver-ce/tomcat/webapps/pentaho/WEB-INF/lib/pentaho-bi-platform-security-userroledao-3.9.0-stable.jar文件中有hibernated的实体映射文件。&lt;/li&gt;
&lt;/ul&gt;
&lt;/pre&gt;
4.biserver-ce-3.9.0-stable.zip的lib（biserver-ce/tomcat/webapps/pentaho/WEB-INF/lib）目录下的servlete jar包的版本为2.3，版本过低需要替换为更高版本知道源码中不在有servlete编译错误
&lt;/p&gt;
<strong>原创文章，转载请注明：</strong>转载自：<a href='http://blog.javachen.com/Pentaho/2011/09/28/build-pentaho-bi-server-source-code-in-eclipse.html'>在eclipse中构建Pentaho BI Server工程</a></content>
 </entry>
 
 <entry>
   <title>Pentaho现场支持遇到问题及解决办法</title>
   <link href="http://blog.javachen.com/Pentaho/2011/09/26/resolved-pentaho-problems-9-16.html"/>
   <updated>2011-09-26T00:00:00+08:00</updated>
   <id>http://blog.javachen.com/Pentaho/2011/09/26/resolved-pentaho-problems-9-16</id>
   <content type="html">&lt;p&gt;很久没写文章了，最近在关注Pentaho。&lt;br /&gt;
 以下是9月16日现场提出的问题解决办法：&lt;br /&gt;
      1、PDF预览中文没显示，txt预览中文乱码：&lt;br /&gt;
           1）、设置File-&gt;Configuration -&gt;output-pageable-pdf的encoding 为Identity-H&lt;br /&gt;
           2）、将需要输出中文的报表项目的字体设置为中文字体，例如宋体&lt;br /&gt;
           3）、如要发布到服务器，需要修改如下的配置：&lt;br /&gt;
             pentaho/server/biserver-ee/tomcat/webapps/pentaho/WEB-INF/classes/classic-engine.properties：&lt;br /&gt;
             org.pentaho.reporting.engine.classic.core.modules.output.pageable.pdf.Encoding=Identity-H&lt;br /&gt;
     2、实现文件拷贝方式发布报表&lt;br /&gt;
           可以通过文件方式发布，只要将报表的prpt文件拷贝到Solution的目录（Pentaho安装路径的server\biserver-ee\pentaho-solutions）下就可以了&lt;br /&gt;
     3、报表链接参数传递问题&lt;br /&gt;
          由于参数带中文造成的，可以对参数的值URLENCODE(&quot;value&quot;; &quot;utf-8&quot;)来解决&lt;br /&gt;
     4、查询参数缺省值问题&lt;br /&gt;
          关于日期的默认值。可以使用报表系统提供的日期变量设置，如TODAY，DATE，YEAR。。。&lt;br /&gt;
     5、实现在pie chart上显示文字&lt;br /&gt;
          以把label默认显示的百分比改为文字：label-formate = {0}， 但是label显示百分比，同时在pie图的划分区域显示文字是不能的。&lt;br /&gt;
     6、报表集成时候垂直滚动条是否可以去掉&lt;br /&gt;
          改变报表的高度：报表设计器 file-page setup&lt;br /&gt;
     7、报表中的chart不能导出到Excel2007&lt;br /&gt;
          目前为系统bug，excel2003能够正常导出&lt;br /&gt;
     8、实现隔行换色&lt;br /&gt;
          选中Details中的field再attribute面板上设置name的名称（如“row-band”），然后通过Format--&gt;Row-Banding，可以设置Visible Color 、Inisible Color，再Element中输入&quot;row-band&quot;&lt;br /&gt;
     9、显示top N  ：托一个message field，在里面输入表达式，如，$（topn）,topn为传入的参数&lt;/p&gt;
<strong>原创文章，转载请注明：</strong>转载自：<a href='http://blog.javachen.com/Pentaho/2011/09/26/resolved-pentaho-problems-9-16.html'>Pentaho现场支持遇到问题及解决办法</a></content>
 </entry>
 
 <entry>
   <title>在Fedora 15 上搭建Eucalyptus</title>
   <link href="http://blog.javachen.com/cloud computing/2011/08/20/install-eucalyptus-on-fedora-15.html"/>
   <updated>2011-08-20T00:00:00+08:00</updated>
   <id>http://blog.javachen.com/cloud computing/2011/08/20/install-eucalyptus-on-fedora-15</id>
   <content type="html">&lt;p&gt;&lt;div class=&quot;pic&quot;&gt;&lt;img src=&quot;http://open.eucalyptus.com/themes/eucalyptus/img/eucalyptus_logo_awh.png&quot; alt=&quot;&quot; /&gt;&lt;/div&gt;
在Fedora 15 上搭建Eucalyptus平台，在Fedora 15 上搭建Eucalyptus与在Centos上搭建Eucalyptus有什么区别呢？参照这篇文章&lt;a href=&quot;http://open.eucalyptus.com/wiki/EucalyptusInstallationFedora_v2.0&quot; target=&quot;_blank&quot;&gt;Installing Eucalyptus (2.0) on Fedora 12&lt;/a&gt;，然后注意一些细节，视乎就能安装成功。不管你信不信，我是在虚拟机中安装fedora15，然后安装Eucalyptus失败了，失败的原因是xen的网络没有配置好，查看资源的时候free / max都为0000.&lt;/p&gt;

&lt;p&gt;毕竟是第一次接触云计算，第一次接触XEN，第一次接触Eucalyptus，Eucalyptus改装的都装了，就是XEN的网络没有配置好，当时很是迷糊。在接触了OpenNebula 和OpenStack之后，横向对比，视乎明白了很多千丝万缕的关联与奥秘。在安装OpenNebula，最主要是安装OpenStack成功之后，想到了之前Eucalyptus安装失败的原因。限于现在精力不在云计算上，暂且不去重新安装Eucalyptus，等之后再去尝试。下次尝试，定是醍醐灌顶，行云流水，很是期待。&lt;/p&gt;

&lt;p&gt;如果你也在Fedora上安装Eucalyptus平台，咱们可以交流交流，等到时机成熟，会将在Fedora 15 上搭建Eucalyptus的过程及遇到的问题发表在博客上；如果你想研究Eucalyptus平台java部分的代码，咱们也可以彼此分享各自的心得。&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;
<strong>原创文章，转载请注明：</strong>转载自：<a href='http://blog.javachen.com/cloud computing/2011/08/20/install-eucalyptus-on-fedora-15.html'>在Fedora 15 上搭建Eucalyptus</a></content>
 </entry>
 
 
</feed>
