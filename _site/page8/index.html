
<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8"/>
    <title>JavaChen Blog</title>
    <meta name="description" content="开源、Java、Pentaho、Hadoop、Cassandra以及数据可视化"/>
    <meta name="keywords" content="pentaho、kettle、hadoop、hdfs、hive、hbase、mapreduce、cassandra、openstack、OpenNebula、Eucalyptus、fedora、linux、vim、extjs、dhtmlx、spring、javascript"/>
    <meta name="author" content="JavaChen"/>
    <meta name="copyright" content="© http://blog.javachen.com" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
    <meta name="baidu-site-verification" content="ECLZpXwkOR" />

    <!-- HTML5 shim, for IE6-8 support of HTML elements -->
    <!--[if lt IE 9]>
      <script src="http://html5shim.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->

    <link href="//netdna.bootstrapcdn.com/bootstrap/3.0.0/css/bootstrap.min.css" rel="stylesheet" />
    <link href="//netdna.bootstrapcdn.com/font-awesome/3.2.1/css/font-awesome.css " rel="stylesheet" />
    <link href="/assets/themes/javachen/css/style.css?body=1" rel="stylesheet" type="text/css" media="all" />
    <link href="/assets/themes/javachen/css/pygments.css" rel="stylesheet" type="text/css" media="all" />
    <link href="/assets/themes/javachen/fancybox/jquery.fancybox.css?v=2.1.5" rel="stylesheet" media="all" />

    <!-- fav and touch icons -->
    <link rel="shortcut icon" href="/favicon.ico" />
    <link rel="canonical" href="http://blog.javachen.com/page8/" />
    <!-- Update these with your own images
    <link rel="shortcut icon" href="images/favicon.png" />
    <link rel="apple-touch-icon" href="images/apple-touch-icon.png" />
    <link rel="apple-touch-icon" sizes="72x72" href="images/apple-touch-icon-72x72.png" />
    <link rel="apple-touch-icon" sizes="114x114" href="images/apple-touch-icon-114x114.png" />
    -->

    <!-- atom & rss feed -->
    <link href="/atom.xml" type="application/atom+xml" rel="alternate" title="JavaChen Blog ATOM Feed" />
    <link href="/rss.xml" type="application/rss+xml" rel="alternate" title="JavaChen Blog RSS Feed" />
  </head>

  <body>
    <nav class="navbar navbar-default navbar-fixed-top" role="navigation">
      <div class="container">
      <!-- Brand and toggle get grouped for better mobile display -->
      <div class="navbar-header">
        <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-ex1-collapse">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand" href="/" title="JavaChen Blog">JavaChen Blog</a>
      </div>

      <!-- Collect the nav links, forms, and other content for toggling -->
      <div class="collapse navbar-collapse navbar-ex1-collapse">
        <ul class="nav navbar-nav">
          
          
          


  
    
      
      	
      	<li><a href="/tags.html">Tags</a></li>
      	
      
    
  
    
      
    
  
    
      
    
  
    
  
    
      
      	
      	<li><a href="/categories.html">Categories</a></li>
      	
      
    
  
    
      
      	
      	<li><a href="/archive.html">Archive</a></li>
      	
      
    
  
    
      
    
  
    
      
    
  
    
      
    
  
    
      
      	
      	<li><a href="/about.html">About</a></li>
      	
      
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  




        </ul>

        <ul class="nav navbar-nav navbar-right">
          <li><a href="http://blog.javachen.com/rss.xml" target="_blank" title="RSS"><span class="icon-rss icon-large"></span></a></li>
          
            <li><a href="https://github.com/javachen" target="_blank" title="Github"><span class="icon-github icon-large"></span></a></li>
          
          
          
          
          
            <li><a href="http://weibo.com/chenzhijun" target="_blank" title="Weibo"><span class="icon-weibo icon-large"></span></a></li>
          
        </ul>
      </div><!-- /.navbar-collapse -->
      </div>
    </nav>

    <div class="container">
      <div class="content">
        
<div class="row">

  <div class="col-md-9">
    <article class="page-header-wrapper">
      <header class="page-header">
        <h2>  <small>Life is short,we need passion.</small></h2>
      </header>
	


<article>
    <header>
    	<h1 class="entry-title headline"><a href="/hadoop/2013/03/24/manual-install-Cloudera-Hadoop-CDH">手动安装Cloudera Hadoop CDH</a></h1>
    </header>
    
      <h1>安装版本</h1>
<div class="highlight"><pre><code class="text language-text" data-lang="text">hadoop-2.0.0-cdh4.2.0
hbase-0.94.2-cdh4.2.0
hive-0.10.0-cdh4.2.0
jdk1.6.0_38
</code></pre></div>
<h1>安装前说明</h1>

<ul>
<li>安装目录为/opt</li>
<li>检查hosts文件</li>
<li>关闭防火墙</li>
<li>设置时钟同步</li>
</ul>

<h1>使用说明</h1>

<p>安装hadoop、hbase、hive成功之后启动方式为：</p>

<ul>
<li>启动dfs和mapreduce: desktop1上执行start-dfs.sh和start-yarn.sh</li>
<li>启动hbase: desktop3上执行start-hbase.xml</li>
<li>启动hive: desktop1上执行hive</li>
</ul>

<h1>规划</h1>
<div class="highlight"><pre><code class="text language-text" data-lang="text">    192.168.0.1             NameNode、Hive、ResourceManager
    192.168.0.2             SSNameNode
    192.168.0.3             DataNode、HBase、NodeManager
    192.168.0.4             DataNode、HBase、NodeManager
    192.168.0.6             DataNode、HBase、NodeManager
    192.168.0.7             DataNode、HBase、NodeManager
    192.168.0.8             DataNode、HBase、NodeManager
</code></pre></div>
<h1>部署过程</h1>

<h2>系统和网络配置</h2>

<ol>
<li><p>修改每台机器的名称</p>

<p>[root@desktop1 ~]# cat /etc/sysconfig/network
NETWORKING=yes
HOSTNAME=desktop1</p></li>
<li><p>在各个节点上修改/etc/hosts增加以下内容:</p></li>
</ol>
<div class="highlight"><pre><code class="text language-text" data-lang="text">    [root@desktop1 ~]# cat /etc/hosts
    127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4
    ::1         localhost localhost.localdomain localhost6 localhost6.localdomain6
    192.168.0.1     desktop1
    192.168.0.2     desktop2
    192.168.0.3     desktop3
    192.168.0.4     desktop4
    192.168.0.6     desktop6
    192.168.0.7     desktop7
    192.168.0.8     desktop8
</code></pre></div>
<ol>
<li>配置ssh无密码登陆
以下是设置desktop1上可以无密码登陆到其他机器上。</li>
</ol>
<div class="highlight"><pre><code class="text language-text" data-lang="text">    [root@desktop1 ~]# ssh-keygen
    [root@desktop1 ~]# ssh-copy-id -i .ssh/id_rsa.pub desktop2
    [root@desktop1 ~]# ssh-copy-id -i .ssh/id_rsa.pub desktop3
    [root@desktop1 ~]# ssh-copy-id -i .ssh/id_rsa.pub desktop4
    [root@desktop1 ~]# ssh-copy-id -i .ssh/id_rsa.pub desktop6
    [root@desktop1 ~]# ssh-copy-id -i .ssh/id_rsa.pub desktop7
    [root@desktop1 ~]# ssh-copy-id -i .ssh/id_rsa.pub desktop8
</code></pre></div>
<ol>
<li>每台机器上关闭防火墙：</li>
</ol>
<div class="highlight"><pre><code class="text language-text" data-lang="text">    [root@desktop1 ~]# service iptables stop
</code></pre></div>
<h1>安装Hadoop</h1>

<h2>配置Hadoop</h2>

<p>将jdk1.6.0_38.zip上传到/opt，并解压缩。
将hadoop-2.0.0-cdh4.2.0.zip上传到/opt，并解压缩。</p>

<p>在NameNode上配置以下文件：</p>
<div class="highlight"><pre><code class="text language-text" data-lang="text">core-site.xml fs.defaultFS指定NameNode文件系统，开启回收站功能。
hdfs-site.xml 
    dfs.namenode.name.dir指定NameNode存储meta和editlog的目录，
    dfs.datanode.data.dir指定DataNode存储blocks的目录，
    dfs.namenode.secondary.http-address指定Secondary NameNode地址。
    开启WebHDFS。
slaves 添加DataNode节点主机
</code></pre></div>
<ol>
<li>core-site.xml
该文件指定fs.defaultFS连接desktop1，即NameNode节点。</li>
</ol>
<div class="highlight"><pre><code class="text language-text" data-lang="text">[root@desktop1 hadoop]# pwd
/opt/hadoop-2.0.0-cdh4.2.0/etc/hadoop
[root@desktop1 hadoop]# cat core-site.xml 
&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
&lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&gt;
&lt;configuration&gt;
&lt;!--fs.default.name for MRV1 ,fs.defaultFS for MRV2(yarn) --&gt;
&lt;property&gt;
     &lt;name&gt;fs.defaultFS&lt;/name&gt;
         &lt;!--这个地方的值要和hdfs-site.xml文件中的dfs.federation.nameservices一致--&gt;
     &lt;value&gt;hdfs://desktop1&lt;/value&gt;
&lt;/property&gt;
&lt;property&gt;
&lt;name&gt;fs.trash.interval&lt;/name&gt;
&lt;value&gt;10080&lt;/value&gt;
&lt;/property&gt;
&lt;property&gt;
&lt;name&gt;fs.trash.checkpoint.interval&lt;/name&gt;
&lt;value&gt;10080&lt;/value&gt;
&lt;/property&gt;
&lt;/configuration&gt;
</code></pre></div>
<ol>
<li>hdfs-site.xml
该文件主要设置数据副本保存份数，以及namenode、datanode数据保存路径以及http-address。</li>
</ol>
<div class="highlight"><pre><code class="text language-text" data-lang="text">[root@desktop1 hadoop]# cat hdfs-site.xml 
&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
&lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&gt;
&lt;configuration&gt;
&lt;property&gt;
  &lt;name&gt;dfs.replication&lt;/name&gt;
  &lt;value&gt;1&lt;/value&gt;
&lt;/property&gt;
&lt;property&gt;
  &lt;name&gt;hadoop.tmp.dir&lt;/name&gt;
  &lt;value&gt;/opt/data/hadoop-${user.name}&lt;/value&gt;
&lt;/property&gt;
&lt;property&gt;
&lt;name&gt;dfs.namenode.http-address&lt;/name&gt;
&lt;value&gt;desktop1:50070&lt;/value&gt;
&lt;/property&gt;
&lt;property&gt;
&lt;name&gt;dfs.namenode.secondary.http-address&lt;/name&gt;
&lt;value&gt;desktop2:50090&lt;/value&gt;
&lt;/property&gt;
&lt;property&gt;
&lt;name&gt;dfs.webhdfs.enabled&lt;/name&gt;
&lt;value&gt;true&lt;/value&gt;
&lt;/property&gt;
&lt;/configuration&gt;
</code></pre></div>
<ol>
<li>masters
设置namenode和secondary namenode节点。</li>
</ol>
<div class="highlight"><pre><code class="text language-text" data-lang="text">[root@desktop1 hadoop]# cat masters 
desktop1
desktop2
</code></pre></div>
<ol>
<li>slaves
设置哪些机器上安装datanode节点。</li>
</ol>
<div class="highlight"><pre><code class="text language-text" data-lang="text">[root@desktop1 hadoop]# cat slaves 
desktop3
desktop4
desktop6
desktop7
desktop8
</code></pre></div>
<h2>配置MapReduce</h2>

<ol>
<li>mapred-site.xml
配置使用yarn计算框架，以及jobhistory的地址。</li>
</ol>
<div class="highlight"><pre><code class="text language-text" data-lang="text">[root@desktop1 hadoop]# cat mapred-site.xml
&lt;?xml version=&quot;1.0&quot;?&gt;
&lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&gt;
&lt;configuration&gt;
&lt;property&gt;
 &lt;name&gt;mapreduce.framework.name&lt;/name&gt;
 &lt;value&gt;yarn&lt;/value&gt;
&lt;/property&gt;
&lt;property&gt;
 &lt;name&gt;mapreduce.jobhistory.address&lt;/name&gt;
 &lt;value&gt;desktop1:10020&lt;/value&gt;
&lt;/property&gt;
&lt;property&gt;
 &lt;name&gt;mapreduce.jobhistory.webapp.address&lt;/name&gt;
 &lt;value&gt;desktop1:19888&lt;/value&gt;
&lt;/property&gt;
&lt;/configuration&gt;
</code></pre></div>
<ol>
<li>yarn-site.xml
主要配置resourcemanager地址以及<code>yarn.application.classpath</code>（这个路径很重要，要不然集成hive时候会提示找不到class）</li>
</ol>
<div class="highlight"><pre><code class="text language-text" data-lang="text">[root@desktop1 hadoop]# cat yarn-site.xml 
&lt;?xml version=&quot;1.0&quot;?&gt;
&lt;configuration&gt;
&lt;property&gt;
    &lt;name&gt;yarn.resourcemanager.resource-tracker.address&lt;/name&gt;
    &lt;value&gt;desktop1:8031&lt;/value&gt;
  &lt;/property&gt;
  &lt;property&gt;
    &lt;name&gt;yarn.resourcemanager.address&lt;/name&gt;
    &lt;value&gt;desktop1:8032&lt;/value&gt;
  &lt;/property&gt;
  &lt;property&gt;
    &lt;name&gt;yarn.resourcemanager.scheduler.address&lt;/name&gt;
    &lt;value&gt;desktop1:8030&lt;/value&gt;
  &lt;/property&gt;
  &lt;property&gt;
    &lt;name&gt;yarn.resourcemanager.admin.address&lt;/name&gt;
    &lt;value&gt;desktop1:8033&lt;/value&gt;
  &lt;/property&gt;
  &lt;property&gt;
    &lt;name&gt;yarn.resourcemanager.webapp.address&lt;/name&gt;
    &lt;value&gt;desktop1:8088&lt;/value&gt;
  &lt;/property&gt;
  &lt;property&gt;
    &lt;description&gt;Classpath for typical applications.&lt;/description&gt;
    &lt;name&gt;yarn.application.classpath&lt;/name&gt;
    &lt;value&gt;$HADOOP_CONF_DIR,$HADOOP_COMMON_HOME/share/hadoop/common/*,
    $HADOOP_COMMON_HOME/share/hadoop/common/lib/*,
    $HADOOP_HDFS_HOME/share/hadoop/hdfs/*,$HADOOP_HDFS_HOME/share/hadoop/hdfs/lib/*,
    $YARN_HOME/share/hadoop/yarn/*,$YARN_HOME/share/hadoop/yarn/lib/*,
    $YARN_HOME/share/hadoop/mapreduce/*,$YARN_HOME/share/hadoop/mapreduce/lib/*&lt;/value&gt;
  &lt;/property&gt;
  &lt;property&gt;
    &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;
    &lt;value&gt;mapreduce.shuffle&lt;/value&gt;
  &lt;/property&gt;
  &lt;property&gt;
    &lt;name&gt;yarn.nodemanager.aux-services.mapreduce.shuffle.class&lt;/name&gt;
    &lt;value&gt;org.apache.hadoop.mapred.ShuffleHandler&lt;/value&gt;
  &lt;/property&gt;
  &lt;property&gt;
    &lt;name&gt;yarn.nodemanager.local-dirs&lt;/name&gt;
    &lt;value&gt;/opt/data/yarn/local&lt;/value&gt;
  &lt;/property&gt;
  &lt;property&gt;
    &lt;name&gt;yarn.nodemanager.log-dirs&lt;/name&gt;
    &lt;value&gt;/opt/data/yarn/logs&lt;/value&gt;
  &lt;/property&gt;
  &lt;property&gt;
    &lt;description&gt;Where to aggregate logs&lt;/description&gt;
    &lt;name&gt;yarn.nodemanager.remote-app-log-dir&lt;/name&gt;
    &lt;value&gt;/opt/data/yarn/logs&lt;/value&gt;
  &lt;/property&gt;
  &lt;property&gt;
    &lt;name&gt;yarn.app.mapreduce.am.staging-dir&lt;/name&gt;
    &lt;value&gt;/user&lt;/value&gt;
 &lt;/property&gt;
&lt;/configuration&gt;
</code></pre></div>
<h2>同步配置文件</h2>

<p>修改.bashrc环境变量，并将其同步到其他几台机器，并且source .bashrc</p>
<div class="highlight"><pre><code class="text language-text" data-lang="text">[root@desktop1 ~] # cat .bashrc 
# .bashrc
alias rm=&#39;rm -i&#39;
alias cp=&#39;cp -i&#39;
alias mv=&#39;mv -i&#39;
# Source global definitions
if [ -f /etc/bashrc ]; then
        . /etc/bashrc
fi
# User specific environment and startup programs
export LANG=zh_CN.utf8
export JAVA_HOME=/opt/jdk1.6.0_38
export JRE_HOME=$JAVA_HOME/jre
export CLASSPATH=./:$JAVA_HOME/lib:$JRE_HOME/lib:$JRE_HOME/lib/tools.jar
export HADOOP_HOME=/opt/hadoop-2.0.0-cdh4.2.0
export HIVE_HOME=/opt/hive-0.10.0-cdh4.2.0
export HBASE_HOME=/opt/hbase-0.94.2-cdh4.2.0
export HADOOP_MAPRED_HOME=${HADOOP_HOME}
export HADOOP_COMMON_HOME=${HADOOP_HOME}
export HADOOP_HDFS_HOME=${HADOOP_HOME}
export YARN_HOME=${HADOOP_HOME}
export HADOOP_YARN_HOME=${HADOOP_HOME}
export HADOOP_CONF_DIR=${HADOOP_HOME}/etc/hadoop
export HDFS_CONF_DIR=${HADOOP_HOME}/etc/hadoop
export YARN_CONF_DIR=${HADOOP_HOME}/etc/hadoop
export PATH=$PATH:$HOME/bin:$JAVA_HOME/bin:$HADOOP_HOME/sbin:$HBASE_HOME/bin:$HIVE_HOME/bin
</code></pre></div>
<p>修改配置文件之后，使其生效。</p>
<div class="highlight"><pre><code class="text language-text" data-lang="text">[root@desktop1 ~]# source .bashrc 
</code></pre></div>
<p>将desktop1上的/opt/hadoop-2.0.0-cdh4.2.0拷贝到其他机器上</p>

<h2>启动脚本</h2>

<p>第一次启动hadoop需要先格式化NameNode，该操作只做一次。当修改了配置文件时，需要重新格式化</p>
<div class="highlight"><pre><code class="text language-text" data-lang="text">[root@desktop1 hadoop]hadoop namenode -format
</code></pre></div>
<p>在desktop1上启动hdfs：</p>
<div class="highlight"><pre><code class="text language-text" data-lang="text">[root@desktop1 hadoop]#start-dfs.sh
</code></pre></div>
<p>在desktop1上启动mapreduce：</p>
<div class="highlight"><pre><code class="text language-text" data-lang="text">[root@desktop1 hadoop]#start-yarn.sh
</code></pre></div>
<p>在desktop1上启动historyserver：</p>
<div class="highlight"><pre><code class="text language-text" data-lang="text">[root@desktop1 hadoop]#mr-jobhistory-daemon.sh start historyserver
</code></pre></div>
<p>查看MapReduce：</p>
<div class="highlight"><pre><code class="text language-text" data-lang="text">http://desktop1:8088/cluster
</code></pre></div>
<p>查看节点：</p>
<div class="highlight"><pre><code class="text language-text" data-lang="text">http://desktop2:8042/
http://desktop2:8042/node
</code></pre></div>
<h2>检查集群进程</h2>
<div class="highlight"><pre><code class="text language-text" data-lang="text">[root@desktop1 ~]# jps
5389 NameNode
5980 Jps
5710 ResourceManager
7032 JobHistoryServer
[root@desktop2 ~]# jps
3187 Jps
3124 SecondaryNameNode
[root@desktop3 ~]# jps
3187 Jps
3124 DataNode
5711 NodeManager
</code></pre></div>
     

      <div class="status">
        
        <div class="clearfix"></div>
      </div>
</article>
<hr/>

<article>
    <header>
    	<h1 class="entry-title headline"><a href="/hadoop/2013/03/08/note-about-installing-hadoop-cluster">【笔记】Hadoop安装部署</a></h1>
    </header>
    
      <h1>安装虚拟机</h1>

<p>VirtualBox安装rhel6.3，存储为30G，内存为1G，并复制2份</p>

<h1>配置网络</h1>

<p>a. VirtualBox全局设定-网络中添加一个新的连接：vboxnet0</p>

<p>b. 设置每一个虚拟机的网络为Host-Only</p>

<p>c.分别修改每个虚拟机的ip，DHCP或手动设置</p>
<div class="highlight"><pre><code class="text language-text" data-lang="text">vim /etc/sysconfig/network-scripts/ifcfg-eth0
vim /etc/udev/rules.d/70-persistent-net.rules  #删掉第一个，修改第二个名字为eth0
start_udev
</code></pre></div>
<p>d.修改主机名</p>
<div class="highlight"><pre><code class="text language-text" data-lang="text">vim /etc/sysconfig/network
</code></pre></div>
<p>e.每个虚拟机中修改hosts：</p>
<div class="highlight"><pre><code class="text language-text" data-lang="text">192.168.56.100 rhel-june
192.168.56.101 rhel-june-1
192.168.56.102 rhel-june-2
</code></pre></div>
<p>最后机器列表为：</p>
<div class="highlight"><pre><code class="text language-text" data-lang="text">rhel-june:   192.168.56.100
rhel-june-1: 192.168.56.101
rhel-june-2: 192.168.56.102
</code></pre></div>
<h1>机群规划</h1>

<p>版本：</p>
<div class="highlight"><pre><code class="text language-text" data-lang="text">hadoop:1.1.1
JDK:1.6.0_38
</code></pre></div>
<p>集群各节点：</p>
<div class="highlight"><pre><code class="text language-text" data-lang="text">NameNode:192.168.56.100
NameSecondary:192.168.56.100
DataNode:192.168.56.101
DataNode:192.168.56.102
</code></pre></div>
<h1>安装过程</h1>

<p>a.解压缩到/opt</p>

<p>b.设置配置文件：</p>
<div class="highlight"><pre><code class="text language-text" data-lang="text">core-site.xml
hdfs-site.sml
mapred-site.xml
</code></pre></div>
<p>c.设置master、slaves</p>

<p>d.设置环境变量</p>

<p>方便执行java命令及hadoop命令. 使用root登录，vi ~/.bash_profile 追加下列信息</p>
<div class="highlight"><pre><code class="text language-text" data-lang="text">export JAVA_HOME=/opt/jdk1.6.0_38
export HADOOP_INSTALL=/opt/hadoop-1.1.1
export PATH=$PATH:$HADOOP_INSTALL/bin:$JAVA_HOME/bin
</code></pre></div>
<p>e.修改hadoop脚本中<code>JAVA_HOME</code>：/opt/hadoop-1.1.1/conf/hadoop-env.sh</p>

<p>f.格式化namenode</p>
<div class="highlight"><pre><code class="text language-text" data-lang="text">hadoop namenode -format
</code></pre></div>
<p>g.启动hdfs集群</p>
<div class="highlight"><pre><code class="text language-text" data-lang="text">sh /opt/hadoop-1.1.1/bin/start-all.sh
</code></pre></div>
<p>h.查看节点进程</p>
<div class="highlight"><pre><code class="text language-text" data-lang="text">jps
</code></pre></div>
<h1>查看状态</h1>
<div class="highlight"><pre><code class="text language-text" data-lang="text">http://rhel-june:50030/
http://rhel-june:50070/
</code></pre></div>
<h1>推荐</h1>

<ul>
<li>[]</li>
</ul>

     

      <div class="status">
        
        <div class="clearfix"></div>
      </div>
</article>
<hr/>

<article>
    <header>
    	<h1 class="entry-title headline"><a href="/work/2013/02/20/summary-of-the-work-in-2012">2012年度总结</a></h1>
    </header>
    
      <p>2012年是在公司工作的第二年，在总结2012年的得与失的时候，有必要和《2011的度年终总结》相比较，在比较中审视自己在2012年是否有改进2011年存在的不足、是否有实现2011年定下的2012年工作计划。
以下是2012年相对于2011年的一些变化。</p>

<ul>
<li> 2011年，在调研云计算产品过程中，深刻的意识到自身在linux方面存在的不足；2012年，熟悉了基本的linux命令，能够读懂并编写简单shell脚本；</li>
<li>  2011年，工作环境是win7+fedora；2012年，一直使用fedora操作系统工作、编码；</li>
<li>  2011年，较多的时间花在编写代码、完成开发任务上；2012年，更多的时间花在学习架构的设计、系统的运维、项目的管理上，视野不再局限于开发、精力不再局限于编码。</li>
<li>  2011年，在工作中没有及时提交项目周报，没有及时的跟踪、检查分配下去的任务完成情况，对新人的指导不够；2012年，没有写过项目周报，做到了及是跟踪、检查分配下去的任务完成情况；</li>
<li>  2011年，博客文章篇数较少，平时的总结与分享不够积极；2012年，很少有时间写技术方面的博客；</li>
<li>  2011年，在与客户的交流中底气不足、表达能力不够；2012年，还是发现自己与客户交流中胆怯、没有底气；</li>
<li>  2011年，希望能够将Pentaho的咨询服务工作更多交给其他人完成；2012年，发现大部分的工作还是落在自己身上一个人去完成，没有发挥其他人员的作用；</li>
<li>  2011年，希望2012年能够深入理解Spring、Jboss、Pentaho、缓存、云计算、架构等技术；2012年，了解gemfire、infinispan、jboss cache、cassandra等分布式缓存的实现及原理，但每一个方面都没有时间去深入研究和学习；</li>
<li>  2011年，公司在代码复查方面做的不够；2012年，这方面还是做的不够；</li>
<li>  2011年，项目开发方面没有形成一套成型的开发框架；2012年，还是没有看到一个成熟、易用、简单的开发框架以及相配套开发文档；</li>
<li>  2011年，项目于项目之间在一些同时使用的相关技术上面的沟通于交流做的不够；2012年，团队在项目上还是缺少沟通交流，尤其体现在XXXX项目网站开发上。</li>
<li>  2011年，花了一些时间在Pentaho上，并希望2012年能够创建一个Pentaho社区、一个QQ分享群；2012年，Pentaho方面基本上没有投入；</li>
<li>  2011年，编写文档时候，没有可参考的模版，导致文档编写不规范；2012年，每次写文档时都要去找文档模版；</li>
</ul>

<p>2012年参与了XXXXX项目、cassandra项目，。。。。。。此处省略314.15926个字。</p>

<p>2012年，公司也存在一些不足：上级对下级、项目经理对团队人员了解不足，不知道其工作上、生活上的内心想法以及遇到何种困难；多数情况下，团队自我要求低，积极性不高，没有生机与活力；对新人能力审核不够，对新人培养不够重视，对新人的存在感不够关注；在各个项目的人员安排及使用上、任务分配和工作计划上不合理，导致经常被动加班、熬夜等等。</p>

<h3>2013年工作计划：</h3>

<ul>
<li>通过CE考试，熟练掌握shell编程；</li>
<li>做好项目管理者的角色，培养新人，提高团队人员编码、处理问题的能力；</li>
<li>深入理解、学习cassandra源码、原理以及cassandra的运维；</li>
<li>学习hadoop的安装、部署、原理、开发及运维，掌握kettle和nosql的集成，希望积累几个hadoop项目经验；</li>
<li>学习分布式缓存理论知识，阅读源代码，完善缓存系统的监控及运维</li>
</ul>

<p>在经历了2011年和2012年之后，2013年希望自己能够专注细节，深入理解，在技术、管理、交际方面有所成长；希望公司能够重视对团队的培养，能够规范各种规章制度，能够更上一层楼！</p>

     

      <div class="status">
        
        <div class="clearfix"></div>
      </div>
</article>
<hr/>

<article>
    <header>
    	<h1 class="entry-title headline"><a href="/work/2012/06/03/migrate-blog-form-wordpress-to-github-with-octopress">使用Octopress将博客从wordpress迁移到GitHub上</a></h1>
    </header>
    
      <h1>Step1 - 在本机安装Octopress</h1>

<p>首先，必须先在本机安装配置<a href="http://git-scm.com/">Git</a>和<a href="https://rvm.beginrescueend.com/rvm/install/">Ruby</a>,Octopress需要Ruby版本至少为1.9.2。你可以使用<a href="http://rvm.beginrescueend.com/">RVM</a>或<a href="https://github.com/sstephenson/rbenv">rbenv</a>安装ruby，安装方法见Octopress官方文档：<a href="http://octopress.org/docs/setup/">http://octopress.org/docs/setup/</a></p>

<p>我使用rvm安装：
    rvm install 1.9.2 &amp;&amp; rvm use 1.9.2
安装完之后可以查看ruby版本：
    ruby --version
结果为：
    ruby 1.9.2p320 (2012-04-20 revision 35421) [x86_64-linux]</p>

<p>然后需要从github下载Octopress：
    git clone git://github.com/imathis/octopress.git octopress</p>

<p>因为我fork了Octopress，并在配置文件上做了一些修改，故我从我的仓库地址下载Octopress，命令如下：
    git clone git@github.com:javachen/octopress.git
运行上面的代码后，你会看到：
    Cloning into &#39;octopress&#39;...
    remote: Counting objects: 6579, done.
    remote: Compressing objects: 100% (2361/2361), done.
    remote: Total 6579 (delta 3773), reused 6193 (delta 3610)
    Receiving objects: 100% (6579/6579), 1.34 MiB | 35 KiB/s, done.
    Resolving deltas: 100% (3773/3773), done.</p>

<p>接下来进入octopress：
    cd octopress</p>

<p>接下来安装依赖：
    gem install bundler
    rbenv rehash    # If you use rbenv, rehash to be able to run the bundle command
    bundle install</p>

<p>安装Octopress默认的主题：
    rake install</p>

<p>你也可以安装自定义的主题，blog为主题名称：
    rake install[&#39;blog&#39;]</p>

<p>至此，Octopress所需的环境已经搭建成功。</p>

<h1>Step2 - 连接GitHub Pages</h1>

<p>首先，你得有一个GitHub的帐号，并且已经创建了一个新的Repository。如果你准备用自己的域名的话，Repository的名称可以随便取，不过正常人在正常情况下，一般都是以域名取名的。如果你没有自己的域名，GitHub是提供二级域名使用的，但是你得把Repository取名为<code>你的帐号.github.com</code>，并且，部署的时候会占用你的master分支。</p>

<p><em>Tips：</em>
如果用自己的一级域名，记得把source/CNAME文件内的域名改成你的一级域名，还有在dns管理中把域名的A Record指向IP：207.97.227.245；
如果用自己的二级域名，记得把source/CNAME文件内的域名改成你的二级域名，还有在dns管理中把域名的CNAME Record指向网址：charlie.github.com；
    echo &#39;your-domain.com&#39; &gt;&gt; source/CNAME
如果用GitHub提供的二级域名，记得把source/CNAME删掉。</p>

<p>完成上述准备工作后，运行：
    rake setup<u>github</u>pages
它会提示你输入有读写权限的Repository Url，这个在GitHub上可以找到。Url形如：https://github.com/javachen/javachen.github.com.git，javachen.github.com是我的Repository的名称。</p>

<h1>Step3 - 配置你的博客</h1>

<p>需要配置博客url、名称、作者、rss等信息。
    url: http://javachen.github.com
    title: JavaChen on Java
    subtitle: Just some random thoughts about technology,Java and life.
    author: javachen
    simple_search: http://google.com/search
    description:</p>
<div class="highlight"><pre><code class="text language-text" data-lang="text">date_format: &quot;%Y年%m月%d日&quot;

subscribe_rss: /atom.xml
subscribe_email:
email:

# 如果你使用的是一个子目录，如http://site.com/project，则设置为&#39;root: /project&#39;
root: /
# 文章标题格式
permalink: /:year/:month/:day/:title/
source: source
destination: public
plugins: plugins
code_dir: downloads/code
# 分类存放路径
category_dir: categories
markdown: rdiscount
pygments: false # default python pygments have been replaced by pygments.rb
</code></pre></div>
<h1>Step4 - 部署</h1>

<p>先把整个项目静态化，然后再部署到GitHub：
    rake generate
    rake deploy
当你看到“Github Pages deploy complete”后，就表示你大功已成。Enjoy!</p>

<p><em>Tips：</em>
Octopress提供的所有rake方法，可以运行<code>rake -T</code>查看。
如果在执行上述命令中ruby报错，则需要一一修复错误，这一步是没有接触过ruby的人比较苦恼的。</p>

<h1>Step5 - 从Wordpress迁移到Octopress</h1>

<h2>备份</h2>

<h3>备份评论内容</h3>

<p>Octopress由于是纯静态，所以没有办法存储用户评论了，我们可以使用DISQUS提供的“云评论”服务。首先安装DISQUS的WordPress插件，在插件设置中我们可以将现有的评论内容导入到DISQUS中。DISQUS处理导入数据的时间比较长，往往需要24小时甚至以上的时间。</p>

<h3>备份文章内容</h3>

<p>在WordPress后台我们可以将整站数据备份成一个.xml文件下载下来。同时，我原先文章中的图片都是直接在Wordpress后台上传的，所以要把服务器上<code>wp-content/uploads</code>下的所有文件备份下来。</p>

<h2>迁移</h2>

<h3>迁移文章</h3>

<p>jekyll本身提供了一个从WordPress迁移文章的工具，不过对中文实在是不太友好。这里我使用了YORKXIN的修改版本。将上面备份的wordpress.xml放到Octopress根目录，把脚本放到新建的utils目录中，然后运行：
    ruby -r &quot;./utils/wordpressdotcom.rb&quot; -e &quot;Jekyll::WordpressDotCom.process&quot;
于是转换好的文章都放进source目录了。</p>

<h3>迁移URL</h3>

<p>迁移URL，便是要保证以前的文章链接能够自动重定向到新的链接上。这样既能保证搜索引擎的索引不受影响，也是一项对读者负责任的行为是吧。不过这是一项挺麻烦的事情。</p>

<p>幸好我当初建立WordPress的时候就留下了后路。原先网站的链接是这样的：
    http://XXXXXXXXX.com/[year]/[month]/[the-long-long-title].html
    http://XXXXXXXXX.com/page/xx/
    http://XXXXXXXXX.com/category/[category-name]/
这样的格式是比较容易迁移的。如果原先的文章URL是带有数字ID的话，只能说声抱歉了。到<u>config.yml里面设置一下新站点的文章链接格式，跟原先的格式保持一致：
    permalink: /:year/:month/:title/
    category</u>dir: category
    pagination_dir:  # 留空</p>

<h3>迁移评论</h3>

<p>既然做好了301，那么迁移评论就显得非常简单了。登录DISQUS后台，进入站点管理后台的“Migrate Threads”栏目，那里有一个“Redirect Crawler”的功能，便是自动跟随301重定向，将评论指向新的网址。点一下那个按钮就大功告成。</p>

<h3>迁移图片</h3>

<p>可以参考<a href="http://log4d.com/2012/05/image-host/">使用独立图床子域名</a></p>

<h1>Step6 - 再次部署</h1>
<div class="highlight"><pre><code class="text language-text" data-lang="text">rake generate
rake deploy
</code></pre></div>
<h1>参考文章</h1>

<ul>
<li>Octopress Setup： http://octopress.org/docs/setup/</li>
<li>Octopress Deploying：http://octopress.org/docs/deploying/</li>
<li>Blog = GitHub + Octopress：http://mrzhang.me/blog/blog-equals-github-plus-octopress.html</li>
<li>从Wordpress迁移到Octopress：http://blog.dayanjia.com/2012/04/migration-to-octopress-from-wordpress/</li>
<li>使用独立图床子域名：http://log4d.com/2012/05/image-host/ http://log4d.com/2012/05/image-host/</li>
</ul>

     

      <div class="status">
        
        <div class="clearfix"></div>
      </div>
</article>
<hr/>

<article>
    <header>
    	<h1 class="entry-title headline"><a href="/kettle/2012/04/13/kettle-dependency-management">Kettle dependency management</a></h1>
    </header>
    
      <p>pentaho的项目使用了ant和ivy解决项目依赖,所以必须编译源码需要ivy工具.直接使用ivy编译pentaho的bi server项目,一直没有编译成功.<br />
使用ivy编译kettle的源代码却是非常容易的事情.</p>

<p>该篇文章翻译并参考了Will Gorman在pentaho的wiki上添加的<a href="http://wiki.pentaho.com/display/EAI/Kettle+dependency+management" target="_blank">Kettle dependency management</a>,文章标题没作修改.<br />
编写此文,是为了记录编译kettle源码的方法和过程.</p>

<p><strong>以下是对原文的一个简单翻译.</strong>
将kettle作为一个产品发行是一个很有趣的事情.有很多来自于pentaho其他项目(其中有一些有依赖于kettle)的jar包被导入到kettle.这些jar包必须在发行的时候构建并且加入到kettle中.如果一个核心的库被更新了,我们必须将其导入到kettle中(如果有必要).bi服务器,pentaho报表以及pentaho元数据编辑器都将kettle作为一个服务/引擎资源而被构建的.自从我们已经将这些jar导入到我们的源码仓库,这些项目必须使用ivy明确列出kettle以及他的依赖.当kettle的依赖变化的时候,我们必须审查libext文件是否需要更新.</p>

<p>pentaho创建了一系列的脚本来自动化的安装ivy,解决jar(或者是artifacts),构建并发行artifacts.kettle已经升级使用subfloor(简单的意味着build.xml继承自subfloor的构建脚本).subfloor使用ivy从pentaho仓库()或者ibiblio maven2仓库来获取跟新jar.ibiblio仓库用于大多数第三方的jar文件(如apache-commons).pentaho仓库用于在线的pentaho项目或者一些比在ibiblio的三方库.为了解决kettle的依赖,我们不得不在ivy.xml里创建一个清单.这个文件明确地列出每一个没有传递依赖的jar文件.这意味着libext文件的映射在ivy.xml中是一对一的.
<!--more-->
<strong>关于Ivy</strong>
<a href="http://ant.apache.org/ivy/" target="_blank">Apache Ivy™</a>是一个流行的致力于灵活性和简单性的依赖管理工具.更多的参考:<a href="http://ant.apache.org/ivy/features.html" target="_blank">enterprise features</a>, <a href="http://ant.apache.org/ivy/testimonials.html" target="_blank">what people say about it</a>, 以及 <a href="http://ant.apache.org/ivy/history/latest-milestone/index.html" target="_blank">how it can improve your build system</a></p>

<p><strong>在kettle中使用ivyIDE</strong>
首先,从svn上下载kettle的源代码:
<pre>
svn://source.pentaho.org/svnkettleroot/Kettle/trunk
</pre>
如果你想在Eclipse上使用<a href="http://ant.apache.org/ivy/ivyde/download.cgi" target="_blank">ivyde plugin</a>.<br />
请参考相关文章安装该插件.</p>

<p>如果你不想使用ivyde,你可以简单快速并且容易的开始并编译代码.<br />
1.执行<code>ant resolve</code>,这个命令将会创建一个叫做resolved-libs的文件夹.<br />
2.使用下面命令更新classpath <br />
  a.手动的添加这些jar文件到你的ide的classpath<br />
  b.执行ant create-dot-classpath,将会修改你的.classpath文件(注意刷新项目以使改变生效)<br />
注意:kettle项目中的构建脚本会自动安装ivy插件.</p>

<p><strong>构建Kettle</strong>
你可以下载kettle源代码然后立即执行<code>ant distrib</code>命令<br />
或者你可以在ide中导入下载的kettle工程,然后按照你的操作系统(默认的是Windows 32-bit)版本修改依赖的swt.jar文件.</p>

<p><strong>ivy中未完成的</strong>
<strong>pentaho-database-</strong>这是一个依赖kettle-db的常用项目,但又被kettle-ui使用.这样会导致循环依赖,将来可能会将其引入到kettle项目或是从该项目中去掉对kettle的依赖.
<strong>swt-</strong>swt文件目前没有包括在ivy.xml文件中
<strong>library configurations-</strong>每一个kettle库(kettle-db,kettle-core等等)应该在ivy.xml中有他自己的依赖.这些库应该继承一些特定的依赖,而取代继承整个kettle依赖.
<strong>checked-in plugins-</strong>当前引入的插件如;DummyJob, DummyPlugin, S3CsvInput, ShapeFileReader3,versioncheck应该都移到ivy的plugin配置中.</p>

<p><strong>参考文章</strong>
<a href="http://wiki.pentaho.com/display/EAI/Kettle+dependency+management" target="_blank">Kettle dependency management</a>
</p>

     

      <div class="status">
        
        <div class="clearfix"></div>
      </div>
</article>
<hr/>


<!-- Pagination links -->
<div class="pull-right">
  
    
    <a class="btn btn-default btn-sm" href="/">Home</a>
    <a class="btn btn-default btn-sm" href="/page7/">&laquo; Prev</a>
    
  
  <span><a class="btn btn-default btn-sm disabled">Page: 8 of 14</a></span>
  
    <a class="btn btn-default btn-sm" href="/page9/">Next &raquo;</a>
    <a class="btn btn-default btn-sm" href="/page14/">Last</a>
  
</div>

    </article>
  </div>

  <div class="col-md-3 visible-lg visible-md">
                <section>
   	 <h2><i class="icon-home icon-large"></i>About</h2>
	 <p class="entry-content">
   	 一个Java方案架构师，主要从事hadoop相关工作。<a href="/about.html">更多信息</a> 
	</p>
       </section>


            <section>
       <h2><i class="icon-home icon-large"></i>Posts</h2>
       <div class="category">
         <ul>
           
           <li><a href="/linux/2014/01/25/how-to-install-ganglia-on-centos6">在centos6系统上安装Ganlia</a></li>
           
           <li><a href="/linux/2014/01/24/how-to-install-nagios-on-rhel6">在rhel系统上安装Nagios</a></li>
           
           <li><a href="/post/2014/01/22/all-things-opentsdb">All Things OpenTSDB</a></li>
           
           <li><a href="/post/2014/01/21/all-things-about-jekyll">All Things Jekyll</a></li>
           
           <li><a href="/linux/2014/01/18/bash-problem-when-ssh-access">ssh连接环境变量问题</a></li>
           
           <li><a href="/hbase/2014/01/16/hbase-region-split-policy">HBase Region Split策略</a></li>
           
           <li><a href="/linux/2014/01/14/vim-config-and-plugins">vim配置和插件管理</a></li>
           
           <li><a href="/java/2014/01/13/about-sitemesh">SiteMesh介绍</a></li>
           
           <li><a href="/linux/2014/01/09/after-reinstall-the-system">重装linux-mint系统之后</a></li>
           
           <li><a href="/hive/2014/01/08/hive-ha-by-haproxy">Hive使用HAProxy配置HA</a></li>
           
           <li><a href="/linux/2013/12/27/some-git-configs-and-cammands">git配置和一些常用命令</a></li>
           
           <li><a href="/saltstack/2013/11/18/study-note-of-saltstack">SaltStack学习笔记[未完成]</a></li>
           
           <li><a href="/saltstack/2013/11/16/install-jboss-with-saltstack">使用saltstack安装jboss</a></li>
           
           <li><a href="/saltstack/2013/11/11/install-saltstack-and-halite">安装saltstack和halite</a></li>
           
           <li><a href="/hbase/2013/11/01/debug-hbase-in-eclipse">在eclipse中调试运行hbase</a></li>
           
           <li><a href="/hbase/2013/10/28/compile-hbase-source-code-and-apply-patches">编译hbase源代码并打补丁</a></li>
           
         </ul>
       </div>
      </section>


            <section>
        <h2><i class="icon-home icon-large"></i>Comments</h2>
        <div class="category">
          <ul class="ds-recent-comments" data-num-items="5" data-show-avatars="0" data-show-time="0" data-show-title="0" data-show-admin="0" data-excerpt-length="14"></ul>
        </div>
      </section>


            
      <section>
        <h2><i class="icon-home icon-large"></i>Blogroll</h2>
        <div class="category">
          <ul>
            
              <li><a href="//blog.boluotou.com/" target="_blank">圆木菠萝罐</a></li>
            
              <li><a href="//log4d.com/" target="_blank">Log4D</a></li>
            
              <li><a href="//blog.frankel.ch" target="_blank">A Java geek</a></li>
            
              <li><a href="//kohsuke.org/" target="_blank">Kohsuke Kawaguchi</a></li>
            
              <li><a href="http://www.parallellabs.com/" target="_blank">并行实验室</a></li>
            
              <li><a href="http://www.yanjiuyanjiu.com/" target="_blank">研究研究</a></li>
            
          </ul>
        </div>
      </section>
      


            <section>
        <h2><i class="icon-home icon-large"></i>License</h2>
        <ul>
          <li>
            <a href="http://www.creativecommons.org/licenses/by-nc-sa/3.0/cn/deed.zh" target="_blank"><img alt="License" src="http://blog.javachen.com/assets/images/CC.png" title="署名-非商业性使用-相同方式共享 3.0 中国大陆"></a>
          </li>
        </ul>
      </section>


       <script type="text/javascript">
      var duoshuoQuery = {short_name:"javachen"};
      (function() {
        var ds = document.createElement('script');
        ds.type = 'text/javascript';ds.async = true;
        ds.src = 'http://static.duoshuo.com/embed.js';
        ds.charset = 'UTF-8';
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(ds);
      })();
    </script>

  </div>

</div>


      </div>

      <footer class="text-left">
        <p>&copy; 2009-2014 <a href="/" target="_blank">JavaChen</a>. Help from <a href="http://jekyllrb.com/" target="_blank" title="Jekyll">Jekyll </a>| <script language="javascript" type="text/javascript" src="http://js.users.51.la/12111481.js"></script>
<noscript><a href="http://www.51.la/?12111481" target="_blank"><img alt="Statistic" src="http://img.users.51.la/12111481.asp" style="border:none" /></a></noscript>
</p>
      </footer>
            <div id="toTop"><a href="#">▲</a><a href="#footer">▼</a></div>

    </div> <!-- /container -->

    <script type="text/javascript" src="//libs.baidu.com/jquery/1.8.3/jquery.min.js"></script>
    <script>window.jQuery || document.write('<script src="/assets/themes/javachen/js/jquery.min.js"><\/script>')</script>
    <script type="text/javascript" src="//netdna.bootstrapcdn.com/bootstrap/3.0.0/js/bootstrap.min.js"></script>
    <script type="text/javascript" src="/assets/themes/javachen/fancybox/jquery.fancybox.pack.js?v=2.1.5"></script>
    <script type="text/javascript" src="/assets/themes/javachen/js/main.js"></script>
        <!-- Baidu Button BEGIN -->
    <script type="text/javascript" id="bdshell_js"></script>
    <script>document.write(unescape('%3Cscript%20type%3D%22text/javascript%22%20id%3D%22bdshare_js%22%20data%3D%22type%3Dtools%26amp%3Buid%3D1539895%22%3E%3C/script%3E'));</script>
    <script type="text/javascript">
      var bds_config = {
        'review':'normal',
        'snsKey':{
        }
      };
      document.getElementById("bdshell_js").src = "http://bdimg.share.baidu.com/static/js/shell_v2.js?cdnversion=" + Math.ceil(new Date()/3600000)
    </script>
    <!-- Baidu Button END -->

    


  <script type="text/javascript">
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "//hm.baidu.com/hm.js?83708c1ec341327ce3307ac0dd35e4f0";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();
</script>





  </body>
</html>

