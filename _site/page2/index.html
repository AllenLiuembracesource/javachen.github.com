
<!DOCTYPE html>
<html lang="zh-CN">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta http-equiv="pragma" content="no-cache" />
    <title>JavaChen Blog</title>
    <meta name="author" content="JavaChen">
    <meta name="copyright" content="© http://blog.javachen.com" />
    <meta property="wb:webmaster" content="61eb31a6e636506d" />
    <meta name="ujianVerification" content="f8b60286538bf86567069598d8a5d6cc" />
    <meta name="wumiiVerification" content="eec4ca3c-ccdb-4c0f-9fe3-4499d87649a3" />

    <!-- Le HTML5 shim, for IE6-8 support of HTML elements -->
    <!--[if lt IE 9]>
      <script src="http://html5shim.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->

    <!-- Le styles -->
    <link href="/assets/themes/twitter/bootstrap/css/bootstrap.min.css" rel="stylesheet">
    <link href="/assets/themes/twitter/bootstrap/css/bootstrap-responsive.css" rel="stylesheet">
    <link href="/assets/themes/twitter/css/style.css?body=1" rel="stylesheet" type="text/css" media="all">

    <!-- Le fav and touch icons -->
    <link rel="shortcut icon" href="/favicon.ico">
	<!-- Update these with your own images
	<link rel="apple-touch-icon" href="images/apple-touch-icon.png">
	<link rel="apple-touch-icon" sizes="72x72" href="images/apple-touch-icon-72x72.png">
	<link rel="apple-touch-icon" sizes="114x114" href="images/apple-touch-icon-114x114.png">
	-->
    <link href="/atom.xml" type="application/atom+xml" rel="alternate" title="Sitewide ATOM Feed">
    <link href="/rss.xml" type="application/rss+xml" rel="alternate" title="Sitewide RSS Feed">
    <link rel="stylesheet" href="http://fonts.googleapis.com/css?family=Handlee">
  </head>

  <body data-spy="scroll" data-target=".subnav" data-offset="100">

    <div class="navbar navbar-fixed-top">
      <div class="navbar-inner">
        <div class="container">
          <a class="brand" href="/">JavaChen Blog</a>
          <div class="nav-collapse">
            <ul class="nav">


              <li><a href="/categories.html">Categories</a></li>
              <li><a href="/about.html">About me</a></li>
              <li><a href="/archive.html">Archive</a></li>
              <li><a href="/tags.html">Tags</a></li>
            </ul>
          </div>
        </div>
      </div>
    </div>

    <div class="container">

      <div class="content">
        


<div class="row">
  <div class="span12">
    


<div class="row">
  <div class="span8">
  
  
  <section id="hadoop20130329install-impala">
    <article>
      <header>
      <h3><a href="/hadoop/2013/03/29/install-impala">安装impala过程</a></h3>
      <div class="c9">
     		Author: JavaChen
     		&emsp;&emsp;
		Categories：
			
			<a href="/categories.html#hadoop">hadoop</a>
			
			
		&emsp;&emsp;
		Tags：
			
			<a href="/tags.html#hadoop-ref">hadoop</a>
			
			,
			
			
			<a href="/tags.html#impala-ref">impala</a>
			
			,
			
			
			<a href="/tags.html#cloudera-ref">cloudera</a>
			
			
		&emsp;&emsp;
		<a href='/hadoop/2013/03/29/install-impala#comment' title='分享文章、查看评论' style="float:right;margin-right:.5em;">Comments</a>
	</div>
    </header>
    <div class="content"><p>与Hive类似，Impala也可以直接与HDFS和HBase库直接交互。只不过Hive和其它建立在MapReduce上的框架适合需要长时间运行的批处理任务。例如那些批量提取，转化，加载（ETL）类型的Job。而Impala主要用于实时查询。</p>

<h3>install</h3>

<p>下载 impala，目前最新版本为0.6-1，<a href="http://beta.cloudera.com/impala/redhat/6/x86_64/impala/0/RPMS/x86_64/">下载地址</a>。</p>

<h3>安装过程</h3>

<p>安装前提：先安装好hadoop集群以及hive，可以参考我的文章：</p>

<ul>
<li><a href="http://blog.javachen.com/Hadoop/2013/03/24/manual-install-Cloudera-Hadoop-CDH4.2.html">手动安装Cloudera Hadoop CDH4.2</a></li>
<li><a href="http://blog.javachen.com/Hadoop/2013/03/24/manual-install-Cloudera-hive-CDH4.2.html">手动安装Cloudera Hive CDH4.2</a></li>
</ul>

<ol>
<li><p>DataNode节点</p>

<p>yum install -y impala-0.6-1.p0.548.el6.x86<em>64.rpm   impala-server-0.6-1.p0.548.el6.x86</em>64.rpm impala-state-store-0.6-1.p0.548.el6.x86<em>64.rpm    impala-shell-0.6-1.p0.548.el6.x86</em>64.rpm libevent-1.4.13-4.el6.x86_64.rpm bigtop-utils-0.4+300-1.cdh4.0.1.p0.1.el6.noarch.rpm --skip-broken</p></li>
<li><p>在hive节点上</p>

<p>yum install -y impala-0.6-1.p0.548.el6.x86<em>64.rpm   impala-server-0.6-1.p0.548.el6.x86</em>64.rpm \
impala-state-store-0.6-1.p0.548.el6.x86<em>64.rpm  impala-shell-0.6-1.p0.548.el6.x86</em>64.rpm \
libevent-1.4.13-4.el6.x86_64.rpm    bigtop-utils-0.4+300-1.cdh4.0.1.p0.1.el6.noarch.rpm</p></li>
</ol>

<h3>配置Impala</h3>

<h4>查看安装路径</h4>
<div class="highlight"><pre><code class="text language-text" data-lang="text">[root@desktop1 conf]# find / -name impala
/var/run/impala
/var/lib/alternatives/impala
/var/log/impala
/usr/lib/impala
/etc/alternatives/impala
/etc/default/impala
/etc/impala
</code></pre></div>
<h4>添加配置文件</h4>

<p>impalad的配置文件路径由环境变量<code>IMPALA_CONF_DIR</code>指定，默认为<code>/usr/lib/impala/conf</code></p>

<p>在节点desktop1上 拷贝<code>hive-site.xml</code>、<code>core-site.xml</code>、<code>hdfs-site.xml</code>至<code>/usr/lib/impala/conf</code>目录下:</p>
<div class="highlight"><pre><code class="text language-text" data-lang="text">[root@desktop1 conf]# mkdir /usr/lib/impala/conf/
[root@desktop1 conf]# cp /opt/hadoop-2.0.0-cdh4.2.0/etc/hadoop/log4j.properties /usr/lib/impala/conf/
[root@desktop1 conf]# cp /opt/hadoop-2.0.0-cdh4.2.0/etc/hadoop/core-site.xml /usr/lib/impala/conf/
[root@desktop1 conf]# cp /opt/hadoop-2.0.0-cdh4.2.0/etc/hadoop/hdfs-site.xml /usr/lib/impala/conf/
[root@desktop1 conf]# cp /opt/hive-0.10.0-cdh4.2.0/conf/hive-site.xml /usr/lib/impala/conf/
</code></pre></div>
<p>并作下面修改在<code>hdfs-site.xml</code>文件中添加如下内容：</p>
<div class="highlight"><pre><code class="text language-text" data-lang="text">&lt;property&gt;
    &lt;name&gt;dfs.client.read.shortcircuit&lt;/name&gt;
    &lt;value&gt;true&lt;/value&gt;
&lt;/property&gt;

&lt;property&gt;
    &lt;name&gt;dfs.domain.socket.path&lt;/name&gt;
    &lt;value&gt;/var/run/hadoop-hdfs/dn._PORT&lt;/value&gt;
&lt;/property&gt;

&lt;property&gt;
  &lt;name&gt;dfs.datanode.hdfs-blocks-metadata.enabled&lt;/name&gt;
  &lt;value&gt;true&lt;/value&gt;
&lt;/property&gt;

&lt;property&gt;
  &lt;name&gt;dfs.datanode.hdfs-blocks-metadata.enabled&lt;/name&gt;
  &lt;value&gt;true&lt;/value&gt;
&lt;/property&gt;
</code></pre></div>
<p>同步以上文件到其他节点</p>
<div class="highlight"><pre><code class="text language-text" data-lang="text">[root@desktop1 ~]# scp -r /usr/lib/impala/conf desktop3:/usr/lib/impala/
[root@desktop1 ~]# scp -r /usr/lib/impala/conf desktop4:/usr/lib/impala/
[root@desktop1 ~]# scp -r /usr/lib/impala/conf desktop6:/usr/lib/impala/
[root@desktop1 ~]# scp -r /usr/lib/impala/conf desktop7:/usr/lib/impala/
[root@desktop1 ~]# scp -r /usr/lib/impala/conf desktop8:/usr/lib/impala/
</code></pre></div>
<h4>hadoop中添加native包</h4>

<p>拷贝hadoop native包到hadoop安装路径下，并同步hadoop文件到其他节点：</p>
<div class="highlight"><pre><code class="text language-text" data-lang="text">[root@desktop1 ~]# cp /usr/lib/impala/lib/*.so* /opt/hadoop-2.0.0-cdh4.2.0/lib/native/
</code></pre></div>
<h4>创建socket path</h4>

<p>在每个节点上创建/var/run/hadoop-hdfs:</p>
<div class="highlight"><pre><code class="text language-text" data-lang="text">[root@desktop1 ~]# mkdir -p /var/run/hadoop-hdfs
[root@desktop3 ~]# mkdir -p /var/run/hadoop-hdfs
[root@desktop4 ~]# mkdir -p /var/run/hadoop-hdfs
[root@desktop6 ~]# mkdir -p /var/run/hadoop-hdfs
[root@desktop7 ~]# mkdir -p /var/run/hadoop-hdfs
[root@desktop8 ~]# mkdir -p /var/run/hadoop-hdfs
</code></pre></div>
<p>拷贝postgres jdbc jar：</p>
<div class="highlight"><pre><code class="text language-text" data-lang="text">cp /opt/hive-0.10.0-cdh4.2.0/lib/postgresql-9.1-903.jdbc* /usr/lib/impala/lib/
</code></pre></div>
<h3>启动服务</h3>

<ol>
<li><p>在hive所在节点启动statestored（默认端口为24000）:</p>

<p>GLOG<em>v=1 nohup statestored -state</em>store_port=24000 &amp;</p></li>
</ol>

<p>如果statestore正常启动，可以在/tmp/statestored.INFO查看。如果出现异常，可以查看/tmp/statestored.ERROR定位错误信息。</p>

<ol>
<li><p>在所有impalad节点上：</p>

<p>HADOOP<em>CONF</em>DIR=&quot;/usr/lib/impala/conf&quot; nohup impalad -state<em>store</em>host=desktop1 -nn=desktop1 \
    -nn_port=8020 -hostname=desktop3 -ipaddress=192.168.0.3 &amp;</p></li>
</ol>

<p>注意： 其中的<code>-hostname</code>和<code>-ipaddress</code>表示当前启动impalad实例所在机器的主机名和ip地址。</p>

<p>如果impalad正常启动，可以在<code>/tmp/ impalad.INFO</code>查看。如果出现异常，可以查看<code>/tmp/impalad.ERROR</code>定位错误信息。</p>

<h3>使用shell</h3>

<p>使用<code>impala-shell</code>启动Impala Shell，分别连接各Impalad主机(desktop3、desktop4、desktop6、desktop7、desktop8)，刷新元数据，之后就可以执行shell命令。相关的命令如下(可以在任意节点执行)：</p>
<div class="highlight"><pre><code class="text language-text" data-lang="text">&gt;impala-shell
[Not connected] &gt;connect desktop3:21000
[desktop3:21000] &gt;refresh
[desktop3:21000] &gt;connect desktop4:21000
[desktop4:21000] &gt;refresh
</code></pre></div>
<h3>注意：</h3>

<ol>
<li>如果hive使用mysql或postgres数据库作为metastore的存储，则需要拷贝相应的jdbc jar到<code>/usr/lib/impala/lib</code>目录下</li>
<li>E0325 11:04:19.937718  7239 statestored-main.cc:52] Could not start webserver on port: 25010</li>
</ol>

<p>可能是已经启动了statestored进程</p>

<h3>参考文章</h3>

<ul>
<li><a href="http://yuntai.1kapp.com/?p=904">Impala安装文档完整版</a></li>
<li><a href="http://tech.uc.cn/?p=817">Impala入门笔记</a></li>
<li><a href="https://ccp.cloudera.com/display/IMPALA10BETADOC/Installing+and+Using+Cloudera+Impala">Installing and Using Cloudera Impala</a></li>
</ul>
</div>
    </article>
  </section>
  
  
  <section id="hadoop20130324manual-install-Cloudera-Hadoop-CDH4.2">
    <article>
      <header>
      <h3><a href="/hadoop/2013/03/24/manual-install-Cloudera-Hadoop-CDH4.2">手动安装Cloudera Hadoop CDH4.2</a></h3>
      <div class="c9">
     		Author: JavaChen
     		&emsp;&emsp;
		Categories：
			
			<a href="/categories.html#hadoop">hadoop</a>
			
			
		&emsp;&emsp;
		Tags：
			
			<a href="/tags.html#hadoop-ref">hadoop</a>
			
			,
			
			
			<a href="/tags.html#cdh-ref">cdh</a>
			
			
		&emsp;&emsp;
		<a href='/hadoop/2013/03/24/manual-install-Cloudera-Hadoop-CDH4.2#comment' title='分享文章、查看评论' style="float:right;margin-right:.5em;">Comments</a>
	</div>
    </header>
    <div class="content"><h2>安装版本</h2>
<div class="highlight"><pre><code class="text language-text" data-lang="text">hadoop-2.0.0-cdh4.2.0
hbase-0.94.2-cdh4.2.0
hive-0.10.0-cdh4.2.0
jdk1.6.0_38
</code></pre></div>
<h2>安装前说明</h2>

<ul>
<li>安装目录为/opt</li>
<li>检查hosts文件</li>
<li>关闭防火墙</li>
<li>设置时钟同步</li>
</ul>

<h2>使用说明</h2>

<p>安装hadoop、hbase、hive成功之后启动方式为：</p>

<ul>
<li>启动dfs和mapreduce
desktop1上执行start-dfs.sh和start-yarn.sh</li>
<li>启动hbase
desktop3上执行start-hbase.xml</li>
<li>启动hive
desktop1上执行hive</li>
</ul>

<h2>规划</h2>
<div class="highlight"><pre><code class="text language-text" data-lang="text">    192.168.0.1             NameNode、Hive、ResourceManager
    192.168.0.2             SSNameNode
    192.168.0.3             DataNode、HBase、NodeManager
    192.168.0.4             DataNode、HBase、NodeManager
    192.168.0.6             DataNode、HBase、NodeManager
    192.168.0.7             DataNode、HBase、NodeManager
    192.168.0.8             DataNode、HBase、NodeManager
</code></pre></div>
<h2>部署过程</h2>

<h3>系统和网络配置</h3>

<ol>
<li><p>修改每台机器的名称
<pre>
[root@desktop1 ~]# cat /etc/sysconfig/network
NETWORKING=yes
HOSTNAME=desktop1
</pre></p></li>
<li><p>在各个节点上修改/etc/hosts增加以下内容:
<pre>
[root@desktop1 ~]# cat /etc/hosts
127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4
::1         localhost localhost.localdomain localhost6 localhost6.localdomain6
192.168.0.1     desktop1
192.168.0.2     desktop2
192.168.0.3     desktop3
192.168.0.4     desktop4
192.168.0.6     desktop6
192.168.0.7     desktop7
192.168.0.8     desktop8
</pre></p></li>
<li><p>配置ssh无密码登陆
以下是设置desktop1上可以无密码登陆到其他机器上。</p></li>
</ol>

<pre>
    [root@desktop1 ~]# ssh-keygen
    [root@desktop1 ~]# ssh-copy-id -i .ssh/id_rsa.pub desktop2
    [root@desktop1 ~]# ssh-copy-id -i .ssh/id_rsa.pub desktop3
    [root@desktop1 ~]# ssh-copy-id -i .ssh/id_rsa.pub desktop4
    [root@desktop1 ~]# ssh-copy-id -i .ssh/id_rsa.pub desktop6
    [root@desktop1 ~]# ssh-copy-id -i .ssh/id_rsa.pub desktop7
    [root@desktop1 ~]# ssh-copy-id -i .ssh/id_rsa.pub desktop8
</pre>

<ol>
<li>每台机器上关闭防火墙：</li>
</ol>
<div class="highlight"><pre><code class="text language-text" data-lang="text">    [root@desktop1 ~]# service iptables stop
</code></pre></div>
<h3>安装Hadoop</h3>

<h4>配置Hadoop</h4>

<p>将jdk1.6.0_38.zip上传到/opt，并解压缩。
将hadoop-2.0.0-cdh4.2.0.zip上传到/opt，并解压缩。</p>

<p>在NameNode上配置以下文件：</p>
<div class="highlight"><pre><code class="text language-text" data-lang="text">core-site.xml fs.defaultFS指定NameNode文件系统，开启回收站功能。
hdfs-site.xml 
    dfs.namenode.name.dir指定NameNode存储meta和editlog的目录，
    dfs.datanode.data.dir指定DataNode存储blocks的目录，
    dfs.namenode.secondary.http-address指定Secondary NameNode地址。
    开启WebHDFS。
slaves 添加DataNode节点主机
</code></pre></div>
<ol>
<li>core-site.xml
该文件指定fs.defaultFS连接desktop1，即NameNode节点。</li>
</ol>
<div class="highlight"><pre><code class="text language-text" data-lang="text">[root@desktop1 hadoop]# pwd
/opt/hadoop-2.0.0-cdh4.2.0/etc/hadoop
[root@desktop1 hadoop]# cat core-site.xml 
&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
&lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&gt;
&lt;configuration&gt;
&lt;!--fs.default.name for MRV1 ,fs.defaultFS for MRV2(yarn) --&gt;
&lt;property&gt;
     &lt;name&gt;fs.defaultFS&lt;/name&gt;
         &lt;!--这个地方的值要和hdfs-site.xml文件中的dfs.federation.nameservices一致--&gt;
     &lt;value&gt;hdfs://desktop1&lt;/value&gt;
&lt;/property&gt;
&lt;property&gt;
&lt;name&gt;fs.trash.interval&lt;/name&gt;
&lt;value&gt;10080&lt;/value&gt;
&lt;/property&gt;
&lt;property&gt;
&lt;name&gt;fs.trash.checkpoint.interval&lt;/name&gt;
&lt;value&gt;10080&lt;/value&gt;
&lt;/property&gt;
&lt;/configuration&gt;
</code></pre></div>
<ol>
<li>hdfs-site.xml
该文件主要设置数据副本保存份数，以及namenode、datanode数据保存路径以及http-address。</li>
</ol>
<div class="highlight"><pre><code class="text language-text" data-lang="text">[root@desktop1 hadoop]# cat hdfs-site.xml 
&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
&lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&gt;
&lt;configuration&gt;
&lt;property&gt;
  &lt;name&gt;dfs.replication&lt;/name&gt;
  &lt;value&gt;1&lt;/value&gt;
&lt;/property&gt;

&lt;property&gt;
  &lt;name&gt;hadoop.tmp.dir&lt;/name&gt;
  &lt;value&gt;/opt/data/hadoop-${user.name}&lt;/value&gt;
&lt;/property&gt;

&lt;property&gt;
&lt;name&gt;dfs.namenode.http-address&lt;/name&gt;
&lt;value&gt;desktop1:50070&lt;/value&gt;
&lt;/property&gt;

&lt;property&gt;
&lt;name&gt;dfs.namenode.secondary.http-address&lt;/name&gt;
&lt;value&gt;desktop2:50090&lt;/value&gt;
&lt;/property&gt;

&lt;property&gt;
&lt;name&gt;dfs.webhdfs.enabled&lt;/name&gt;
&lt;value&gt;true&lt;/value&gt;
&lt;/property&gt;
&lt;/configuration&gt;
</code></pre></div>
<ol>
<li>masters
设置namenode和secondary namenode节点。</li>
</ol>
<div class="highlight"><pre><code class="text language-text" data-lang="text">[root@desktop1 hadoop]# cat masters 
desktop1
desktop2
</code></pre></div>
<ol>
<li>slaves
设置哪些机器上安装datanode节点。</li>
</ol>
<div class="highlight"><pre><code class="text language-text" data-lang="text">[root@desktop1 hadoop]# cat slaves 
desktop3
desktop4
desktop6
desktop7
desktop8
</code></pre></div>
<h4>配置MapReduce</h4>

<ol>
<li>mapred-site.xml
配置使用yarn计算框架，以及jobhistory的地址。</li>
</ol>
<div class="highlight"><pre><code class="text language-text" data-lang="text">[root@desktop1 hadoop]# cat mapred-site.xml
&lt;?xml version=&quot;1.0&quot;?&gt;
&lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&gt;
&lt;configuration&gt;
&lt;property&gt;
 &lt;name&gt;mapreduce.framework.name&lt;/name&gt;
 &lt;value&gt;yarn&lt;/value&gt;
&lt;/property&gt;

&lt;property&gt;
 &lt;name&gt;mapreduce.jobhistory.address&lt;/name&gt;
 &lt;value&gt;desktop1:10020&lt;/value&gt;
&lt;/property&gt;

&lt;property&gt;
 &lt;name&gt;mapreduce.jobhistory.webapp.address&lt;/name&gt;
 &lt;value&gt;desktop1:19888&lt;/value&gt;
&lt;/property&gt;
&lt;/configuration&gt;
</code></pre></div>
<ol>
<li>yarn-site.xml
主要配置resourcemanager地址以及<code>yarn.application.classpath</code>（这个路径很重要，要不然集成hive时候会提示找不到class）</li>
</ol>
<div class="highlight"><pre><code class="text language-text" data-lang="text">[root@desktop1 hadoop]# cat yarn-site.xml 
&lt;?xml version=&quot;1.0&quot;?&gt;
&lt;configuration&gt;
&lt;property&gt;
    &lt;name&gt;yarn.resourcemanager.resource-tracker.address&lt;/name&gt;
    &lt;value&gt;desktop1:8031&lt;/value&gt;
  &lt;/property&gt;
  &lt;property&gt;
    &lt;name&gt;yarn.resourcemanager.address&lt;/name&gt;
    &lt;value&gt;desktop1:8032&lt;/value&gt;
  &lt;/property&gt;
  &lt;property&gt;
    &lt;name&gt;yarn.resourcemanager.scheduler.address&lt;/name&gt;
    &lt;value&gt;desktop1:8030&lt;/value&gt;
  &lt;/property&gt;
  &lt;property&gt;
    &lt;name&gt;yarn.resourcemanager.admin.address&lt;/name&gt;
    &lt;value&gt;desktop1:8033&lt;/value&gt;
  &lt;/property&gt;
  &lt;property&gt;
    &lt;name&gt;yarn.resourcemanager.webapp.address&lt;/name&gt;
    &lt;value&gt;desktop1:8088&lt;/value&gt;
  &lt;/property&gt;
  &lt;property&gt;
    &lt;description&gt;Classpath for typical applications.&lt;/description&gt;
    &lt;name&gt;yarn.application.classpath&lt;/name&gt;
    &lt;value&gt;$HADOOP_CONF_DIR,$HADOOP_COMMON_HOME/share/hadoop/common/*,
    $HADOOP_COMMON_HOME/share/hadoop/common/lib/*,
    $HADOOP_HDFS_HOME/share/hadoop/hdfs/*,$HADOOP_HDFS_HOME/share/hadoop/hdfs/lib/*,
    $YARN_HOME/share/hadoop/yarn/*,$YARN_HOME/share/hadoop/yarn/lib/*,
    $YARN_HOME/share/hadoop/mapreduce/*,$YARN_HOME/share/hadoop/mapreduce/lib/*&lt;/value&gt;
  &lt;/property&gt;
  &lt;property&gt;
    &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;
    &lt;value&gt;mapreduce.shuffle&lt;/value&gt;
  &lt;/property&gt;
  &lt;property&gt;
    &lt;name&gt;yarn.nodemanager.aux-services.mapreduce.shuffle.class&lt;/name&gt;
    &lt;value&gt;org.apache.hadoop.mapred.ShuffleHandler&lt;/value&gt;
  &lt;/property&gt;

  &lt;property&gt;
    &lt;name&gt;yarn.nodemanager.local-dirs&lt;/name&gt;
    &lt;value&gt;/opt/data/yarn/local&lt;/value&gt;
  &lt;/property&gt;
  &lt;property&gt;
    &lt;name&gt;yarn.nodemanager.log-dirs&lt;/name&gt;
    &lt;value&gt;/opt/data/yarn/logs&lt;/value&gt;
  &lt;/property&gt;
  &lt;property&gt;
    &lt;description&gt;Where to aggregate logs&lt;/description&gt;
    &lt;name&gt;yarn.nodemanager.remote-app-log-dir&lt;/name&gt;
    &lt;value&gt;/opt/data/yarn/logs&lt;/value&gt;
  &lt;/property&gt;

  &lt;property&gt;
    &lt;name&gt;yarn.app.mapreduce.am.staging-dir&lt;/name&gt;
    &lt;value&gt;/user&lt;/value&gt;
 &lt;/property&gt;

&lt;/configuration&gt;
</code></pre></div>
<h4>同步配置文件</h4>

<p>修改.bashrc环境变量，并将其同步到其他几台机器，并且source .bashrc</p>
<div class="highlight"><pre><code class="text language-text" data-lang="text">[root@desktop1 ~]# cat .bashrc 
# .bashrc
alias rm=&#39;rm -i&#39;
alias cp=&#39;cp -i&#39;
alias mv=&#39;mv -i&#39;

# Source global definitions
if [ -f /etc/bashrc ]; then
        . /etc/bashrc
fi
# User specific environment and startup programs
export LANG=zh_CN.utf8

export JAVA_HOME=/opt/jdk1.6.0_38
export JRE_HOME=$JAVA_HOME/jre
export CLASSPATH=./:$JAVA_HOME/lib:$JRE_HOME/lib:$JRE_HOME/lib/tools.jar

export HADOOP_HOME=/opt/hadoop-2.0.0-cdh4.2.0
export HIVE_HOME=/opt/hive-0.10.0-cdh4.2.0
export HBASE_HOME=/opt/hbase-0.94.2-cdh4.2.0

export HADOOP_MAPRED_HOME=${HADOOP_HOME}
export HADOOP_COMMON_HOME=${HADOOP_HOME}
export HADOOP_HDFS_HOME=${HADOOP_HOME}
export YARN_HOME=${HADOOP_HOME}
export HADOOP_YARN_HOME=${HADOOP_HOME}
export HADOOP_CONF_DIR=${HADOOP_HOME}/etc/hadoop
export HDFS_CONF_DIR=${HADOOP_HOME}/etc/hadoop
export YARN_CONF_DIR=${HADOOP_HOME}/etc/hadoop

export PATH=$PATH:$HOME/bin:$JAVA_HOME/bin:$HADOOP_HOME/sbin:$HBASE_HOME/bin:$HIVE_HOME/bin
</code></pre></div>
<p>修改配置文件之后，使其生效。</p>
<div class="highlight"><pre><code class="text language-text" data-lang="text">[root@desktop1 ~]# source .bashrc 
</code></pre></div>
<p>将desktop1上的/opt/hadoop-2.0.0-cdh4.2.0拷贝到其他机器上</p>

<h4>启动脚本</h4>

<p>第一次启动hadoop需要先格式化NameNode，该操作只做一次。当修改了配置文件时，需要重新格式化</p>
<div class="highlight"><pre><code class="text language-text" data-lang="text">[root@desktop1 hadoop]hadoop namenode -format
</code></pre></div>
<p>在desktop1上启动hdfs：</p>
<div class="highlight"><pre><code class="text language-text" data-lang="text">[root@desktop1 hadoop]#start-dfs.sh
</code></pre></div>
<p>在desktop1上启动mapreduce：</p>
<div class="highlight"><pre><code class="text language-text" data-lang="text">[root@desktop1 hadoop]#start-yarn.sh
</code></pre></div>
<p>在desktop1上启动historyserver：</p>
<div class="highlight"><pre><code class="text language-text" data-lang="text">[root@desktop1 hadoop]#mr-jobhistory-daemon.sh start historyserver
</code></pre></div>
<p>查看MapReduce：</p>
<div class="highlight"><pre><code class="text language-text" data-lang="text">http://desktop1:8088/cluster
</code></pre></div>
<p>查看节点：</p>
<div class="highlight"><pre><code class="text language-text" data-lang="text">http://desktop2:8042/
http://desktop2:8042/node
</code></pre></div>
<h4>检查集群进程</h4>
<div class="highlight"><pre><code class="text language-text" data-lang="text">[root@desktop1 ~]# jps
5389 NameNode
5980 Jps
5710 ResourceManager
7032 JobHistoryServer

[root@desktop2 ~]# jps
3187 Jps
3124 SecondaryNameNode

[root@desktop3 ~]# jps
3187 Jps
3124 DataNode
5711 NodeManager
</code></pre></div></div>
    </article>
  </section>
  
  
  <section id="hadoop20130324manual-install-Cloudera-hbase-CDH4.2">
    <article>
      <header>
      <h3><a href="/hadoop/2013/03/24/manual-install-Cloudera-hbase-CDH4.2">手动安装Cloudera HBase CDH4.2</a></h3>
      <div class="c9">
     		Author: JavaChen
     		&emsp;&emsp;
		Categories：
			
			<a href="/categories.html#hadoop">hadoop</a>
			
			
		&emsp;&emsp;
		Tags：
			
			<a href="/tags.html#hbase-ref">hbase</a>
			
			,
			
			
			<a href="/tags.html#cdh-ref">cdh</a>
			
			
		&emsp;&emsp;
		<a href='/hadoop/2013/03/24/manual-install-Cloudera-hbase-CDH4.2#comment' title='分享文章、查看评论' style="float:right;margin-right:.5em;">Comments</a>
	</div>
    </header>
    <div class="content"><p>本文主要记录手动安装cloudera HBase cdh4.2.0集群过程，环境设置及Hadoop安装过程见上篇文章。</p>

<h3>安装HBase</h3>

<p>HBase安装在desktop3、desktop4、desktop6、desktop7、desktop8机器上。</p>

<ol>
<li><p>上传文件
上传hbase-0.94.2-cdh4.2.0.zip到desktop3上，先在desktop3上修改好配置文件，在同步到其他机器上。</p></li>
<li><p>hbase-site.xml </p></li>
</ol>
<div class="highlight"><pre><code class="text language-text" data-lang="text">[root@desktop3 conf]# pwd
/opt/hbase-0.94.2-cdh4.2.0/conf
[root@desktop3 conf]# cat hbase-site.xml 
&lt;?xml version=&quot;1.0&quot;?&gt;
&lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&gt;
&lt;configuration&gt;
&lt;property&gt;
&lt;name&gt;hbase.rootdir&lt;/name&gt;
&lt;value&gt;hdfs://desktop1/hbase-${user.name}&lt;/value&gt;
&lt;/property&gt;
&lt;property&gt;
&lt;name&gt;hbase.cluster.distributed&lt;/name&gt;
&lt;value&gt;true&lt;/value&gt;
&lt;/property&gt;

&lt;property&gt;
&lt;name&gt;hbase.tmp.dir&lt;/name&gt;
&lt;value&gt;/opt/data/hbase-${user.name}&lt;/value&gt;
&lt;/property&gt;

&lt;property&gt;
&lt;name&gt;hbase.zookeeper.quorum&lt;/name&gt;
&lt;value&gt;desktop3,desktop4,desktop6,desktop7,desktop8&lt;/value&gt;
&lt;/property&gt;

&lt;/configuration&gt;
</code></pre></div>
<ol>
<li>regionservers</li>
</ol>
<div class="highlight"><pre><code class="text language-text" data-lang="text">[root@desktop3 conf]# cat regionservers 
desktop3
desktop4
desktop6
desktop7
desktop8
</code></pre></div>
<ol>
<li><p>环境变量
参考hadoop中环境变量的设置</p></li>
<li><p>同步文件
同步文件到其他4台机器上</p></li>
<li><p>启动脚本
可以在desktop3上配置无密码登陆到其他机器，然后在desktop3上启动hbase，这样其他节点上hbase都可以启动，否则，需要每台机器上单独启动hbase</p></li>
</ol>
<div class="highlight"><pre><code class="text language-text" data-lang="text">[root@desktop3 ~]# start-hbase.sh 
</code></pre></div>
<ol>
<li>HBase </li>
</ol>
<div class="highlight"><pre><code class="text language-text" data-lang="text">[root@desktop3 ~]# hbase 
HBase ; enter &#39;help&lt;RETURN&gt;&#39; for list of supported commands.
Type &quot;exit&lt;RETURN&gt;&quot; to leave the HBase 
Version 0.94.2-cdh4.2.0, r, Fri Feb 15 11:37:00 PST 2013

hbase(main):001:0&gt; 
</code></pre></div></div>
    </article>
  </section>
  
  
  <section id="hadoop20130324manual-install-Cloudera-hive-CDH4.2">
    <article>
      <header>
      <h3><a href="/hadoop/2013/03/24/manual-install-Cloudera-hive-CDH4.2">手动安装Cloudera Hive CDH4.2</a></h3>
      <div class="c9">
     		Author: JavaChen
     		&emsp;&emsp;
		Categories：
			
			<a href="/categories.html#hadoop">hadoop</a>
			
			
		&emsp;&emsp;
		Tags：
			
			<a href="/tags.html#hadoop-ref">hadoop</a>
			
			,
			
			
			<a href="/tags.html#cdh-ref">cdh</a>
			
			,
			
			
			<a href="/tags.html#hive-ref">hive</a>
			
			
		&emsp;&emsp;
		<a href='/hadoop/2013/03/24/manual-install-Cloudera-hive-CDH4.2#comment' title='分享文章、查看评论' style="float:right;margin-right:.5em;">Comments</a>
	</div>
    </header>
    <div class="content"><p>本文主要记录手动安装cloudera Hive cdh4.2.0集群过程，环境设置及Hadoop、HBase安装过程见上篇文章。</p>

<h3>安装hive</h3>

<p>hive安装在desktop1上，注意hive默认是使用derby数据库保存元数据，这里替换为postgresql，下面会提到postgresql的安装说明，并且需要拷贝postgres的jdbc jar文件导hive的lib目录下。</p>

<h4>上传文件</h4>

<p>上传<code>hive-0.10.0-cdh4.2.0.tar</code>到desktop1的<code>/opt</code>，并解压缩</p>

<h4>安装postgres</h4>

<ul>
<li>创建数据库</li>
</ul>

<p>这里创建数据库metastore并创建hiveuser用户，其密码为redhat。</p>
<div class="highlight"><pre><code class="text language-text" data-lang="text">psql -U postgres

CREATE DATABASE metastore;
 \c metastore;
CREATE USER hiveuser WITH PASSWORD &#39;redhat&#39;;
GRANT ALL ON DATABASE metastore TO hiveuser;
\q
</code></pre></div>
<ul>
<li>初始化数据库</li>
</ul>
<div class="highlight"><pre><code class="text language-text" data-lang="text">psql  -U hiveuser -d metastore
 \i /opt/hive-0.10.0-cdh4.2.0/scripts/metastore/upgrade/postgres/hive-schema-0.10.0.postgres.sql 
</code></pre></div>
<ul>
<li>编辑postgresql配置文件，修改访问权限</li>
</ul>
<div class="highlight"><pre><code class="text language-text" data-lang="text">[root@desktop1 ~]# vi /opt/PostgreSQL/9.1/data/pg_hba.conf

# IPv4 local connections:
host    all             all             0.0.0.0/0            md5

[root@desktop1 ~]# vi postgresql.conf

standard_conforming_strings = off
</code></pre></div>
<ul>
<li>重起postgres</li>
</ul>
<div class="highlight"><pre><code class="text language-text" data-lang="text">su -c &#39;/opt/PostgreSQL/9.1/bin/pg_ctl -D /opt/PostgreSQL/9.1/data restart&#39; postgres
</code></pre></div>
<ul>
<li>拷贝postgres 的jdbc驱动到<code>/opt/hive-0.10.0-cdh4.2.0/lib</code></li>
</ul>

<h4>修改配置文件</h4>

<ul>
<li>hive-site.xml 
注意修改下面配置文件中postgres数据库的密码，注意配置<code>hive.aux.jars.path</code>，在hive集成hbase时候需要从该路径家在hbase的一些jar文件。</li>
</ul>
<div class="highlight"><pre><code class="text language-text" data-lang="text">[root@desktop1 ~]# cd /opt/hive-0.10.0-cdh4.2.0/conf/
[root@desktop1 conf]# cat hive-site.xml 
&lt;?xml version=&quot;1.0&quot;?&gt;
&lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&gt;
&lt;configuration&gt;
&lt;property&gt;
  &lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt;
  &lt;value&gt;jdbc:postgresql://127.0.0.1/metastore&lt;/value&gt;
  &lt;description&gt;JDBC connect string for a JDBC metastore&lt;/description&gt;
&lt;/property&gt;

&lt;property&gt;
  &lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt;
  &lt;value&gt;org.postgresql.Driver&lt;/value&gt;
  &lt;description&gt;Driver class name for a JDBC metastore&lt;/description&gt;
&lt;/property&gt;

&lt;property&gt;
  &lt;name&gt;javax.jdo.option.ConnectionUserName&lt;/name&gt;
  &lt;value&gt;hiveuser&lt;/value&gt;
  &lt;description&gt;username to use against metastore database&lt;/description&gt;
&lt;/property&gt;

&lt;property&gt;
  &lt;name&gt;javax.jdo.option.ConnectionPassword&lt;/name&gt;
  &lt;value&gt;redhat&lt;/value&gt;
  &lt;description&gt;password to use against metastore database&lt;/description&gt;
&lt;/property&gt;

&lt;property&gt;
 &lt;name&gt;mapred.job.tracker&lt;/name&gt;
 &lt;value&gt;desktop1:8031&lt;/value&gt;
&lt;/property&gt;

&lt;property&gt;
 &lt;name&gt;mapreduce.framework.name&lt;/name&gt;
 &lt;value&gt;yarn&lt;/value&gt;
&lt;/property&gt;

&lt;property&gt;
  &lt;name&gt;hive.aux.jars.path&lt;/name&gt;
  &lt;value&gt;file:///opt/hive-0.10.0-cdh4.2.0/lib/zookeeper-3.4.5-cdh4.2.0.jar,
    file:///opt/hive-0.10.0-cdh4.2.0/lib/hive-hbase-handler-0.10.0-cdh4.2.0.jar,
    file:///opt/hive-0.10.0-cdh4.2.0/lib/hbase-0.94.2-cdh4.2.0.jar,
    file:///opt/hive-0.10.0-cdh4.2.0/lib/guava-11.0.2.jar&lt;/value&gt;
&lt;/property&gt;

&lt;property&gt;
  &lt;name&gt;hive.metastore.warehouse.dir&lt;/name&gt;
  &lt;value&gt;/opt/data/warehouse-${user.name}&lt;/value&gt;
  &lt;description&gt;location of default database for the warehouse&lt;/description&gt;
&lt;/property&gt;

&lt;property&gt;
  &lt;name&gt;hive.exec.scratchdir&lt;/name&gt;
  &lt;value&gt;/opt/data/hive-${user.name}&lt;/value&gt;
  &lt;description&gt;Scratch space for Hive jobs&lt;/description&gt;
&lt;/property&gt;

&lt;property&gt;
  &lt;name&gt;hive.querylog.location&lt;/name&gt;
  &lt;value&gt;/opt/data/querylog-${user.name}&lt;/value&gt;
  &lt;description&gt;
    Location of Hive run time structured log file
  &lt;/description&gt;
&lt;/property&gt;

&lt;property&gt;
  &lt;name&gt;hive.support.concurrency&lt;/name&gt;
  &lt;description&gt;Enable Hive&#39;s Table Lock Manager Service&lt;/description&gt;
  &lt;value&gt;true&lt;/value&gt;
&lt;/property&gt;

&lt;property&gt;
  &lt;name&gt;hive.zookeeper.quorum&lt;/name&gt;
  &lt;description&gt;Zookeeper quorum used by Hive&#39;s Table Lock Manager&lt;/description&gt;
  &lt;value&gt;desktop3,desktop4,desktop6,desktop7,desktop8&lt;/value&gt;
&lt;/property&gt;

&lt;property&gt;
  &lt;name&gt;hive.hwi.listen.host&lt;/name&gt;
  &lt;value&gt;desktop1&lt;/value&gt;
  &lt;description&gt;This is the host address the Hive Web Interface will listen on&lt;/description&gt;
&lt;/property&gt;

&lt;property&gt;
  &lt;name&gt;hive.hwi.listen.port&lt;/name&gt;
  &lt;value&gt;9999&lt;/value&gt;
  &lt;description&gt;This is the port the Hive Web Interface will listen on&lt;/description&gt;
&lt;/property&gt;

&lt;property&gt;
  &lt;name&gt;hive.hwi.war.file&lt;/name&gt;
  &lt;value&gt;lib/hive-hwi-0.10.0-cdh4.2.0.war&lt;/value&gt;
  &lt;description&gt;This is the WAR file with the jsp content for Hive Web Interface&lt;/description&gt;
&lt;/property&gt;
&lt;/configuration&gt;
</code></pre></div>
<ul>
<li>环境变量</li>
</ul>

<p>参考hadoop中环境变量的设置</p>

<ul>
<li>启动脚本</li>
</ul>

<p>在启动完之后，执行一些sql语句可能会提示错误，如何解决错误可以参考<a href="http://kicklinux.com/hive-deploy/">Hive安装与配置</a>。</p>
<div class="highlight"><pre><code class="text language-text" data-lang="text">[root@desktop1 ~] hive
</code></pre></div>
<ul>
<li>hive与hbase集成
在<code>hive-site.xml</code>中配置<code>hive.aux.jars.path</code>,在环境变量中配置hadoop、mapreduce的环境变量</li>
</ul>

<h3>异常说明</h3>

<ul>
<li>FAILED: Error in metadata: MetaException(message:org.apache.hadoop.hbase.ZooKeeperConnectionException: An error is preventing HBase from connecting to ZooKeeper</li>
</ul>

<p>原因：hadoop配置文件没有zk</p>

<ul>
<li>FAILED: Error in metadata: MetaException(message:Got exception: org.apache.hadoop.hive.metastore.api.MetaException javax.jdo.JDODataStoreException: Error executing JDOQL query &quot;SELECT &quot;THIS&quot;.&quot;TBL<em>NAME&quot; AS NUCORDER0 FROM &quot;TBLS&quot; &quot;THIS&quot; LEFT OUTER JOIN &quot;DBS&quot; &quot;THIS</em>DATABASE<em>NAME&quot; ON &quot;THIS&quot;.&quot;DB</em>ID&quot; = &quot;THIS<em>DATABASE</em>NAME&quot;.&quot;DB<em>ID&quot; WHERE &quot;THIS</em>DATABASE<em>NAME&quot;.&quot;NAME&quot; = ? AND (LOWER(&quot;THIS&quot;.&quot;TBL</em>NAME&quot;) LIKE ? ESCAPE &#39;\&#39; ) ORDER BY NUCORDER0 &quot; : ERROR: invalid escape string 建议：Escape string must be empty or one character..</li>
</ul>

<p>参考：https://issues.apache.org/jira/browse/HIVE-3994</p>

<ul>
<li><p>hive&gt; select count(*) from hive_userinfo; 没反应</p></li>
<li><p>zookeeper.ClientCnxn (ClientCnxn.java:logStartConnect(966)) - Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (无法定位登录配置)</p></li>
</ul>

<p>原因：hive中没有设置zk</p>

<ul>
<li>hbase 中提示：WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable</li>
</ul>

<p>原因：cloudera hadoop lib中没有hadoop的native jar</p>

<ul>
<li>Exception in thread &quot;main&quot; java.lang.NoClassDefFoundError: org/apache/hadoop/mapreduce/v2/app/MRAppMaster</li>
</ul>

<p>原因：classpath没有配置正确，检查环境变量以及yarn的classpath</p>

<h3>参考文章</h3>

<ul>
<li><a href="http://kicklinux.com/hive-deploy/">Hive安装与配置</a></li>
<li><a href="https://ccp.cloudera.com/display/CDH4DOC/Hive+Installation">Hive Installation</a></li>
</ul>
</div>
    </article>
  </section>
  
  
  <section id="hadoop20130308note-about-installing-hadoop-cluster">
    <article>
      <header>
      <h3><a href="/hadoop/2013/03/08/note-about-installing-hadoop-cluster">【笔记】Hadoop安装部署</a></h3>
      <div class="c9">
     		Author: JavaChen
     		&emsp;&emsp;
		Categories：
			
			<a href="/categories.html#hadoop">hadoop</a>
			
			
		&emsp;&emsp;
		Tags：
			
			<a href="/tags.html#hadoop-ref">hadoop</a>
			
			,
			
			
			<a href="/tags.html#cdh-ref">cdh</a>
			
			
		&emsp;&emsp;
		<a href='/hadoop/2013/03/08/note-about-installing-hadoop-cluster#comment' title='分享文章、查看评论' style="float:right;margin-right:.5em;">Comments</a>
	</div>
    </header>
    <div class="content"><h3>安装虚拟机</h3>

<p>VirtualBox安装rhel6.3，存储为30G，内存为1G，并复制2份</p>

<h3>配置网络</h3>

<p>a. VirtualBox全局设定-网络中添加一个新的连接：vboxnet0vi</p>

<p>b. 设置每一个虚拟机的网络为Host-Only</p>

<p>c.分别修改每个虚拟机的ip，DHCP或手动设置
        vim etc/sysconfig/network-scripts/ifcfg-eth0
        vim /etc/udev/rules.d/70-persistent-net.rules  #删掉第一个，修改第二个名字为eth0
        start_udev</p>

<p>d.修改主机名
        vim /etc/sysconfig/network</p>

<p>e.每个虚拟机中修改hosts：
        192.168.56.100 rhel-june
        192.168.56.101 rhel-june-1
        192.168.56.102 rhel-june-2</p>

<p>最后机器列表为：
        rhel-june:   192.168.56.100
        rhel-june-1: 192.168.56.101
        rhel-june-2: 192.168.56.102</p>

<h3>机群规划</h3>

<p>版本：
        hadoop:1.1.1
        JDK:1.6.0_38</p>

<p>集群各节点：</p>
<div class="highlight"><pre><code class="text language-text" data-lang="text">    NameNode:192.168.56.100
    NameSecondary:192.168.56.100
    DataNode:192.168.56.101
    DataNode:192.168.56.102
</code></pre></div>
<h3>安装过程</h3>

<p>a.解压缩到/opt</p>

<p>b.设置配置文件：
        core-site.xml
        hdfs-site.sml<br>
        mapred-site.xml</p>

<p>c.设置master、slaves</p>

<p>d.设置环境变量</p>

<p>方便执行java命令及hadoop命令. 使用root登录，vi ~/.bash<em>profile 追加下列信息
        export JAVA</em>HOME=/opt/jdk1.6.0<em>38
        export HADOOP</em>INSTALL=/opt/hadoop-1.1.1
        export PATH=$PATH:$HADOOP<em>INSTALL/bin:$JAVA</em>HOME/bin</p>

<p>e.修改hadoop脚本中JAVA_HOME：/opt/hadoop-1.1.1/conf/hadoop-env.sh</p>

<p>f.格式化namenode
        hadoop namenode -format</p>

<p>g.启动hdfs集群
        sh /opt/hadoop-1.1.1/bin/start-all.sh</p>

<p>h.查看节点进程
        jps</p>

<h3>查看状态</h3>
<div class="highlight"><pre><code class="text language-text" data-lang="text">    http://rhel-june:50030/
    http://rhel-june:50070/
</code></pre></div></div>
    </article>
  </section>
  
  <div class="pagination">
      <ul>
        <li class="next"><a href='/'>&larr; Previous</a></li>
        <li><a href="/archive.html">Archive</a></li>
        <li class="prev"><a href='/page3'>Next &rarr;</a></li>
      </ul>
  </div>
  </div>

  <aside class="span4">
    <section>
   	 <h4>TODO</h4>
   	 <ul style="margin-top: -3px">
		<li>hadoop权威指南</li>
       </ul>
    </section>

    <section>
    <h4>Recent Posts</h4>
    <ul id="recent_posts">
      <li class="post">
        <a href="/hadoop/2013/07/20/install-rhadoop">安装RHadoop</a>
      </li>
      <li class="post">
        <a href="/hadoop/2013/06/24/install-cdh-by-cloudera-manager">通过Cloudera Manager安装CDH</a>
      </li>
      <li class="post">
        <a href="/hadoop/2013/04/17/access-idh-2.3-hbase-in-kettle">kettle访问IDH2.3中的HBase</a>
      </li>
      <li class="post">
        <a href="/kettle/2013/04/07/add-a-field-from-paramter-to-output">kettle中添加一个参数字段到输出</a>
      </li>
      <li class="post">
        <a href="/hadoop/2013/04/06/install-cloudera-cdh4.2-by-yum">从yum安装Cloudera CDH集群</a>
      </li>
      <li class="post">
        <a href="/hadoop/2013/03/29/install-impala">安装impala过程</a>
      </li>
      <li class="post">
        <a href="/hadoop/2013/03/24/manual-install-Cloudera-Hadoop-CDH4.2">手动安装Cloudera Hadoop CDH4.2</a>
      </li>
      <li class="post">
        <a href="/hadoop/2013/03/24/manual-install-Cloudera-hbase-CDH4.2">手动安装Cloudera HBase CDH4.2</a>
      </li>
      <li class="post">
        <a href="/hadoop/2013/03/24/manual-install-Cloudera-hive-CDH4.2">手动安装Cloudera Hive CDH4.2</a>
      </li>
      <li class="post">
        <a href="/hadoop/2013/03/08/note-about-installing-hadoop-cluster">【笔记】Hadoop安装部署</a>
      </li>
    </ul>
    </section>
    
    <script type="text/javascript">
	var duoshuoQuery = {short_name:"javachen"};
	(function() {
		var ds = document.createElement('script');
		ds.type = 'text/javascript';ds.async = true;
		ds.src = 'http://static.duoshuo.com/embed.js';
		ds.charset = 'UTF-8';
		(document.getElementsByTagName('head')[0] 
		|| document.getElementsByTagName('body')[0]).appendChild(ds);
	})();
	</script>
	
	<section>
		<h3>Recent Comments</h3>
		<ul class="ds-recent-comments" data-num-items="5" data-show-avatars="1" data-show-time="1" data-show-title="1" data-show-admin="1" data-excerpt-length="70"></ul>
	<script>if (typeof DUOSHUO !== 'undefined')	DUOSHUO.RecentComments('.ds-recent-visitors');</script>
	</section>
	
	<section>
		<h3>Recent Vistors</h3>
		 <ul class="ds-recent-visitors" data-num-items="16"></ul>
		<script>if (typeof DUOSHUO !== 'undefined')	DUOSHUO.RecentVisitors('.ds-recent-visitors');</script>
	</section>
	 
     <section>
   	 <h4>WeiBo</h4>
   	 <iframe id="sina_widget_1222789964" style="width:100%; height:500px;" frameborder="0" scrolling="no" src="http://v.t.sina.com.cn/widget/widget_blog.php?uid=1222789964&height=500&skin=wd_01&showpic=0"></iframe>
    </section>
    
    <section>
	  
	    <h4>Categories</h4>
	    <ul class="tag_box">
	      
	      


  
     
    	<li><a href="/categories.html#cloud-ref">
    		cloud <span>9</span>
    	</a></li>
     
    	<li><a href="/categories.html#javascript-ref">
    		javascript <span>4</span>
    	</a></li>
     
    	<li><a href="/categories.html#pentaho-ref">
    		pentaho <span>2</span>
    	</a></li>
     
    	<li><a href="/categories.html#extjs-ref">
    		extjs <span>2</span>
    	</a></li>
     
    	<li><a href="/categories.html#xml-ref">
    		xml <span>1</span>
    	</a></li>
     
    	<li><a href="/categories.html#mondrian-ref">
    		mondrian <span>1</span>
    	</a></li>
     
    	<li><a href="/categories.html#kettle-ref">
    		kettle <span>8</span>
    	</a></li>
     
    	<li><a href="/categories.html#jsf-ref">
    		jsf <span>1</span>
    	</a></li>
     
    	<li><a href="/categories.html#seam-ref">
    		seam <span>1</span>
    	</a></li>
     
    	<li><a href="/categories.html#java-ref">
    		java <span>1</span>
    	</a></li>
     
    	<li><a href="/categories.html#cassandra-ref">
    		cassandra <span>2</span>
    	</a></li>
     
    	<li><a href="/categories.html#github-ref">
    		github <span>1</span>
    	</a></li>
     
    	<li><a href="/categories.html#work-ref">
    		work <span>1</span>
    	</a></li>
     
    	<li><a href="/categories.html#hadoop-ref">
    		hadoop <span>9</span>
    	</a></li>
    
  


	    </ul>
	  
     </section>
       <section>
    <h4>Links</h4>
   	<ul>
	<li>Java开发
			<ul>
				<li><a href="http://blog.frankel.ch/" target="_blank">A Java geek</a></li>
				<li><a href="http://xinwang.osdn.cn/" target="_blank">辛望的开发日志</a></li>
				<li><a href="http://kohsuke.org/" target="_blank">Kohsuke Kawaguchi</a></li>
				<li><a href="http://www.longtask.com/blog/" target="_blank">龙浩的blog</a>就职于阿里巴巴云计算</li>
				<li><a href="http://jdkcn.com/" target="_blank">莫多泡泡</a>A Java programmeri</li>
				<li><a href="http://hackfisher.info/" target="_blank">HackFisher</a></li>
				<li><a href="http://bluedash.net/categories/%E7%BC%96%E7%A8%8B/spaces" target="_blank">蓝点</a></li>
				<li><a href="http://javafans.info/" target="_blank">Java爱好者</a></li>
				<li><a href="http://www.yankay.com/" target="_blank">我自然</a>颜开的博客</li>
			</ul>
		</li>
		<li>前端开发
			<ul>
				<li><a href="http://panweizeng.com/" target="_blank">潘魏增</a>美团网前端工程师</li>
				<li><a href="http://14px.com/" target="_blank">十四像素</a></li>
			</ul>
		</li>
		<li>其他
			<ul>
				<li><a href="http://blog.boluotou.com/" target="_blank">圆木菠萝罐</a>一个大学学长</li>
				<li><a href="http://blog.codingnow.com/" target="_blank">云风的BLOG</a></li>
				<li><a href="http://www.yy42.net/blog/" target="_blank">程显峰</a></li>
				<li><a href="http://coolshell.cn/" target="_blank">酷壳–CoolShell.cn</a></li>
				<li><a href="http://www.coder4.com/" target="_blank">四号程序员</a></li>
				<li><a href="http://timyang.net/" target="_blank">Tim[后端技术]</a></li>

				<li><a href="http://www.agiledon.com/" target="_blank">捷道</a>Thoughtworks架构师</li>
				<li><a href="http://log4d.com/" target="_blank">Log4D</a></li>

				<li><a href="http://dev.ymeng.net/" target="_blank">Dev Notes</a></li>
				<li><a href="http://www.dbanotes.net/" target="_blank">DBA Notes</a></li>		
			</ul>
		</li>
	</ul>
  </section>
  </aside>
</div>

  </div>
</div>


      </div>

      <footer>
        <p>&copy; JavaChen 2013 
          with help from <a href="http://jekyllbootstrap.com" target="_blank" title="The Definitive Jekyll Blogging Framework">Jekyll Bootstrap</a>
          and <a href="http://twitter.github.com/bootstrap/" target="_blank">Twitter Bootstrap</a> | <script language="javascript" type="text/javascript" src="http://js.users.51.la/12111481.js"></script>
<noscript><a href="http://www.51.la/?12111481" target="_blank"><img alt="Statistic" src="http://img.users.51.la/12111481.asp" style="border:none" /></a></noscript>
        </p>
      </footer>

    </div> <!-- /container -->
    
  </body>
</html>

